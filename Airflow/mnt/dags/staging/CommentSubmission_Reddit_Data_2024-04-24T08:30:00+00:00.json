[{"comment_ID": "l10uojt", "comment_Body": "Designing Data Intensive Applications.", "comment_Score": 1, "comment_Author": "ChristlikeYe", "comment_Link_id": "1cbrdoa", "Create Date": "2024-04-24 10:14:41+00:00"}, {"comment_ID": "l10umfz", "comment_Body": "I assume you have a webserver to provide access to the surveys.\n  The webserver can dump the complete surveys in AWS S3. From here, you crawl the data with a Glue Crawler and use Athena to query the data using sql.\nS3 is cheap, athena is cheap, might cost nothing", "comment_Score": 1, "comment_Author": "InsightByte", "comment_Link_id": "1canxa4", "Create Date": "2024-04-24 10:14:02+00:00"}, {"comment_ID": "l10tzfg", "comment_Body": "yep. \n\nprogrammer spends his day to get that data from ui to database and normalize it to some extend , then next guy does ETL and pushes it to somekind of datawarehouse, lake or whatever. Then next guy takes that shit again and does some normalization in dwh model and then comes next guy who again denormalizes that data for reports or just for \"easier\" usage.... \n\nthat said, imho process database ,does not need to be fully normalized, then that should be  raw as possible to dwh side of things and there some stuff needs to be normalized , but not everything.", "comment_Score": 1, "comment_Author": "throw_mob", "comment_Link_id": "1cb573s", "Create Date": "2024-04-24 10:06:52+00:00"}, {"comment_ID": "l10ruex", "comment_Body": "We're using DABs and they are working well.  We will manually deploy to our sandbox environment while developing jobs/workflows.  Our CI/CD (github actions) does deploys for our production environment.", "comment_Score": 1, "comment_Author": "vimtastic", "comment_Link_id": "1cbkvjb", "Create Date": "2024-04-24 09:41:34+00:00"}, {"comment_ID": "l10qvyb", "comment_Body": "That's the one, my bad.", "comment_Score": 1, "comment_Author": "Demistr", "comment_Link_id": "1cb8yh8", "Create Date": "2024-04-24 09:29:47+00:00"}, {"comment_ID": "l10p6gp", "comment_Body": "We do the something similar in our company there are multiple tables which are used for lookups .\n\n1. Records comes into the Queue.\n2. Record gets into flink.\n3. Clean, Enrich it and apply some business logic (Use information from the different - different tables from redis and cassandra and they are not related to each other they just contain different domain specific data)\n4. Write denormalized, cleaned and processed result into sink for downstream consumers.", "comment_Score": 1, "comment_Author": "AggravatingParsnip89", "comment_Link_id": "1cb5h8y", "Create Date": "2024-04-24 09:08:16+00:00"}, {"comment_ID": "l10oii5", "comment_Body": "> from a PC under someone's desk\n\nDon't forget regular backup to someone else's PC though!", "comment_Score": 1, "comment_Author": "sib_n", "comment_Link_id": "1cb7mqe", "Create Date": "2024-04-24 08:59:50+00:00"}, {"comment_ID": "l10nj0t", "comment_Body": "What tech stack you are using ?", "comment_Score": 1, "comment_Author": "AggravatingParsnip89", "comment_Link_id": "1cbq5ys", "Create Date": "2024-04-24 08:47:27+00:00"}, {"comment_ID": "l10nevx", "comment_Body": "If you want to perform real time analysis. This is one of the possible solution.  \nData source ---> Kafka ---> Flink or Spark Streaming (Analyse data here) ----> write into elastic search", "comment_Score": 1, "comment_Author": "AggravatingParsnip89", "comment_Link_id": "1cbpprl", "Create Date": "2024-04-24 08:45:58+00:00"}, {"comment_ID": "l10mj8p", "comment_Body": "Sure you can do tagging in Azure, and that can be used in billing reports. However I\u2019m not sure you can identify transaction activity within a shared Resource. Potentially more admin at the end of the day to split out costs, but I\u2019ll look into it more thanks.\n\nAlso- Microsoft invoice us per Subscription. They don\u2019t send invoices per resource grouping.", "comment_Score": 1, "comment_Author": "scan-horizon", "comment_Link_id": "1cbkotm", "Create Date": "2024-04-24 08:34:41+00:00"}, {"comment_ID": "l10mexr", "comment_Body": "I think DABs for job creation makes total sense. Especially in production environments, jobs should be backed by code, and I'd take DABs over Terraform in that regard.", "comment_Score": 1, "comment_Author": "coolbeans201", "comment_Link_id": "1cbkvjb", "Create Date": "2024-04-24 08:33:08+00:00"}, {"comment_ID": "l10m8gv", "comment_Body": "In case of Pure streaming each record flow as a single entity in DAG. It is good when you need low latency (in milliseconds).  \nIf you are trying to implement streaming using micro batching then it should work as well but not as good as former in terms of latency. And you may need to adjust batch size in micro batching, high batch size means high latency and high throughput low batch size means low latency and low thorughput.", "comment_Score": 1, "comment_Author": "AggravatingParsnip89", "comment_Link_id": "1cb8yh8", "Create Date": "2024-04-24 08:30:49+00:00"}, {"comment_ID": "l10ln0c", "comment_Body": "If you're writing Python code in a production environment, enabling structured logging can be a game changer for debugging. (It was for us)  \nI wrote a\u00a0[guide on how to implement structured logging](https://medium.com/p/83a5ee6ae561)\u00a0in a couple of server frameworks (Django, FastAPI, gRPC). Hope it can help others.", "comment_Score": 1, "comment_Author": "tom-cent", "comment_Link_id": "1bt7iv2", "Create Date": "2024-04-24 08:23:09+00:00"}, {"comment_ID": "l10ll13", "comment_Body": "I have less pay too and working as data engineer with 2.6 yoe. There is nothing we can do except preparing for interviews.", "comment_Score": 1, "comment_Author": "AggravatingParsnip89", "comment_Link_id": "1cbq5ys", "Create Date": "2024-04-24 08:22:27+00:00"}, {"comment_ID": "l10kb3l", "comment_Body": "Not deeply familiar with Azure cost tagging, but I\u2019m sure there\u2019s something similar to AWS Cost Allocation tags? Add those to BU-specific infra, and divide the shared infra cost by 5, into each BU.", "comment_Score": 1, "comment_Author": "davrax", "comment_Link_id": "1cbkotm", "Create Date": "2024-04-24 08:06:08+00:00"}, {"comment_ID": "l10i0xw", "comment_Body": "It really depends on the level of security required. Probably but its not always in budget and not necessarily required so we do our best.  County i work for has a serious budget issue. No increase in funding since the mid-1990s. Budget is why we are starting to use llms instead of receptionists, dmv employees, and hiring more human services folks. GCP made it cheap.", "comment_Score": 1, "comment_Author": "asevans48", "comment_Link_id": "1cbkotm", "Create Date": "2024-04-24 07:37:14+00:00"}, {"comment_ID": "l10hspx", "comment_Body": "Sounds fine.\n\nCosts in Synapse is based off how long the cluster has been active for with how many cores as opposed to the size of the data.", "comment_Score": 1, "comment_Author": "MikeDoesEverything", "comment_Link_id": "1cb6yc1", "Create Date": "2024-04-24 07:34:23+00:00"}, {"comment_ID": "l10hnu1", "comment_Body": "Thanks. Should sensitive data be isolated at the orchestration layer (eg. Azure data factory) or only at the storage layer? (Eg sql databases).", "comment_Score": 1, "comment_Author": "scan-horizon", "comment_Link_id": "1cbkotm", "Create Date": "2024-04-24 07:32:38+00:00"}, {"comment_ID": "l10hi9f", "comment_Body": "70gb can fit on a modern laptop with duckdb", "comment_Score": 2, "comment_Author": "Mayabangnatao", "comment_Link_id": "1cb7mqe", "Create Date": "2024-04-24 07:30:42+00:00"}, {"comment_ID": "l10hhtb", "comment_Body": "Personally identifying information, personal health information; and criminal justice information system data.", "comment_Score": 1, "comment_Author": "asevans48", "comment_Link_id": "1cbkotm", "Create Date": "2024-04-24 07:30:32+00:00"}, {"comment_ID": "l10g4ru", "comment_Body": "I ran a TB data warehouse in sql server hosted in an Azure VM a few years back. \n\nUsed MOLAP and Tabular Models for Power BI.\n And worked great.\n\nI left and they moved it all to data lake and they haven't replicated the performance. \n\nSometimes you really don't need a lakehouse.", "comment_Score": 2, "comment_Author": "introvertedguy13", "comment_Link_id": "1cb7mqe", "Create Date": "2024-04-24 07:13:44+00:00"}, {"comment_ID": "l10fuln", "comment_Body": "Sorry what is pii cjis and phi?", "comment_Score": 1, "comment_Author": "scan-horizon", "comment_Link_id": "1cbkotm", "Create Date": "2024-04-24 07:10:20+00:00"}, {"comment_ID": "l10f82i", "comment_Body": "OP, is that word salad supposed to be a sales pitch or a serious commentary on the state of things?\n\nAnyway. You lost me very quickly.", "comment_Score": 1, "comment_Author": "GreenWoodDragon", "comment_Link_id": "1cb3zbp", "Create Date": "2024-04-24 07:02:50+00:00"}, {"comment_ID": "l10f2at", "comment_Body": "Thanks. I do see a problem from a billing POV with a single dw though. As each business unit have their own funds, & pays for their own stuff usually, we\u2019d need to find a way of splitting the Azure costs. Say if one area required more ETL pipelines to be run for their purposes, we\u2019d need to make sure they pay proportionately more (Azure data factory is billed per pipeline activity run / vcore activity).", "comment_Score": 1, "comment_Author": "scan-horizon", "comment_Link_id": "1cbkotm", "Create Date": "2024-04-24 07:00:56+00:00"}, {"comment_ID": "l10el21", "comment_Body": "It would be an OLAP dw, used for querying data regarding the business area.", "comment_Score": 1, "comment_Author": "scan-horizon", "comment_Link_id": "1cbkotm", "Create Date": "2024-04-24 06:55:15+00:00"}, {"comment_ID": "l10dyak", "comment_Body": "KewL lol", "comment_Score": 2, "comment_Author": "just_nave", "comment_Link_id": "1cb2ym3", "Create Date": "2024-04-24 06:47:55+00:00"}, {"comment_ID": "l10dgmw", "comment_Body": "What a coincidence. I had an interview yesterday, I brushed up on Spark Architecture, internal workings, Databricks, Azure Cloud, End-To-End cloud pipelines, data modelling and all he asked me was Hadoop copy from local to HDFS, and how to load table without knowing the path of Hive location. I completely blanked out as that was something I had done quite a long time back. Theoretically, I knew what to do, but it really made me feel stupid that I was not able to write 2 commands -\\_-", "comment_Score": 1, "comment_Author": "napsterv", "comment_Link_id": "1cb2ym3", "Create Date": "2024-04-24 06:42:18+00:00"}, {"comment_ID": "l10d6zd", "comment_Body": "in HIVE database you have table that contains call/ text from/to numbers, this is FACT because somebody called/texted someone.\n\nDimension tables could be user/subscriber who owns phone number that made/received calls.\nSubscriber address data is another dimension so on and so forth.\n\nData warehouse would have curated data collection and mostly used for analytics.\n\nData lake could have structured/unstructured data and meant for analysis and use cases are unknown ATM.", "comment_Score": 1, "comment_Author": "ragshiaar1", "comment_Link_id": "1cb5h8y", "Create Date": "2024-04-24 06:39:12+00:00"}, {"comment_ID": "l10c48q", "comment_Body": "The files are *in* the computer?", "comment_Score": 2, "comment_Author": "bisectional", "comment_Link_id": "1cb7mqe", "Create Date": "2024-04-24 06:26:56+00:00"}, {"comment_ID": "l10bjga", "comment_Body": "Rust layer", "comment_Score": 1, "comment_Author": "fLu_csgo", "comment_Link_id": "1cb7mqe", "Create Date": "2024-04-24 06:20:20+00:00"}, {"comment_ID": "l10bf0v", "comment_Body": "You may want to compare the types of resources that DABs and Terraform allow you to configure, and perhaps combine the two instead of leaving Terraform for DABs.\n\nhttps://docs.databricks.com/en/dev-tools/bundles/settings.html#resources\n\nhttps://registry.terraform.io/providers/databricks/databricks/latest/docs", "comment_Score": 1, "comment_Author": "johnkdb", "comment_Link_id": "1cbkvjb", "Create Date": "2024-04-24 06:18:57+00:00"}, {"comment_ID": "l10b0ag", "comment_Body": "totally agree.\n\ni have not make any code test newer, what i have done is that we looked code and told what it does at backend and sql servers. Then i have told my opinions about it.\n\nWhat i have noticed on dwh side of things that there is a lot of people who know howto code , but they do not have idea how data flows in system , what it means and howto provide it so that i can be easily used on different levels. And when i say , they know howto coe , does not mean that they know howto make fast sql or readable sql, ability or even knowledge of query plans seems to dark magic, and sometimes when you raise question on performance people just dont understand that to get resonable performance might needs some optimizations which depends on platform or data itself.\n\nSo, it is more critical to get people who are aware of things and have ability understand patterns and data than code exactly right on used platform. If you know sql, you will learn to use it on different platform so it works and if there is enough underlying understanding of system , they will generate good and fast code at somepoint. \n\nI find it funny that in database world , it seems that management expect that one its done , it does not need any monitoring or development. That attitude usually leads to very slow databases as query which worked perfectly with 100 rows suddenly gets slow when data amount get to 10k rows or so", "comment_Score": 1, "comment_Author": "throw_mob", "comment_Link_id": "1cb2ym3", "Create Date": "2024-04-24 06:14:22+00:00"}, {"comment_ID": "l10axgo", "comment_Body": "Aren't the DABs just terraform under the hood anyway?", "comment_Score": 1, "comment_Author": "IcyTangelo3634", "comment_Link_id": "1cbkvjb", "Create Date": "2024-04-24 06:13:31+00:00"}, {"comment_ID": "l1090pz", "comment_Body": "Machine Learning engineers are in high demand. DE is saturated but better than DS rn", "comment_Score": 1, "comment_Author": "tipsybug", "comment_Link_id": "1cb2ym3", "Create Date": "2024-04-24 05:52:39+00:00"}, {"comment_ID": "l108sqn", "comment_Body": "This is exactly how we've done it at my last two places. Works well and is cheap", "comment_Score": 2, "comment_Author": "kiwi_bob_1234", "comment_Link_id": "1cb7mqe", "Create Date": "2024-04-24 05:50:17+00:00"}, {"comment_ID": "l105ka2", "comment_Body": "I've worked as a data engineer for 5+ years. Nobody gives a shit about normalization- it's all about balancing minimizing joins while making sure your table doesn't become too big", "comment_Score": 0, "comment_Author": "CorerMaximus", "comment_Link_id": "1cb573s", "Create Date": "2024-04-24 05:17:12+00:00"}, {"comment_ID": "l104z4w", "comment_Body": "Not considered data engineering? My whole career so far is about denormalizing. I haven\u00b4t build any data model  that is 3NF or above (star modeling, data vault). I have clients still working on mainframes you can\u00b4t even consider relational", "comment_Score": 1, "comment_Author": "dehaema", "comment_Link_id": "1cb573f", "Create Date": "2024-04-24 05:11:28+00:00"}, {"comment_ID": "l104eht", "comment_Body": "dm'ed", "comment_Score": 1, "comment_Author": "Shoddy-Avocado-6149", "comment_Link_id": "1cahwty", "Create Date": "2024-04-24 05:05:54+00:00"}, {"comment_ID": "l104cme", "comment_Body": "I feel you. I had the same thing happen to me two weeks ago. Been doing 1-2 data lemur questions a day while timing myself Not so much to practice SQL, but to practice assessment taking.", "comment_Score": 1, "comment_Author": "DMReader", "comment_Link_id": "1cb2ym3", "Create Date": "2024-04-24 05:05:24+00:00"}, {"comment_ID": "l1046fi", "comment_Body": "You could have two tables, based on customer id and vehicle id so you don\u2019t have to copy all of that information and if a customers phone number changes it\u2019s easy to change a single row in another table which will have a primary key of the customer id", "comment_Score": 1, "comment_Author": "MulhollandDr1ve", "comment_Link_id": "1cb573s", "Create Date": "2024-04-24 05:03:45+00:00"}]