[{"comment_ID": "l10g4ru", "comment_Body": "I ran a TB data warehouse in sql server hosted in an Azure VM a few years back. \n\nUsed MOLAP and Tabular Models for Power BI.\n And worked great.\n\nI left and they moved it all to data lake and they haven't replicated the performance. \n\nSometimes you really don't need a lakehouse.", "comment_Score": 1, "comment_Author": "introvertedguy13", "comment_Link_id": "t3_1cb7mqe", "Create Date": "2024-04-24 07:13:44+00:00"}, {"comment_ID": "l10fuln", "comment_Body": "Sorry what is pii cjis and phi?", "comment_Score": 1, "comment_Author": "scan-horizon", "comment_Link_id": "t3_1cbkotm", "Create Date": "2024-04-24 07:10:20+00:00"}, {"comment_ID": "l10f82i", "comment_Body": "OP, is that word salad supposed to be a sales pitch or a serious commentary on the state of things?\n\nAnyway. You lost me very quickly.", "comment_Score": 1, "comment_Author": "GreenWoodDragon", "comment_Link_id": "t3_1cb3zbp", "Create Date": "2024-04-24 07:02:50+00:00"}, {"comment_ID": "l10f2at", "comment_Body": "Thanks. I do see a problem from a billing POV with a single dw though. As each business unit have their own funds, & pays for their own stuff usually, we\u2019d need to find a way of splitting the Azure costs. Say if one area required more ETL pipelines to be run for their purposes, we\u2019d need to make sure they pay proportionately more (Azure data factory is billed per pipeline activity run / vcore activity).", "comment_Score": 1, "comment_Author": "scan-horizon", "comment_Link_id": "t3_1cbkotm", "Create Date": "2024-04-24 07:00:56+00:00"}, {"comment_ID": "l10el21", "comment_Body": "It would be an OLAP dw, used for querying data regarding the business area.", "comment_Score": 1, "comment_Author": "scan-horizon", "comment_Link_id": "t3_1cbkotm", "Create Date": "2024-04-24 06:55:15+00:00"}, {"comment_ID": "l10dyak", "comment_Body": "KewL lol", "comment_Score": 1, "comment_Author": "just_nave", "comment_Link_id": "t3_1cb2ym3", "Create Date": "2024-04-24 06:47:55+00:00"}, {"comment_ID": "l10dgmw", "comment_Body": "What a coincidence. I had an interview yesterday, I brushed up on Spark Architecture, internal workings, Databricks, Azure Cloud, End-To-End cloud pipelines, data modelling and all he asked me was Hadoop copy from local to HDFS, and how to load table without knowing the path of Hive location. I completely blanked out as that was something I had done quite a long time back. Theoretically, I knew what to do, but it really made me feel stupid that I was not able to write 2 commands -\\_-", "comment_Score": 1, "comment_Author": "napsterv", "comment_Link_id": "t3_1cb2ym3", "Create Date": "2024-04-24 06:42:18+00:00"}, {"comment_ID": "l10d6zd", "comment_Body": "in HIVE database you have table that contains call/ text from/to numbers, this is FACT because somebody called/texted someone.\n\nDimension tables could be user/subscriber who owns phone number that made/received calls.\nSubscriber address data is another dimension so on and so forth.\n\nData warehouse would have curated data collection and mostly used for analytics.\n\nData lake could have structured/unstructured data and meant for analysis and use cases are unknown ATM.", "comment_Score": 1, "comment_Author": "ragshiaar1", "comment_Link_id": "t3_1cb5h8y", "Create Date": "2024-04-24 06:39:12+00:00"}, {"comment_ID": "l10c48q", "comment_Body": "The files are *in* the computer?", "comment_Score": 1, "comment_Author": "bisectional", "comment_Link_id": "t3_1cb7mqe", "Create Date": "2024-04-24 06:26:56+00:00"}, {"comment_ID": "l10bjga", "comment_Body": "Rust layer", "comment_Score": 1, "comment_Author": "fLu_csgo", "comment_Link_id": "t3_1cb7mqe", "Create Date": "2024-04-24 06:20:20+00:00"}, {"comment_ID": "l10bf0v", "comment_Body": "You may want to compare the types of resources that DABs and Terraform allow you to configure, and perhaps combine the two instead of leaving Terraform for DABs.\n\nhttps://docs.databricks.com/en/dev-tools/bundles/settings.html#resources\n\nhttps://registry.terraform.io/providers/databricks/databricks/latest/docs", "comment_Score": 1, "comment_Author": "johnkdb", "comment_Link_id": "t3_1cbkvjb", "Create Date": "2024-04-24 06:18:57+00:00"}, {"comment_ID": "l10b0ag", "comment_Body": "totally agree.\n\ni have not make any code test newer, what i have done is that we looked code and told what it does at backend and sql servers. Then i have told my opinions about it.\n\nWhat i have noticed on dwh side of things that there is a lot of people who know howto code , but they do not have idea how data flows in system , what it means and howto provide it so that i can be easily used on different levels. And when i say , they know howto coe , does not mean that they know howto make fast sql or readable sql, ability or even knowledge of query plans seems to dark magic, and sometimes when you raise question on performance people just dont understand that to get resonable performance might needs some optimizations which depends on platform or data itself.\n\nSo, it is more critical to get people who are aware of things and have ability understand patterns and data than code exactly right on used platform. If you know sql, you will learn to use it on different platform so it works and if there is enough underlying understanding of system , they will generate good and fast code at somepoint. \n\nI find it funny that in database world , it seems that management expect that one its done , it does not need any monitoring or development. That attitude usually leads to very slow databases as query which worked perfectly with 100 rows suddenly gets slow when data amount get to 10k rows or so", "comment_Score": 1, "comment_Author": "throw_mob", "comment_Link_id": "t3_1cb2ym3", "Create Date": "2024-04-24 06:14:22+00:00"}, {"comment_ID": "l10axgo", "comment_Body": "Aren't the DABs just terraform under the hood anyway?", "comment_Score": 1, "comment_Author": "IcyTangelo3634", "comment_Link_id": "t3_1cbkvjb", "Create Date": "2024-04-24 06:13:31+00:00"}, {"comment_ID": "l1090pz", "comment_Body": "Machine Learning engineers are in high demand. DE is saturated but better than DS rn", "comment_Score": 1, "comment_Author": "tipsybug", "comment_Link_id": "t3_1cb2ym3", "Create Date": "2024-04-24 05:52:39+00:00"}, {"comment_ID": "l108sqn", "comment_Body": "This is exactly how we've done it at my last two places. Works well and is cheap", "comment_Score": 1, "comment_Author": "kiwi_bob_1234", "comment_Link_id": "t3_1cb7mqe", "Create Date": "2024-04-24 05:50:17+00:00"}, {"comment_ID": "l105ka2", "comment_Body": "I've worked as a data engineer for 5+ years. Nobody gives a shit about normalization- it's all about balancing minimizing joins while making sure your table doesn't become too big", "comment_Score": 1, "comment_Author": "CorerMaximus", "comment_Link_id": "t3_1cb573s", "Create Date": "2024-04-24 05:17:12+00:00"}, {"comment_ID": "l104z4w", "comment_Body": "Not considered data engineering? My whole career so far is about denormalizing. I haven\u00b4t build any data model  that is 3NF or above (star modeling, data vault). I have clients still working on mainframes you can\u00b4t even consider relational", "comment_Score": 1, "comment_Author": "dehaema", "comment_Link_id": "t3_1cb573f", "Create Date": "2024-04-24 05:11:28+00:00"}, {"comment_ID": "l104eht", "comment_Body": "dm'ed", "comment_Score": 1, "comment_Author": "Shoddy-Avocado-6149", "comment_Link_id": "t3_1cahwty", "Create Date": "2024-04-24 05:05:54+00:00"}, {"comment_ID": "l104cme", "comment_Body": "I feel you. I had the same thing happen to me two weeks ago. Been doing 1-2 data lemur questions a day while timing myself Not so much to practice SQL, but to practice assessment taking.", "comment_Score": 1, "comment_Author": "DMReader", "comment_Link_id": "t3_1cb2ym3", "Create Date": "2024-04-24 05:05:24+00:00"}, {"comment_ID": "l1046fi", "comment_Body": "You could have two tables, based on customer id and vehicle id so you don\u2019t have to copy all of that information and if a customers phone number changes it\u2019s easy to change a single row in another table which will have a primary key of the customer id", "comment_Score": 1, "comment_Author": "MulhollandDr1ve", "comment_Link_id": "t3_1cb573s", "Create Date": "2024-04-24 05:03:45+00:00"}, {"comment_ID": "l103nop", "comment_Body": "I feel Elastic Search is limited as data is stored as an index. As result I need to perform 'search' for further analysis of the data. This is the reason I feel it is better to store the data and then retrieve the same for Elastic Search and also for data analysis. Is my understanding right ?", "comment_Score": 1, "comment_Author": "LobsterMost5947", "comment_Link_id": "t3_1b2kluj", "Create Date": "2024-04-24 04:58:48+00:00"}, {"comment_ID": "l103fqi", "comment_Body": "Any etl? \n\n    cat file.txt | sed 's/dog/cat/g'  > output.txt", "comment_Score": 1, "comment_Author": "baubleglue", "comment_Link_id": "t3_1ca3xs2", "Create Date": "2024-04-24 04:56:47+00:00"}, {"comment_ID": "l101kjs", "comment_Body": "Your intuitve solution is correct but it won't cover all the \"edge cases\". I hate those in the interview too.", "comment_Score": 1, "comment_Author": "SmartPersonality1862", "comment_Link_id": "t3_1cb2ym3", "Create Date": "2024-04-24 04:39:58+00:00"}, {"comment_ID": "l101eso", "comment_Body": "Correct me if I'm wrong  but sounds like the interviewer ask you to find the top 3 product by whatever (let say sum of quantity sold). The most intuitive way is to agg and then sort it and take the top 3. But that won't work if let say you have 2 best selling product with the same quantity sold. If you hard code the limit 3 it will only return technically the top 2 (or even 1 if there's 3 product with the same quantity sold) right. So the solution have to be dense rank it using the sum of quantity and later put that in a subquery and use a where statement to check if the rank is <=3. Let me know if im wrong on this since i'm still a college student.", "comment_Score": 1, "comment_Author": "SmartPersonality1862", "comment_Link_id": "t3_1cb2ym3", "Create Date": "2024-04-24 04:38:31+00:00"}, {"comment_ID": "l1012kf", "comment_Body": "thank you for the help and chatting with me. I finally got a working solution:\n\n* load the existing table in S3 into a DF\n* use DataframeWriter's `append` mode and Hudi's `delete` operation\n* write this deleted DF back to the same S3 location\n\nThe reason why Redshift spectrum does not support `insert_overwrite` is because it is unable to read the `replacecommit` metadata files. They only support `commit` files,  i looked through both of those files types and i did not see noticeably different between the two so there must be something i am unaware of", "comment_Score": 1, "comment_Author": "they_paid_for_it", "comment_Link_id": "t3_1c3yov5", "Create Date": "2024-04-24 04:35:29+00:00"}, {"comment_ID": "l100yqp", "comment_Body": "I embarrassed myself in my first job interview saying es-queue-ell instead of \u201csequel\u201d because I was self-taught from books and practice and had never heard it said out loud \ud83d\ude2d \n\nThat said I did get the job though!", "comment_Score": 2, "comment_Author": "mr-jaybird", "comment_Link_id": "t3_1cb2ym3", "Create Date": "2024-04-24 04:34:33+00:00"}, {"comment_ID": "l0zzbla", "comment_Body": "How quickly does the business really needs it? And as soon as we talk about enriching the data, the complexity jumps multiple times because we are now talking about joining and merging with other systems which may not be as quick as let\u2019s say a kinesis firehose. If we already introducing this much latency, then maybe instead of real time, a near real solution which includes some batches (think Lambdas being triggered based on events) and such processes.\n\nSo imo, real time unless absolutely required and supported by existing infrastructure (as it\u2019s usually\nMore expensive than batch or near real time) is not the first choice.  I\u2019ve talking about massive amounts of data.", "comment_Score": 1, "comment_Author": "memyselfandi1987", "comment_Link_id": "t3_1cb8yh8", "Create Date": "2024-04-24 04:20:13+00:00"}, {"comment_ID": "l0zyjw8", "comment_Body": "Yeah 70 GB's is trivial. Save yourself the complexity and run it on sql", "comment_Score": 1, "comment_Author": "doinnuffin", "comment_Link_id": "t3_1cb7mqe", "Create Date": "2024-04-24 04:13:38+00:00"}, {"comment_ID": "l0zxtzz", "comment_Body": "I agree with the others that, given your data volume, it makes sense that a single SQL Server is satisfying your needs in a way where the complexity of managing a lakehouse platform probably isn\u2019t worth the squeeze of the orange. \n\nWhen you say development is more onerous, I\u2019m curious if there\u2019s anything specific that is causing pain in that area?", "comment_Score": 2, "comment_Author": "WHY-ARE-YOU-CLOSED", "comment_Link_id": "t3_1cb7mqe", "Create Date": "2024-04-24 04:07:38+00:00"}, {"comment_ID": "l0zvvy0", "comment_Body": "Yea like that", "comment_Score": 1, "comment_Author": "dcent12345", "comment_Link_id": "t3_1cb3zbp", "Create Date": "2024-04-24 03:51:27+00:00"}, {"comment_ID": "l0zvjjo", "comment_Body": "A common core is nice. You may need others depending on security and complexity. Where I work, one department has pii governed by cjis and another stores phi. Others have arrangements that need to be isolated.", "comment_Score": 2, "comment_Author": "asevans48", "comment_Link_id": "t3_1cbkotm", "Create Date": "2024-04-24 03:48:40+00:00"}, {"comment_ID": "l0zv1u2", "comment_Body": "Same for me", "comment_Score": 1, "comment_Author": "natas_m", "comment_Link_id": "t3_1cb573s", "Create Date": "2024-04-24 03:44:40+00:00"}, {"comment_ID": "l0zuzfc", "comment_Body": "That\u2019s funny. I am a student myself, doing master of Business Analytics. Funny how this is not really a big issue when it comes to working with databases while I am currently struggling to fully understand it. What kind of questions were you asked at interviews? And did you work in these different position titles?", "comment_Score": 1, "comment_Author": "Radioheader377", "comment_Link_id": "t3_1cb573s", "Create Date": "2024-04-24 03:44:07+00:00"}, {"comment_ID": "l0zuy9j", "comment_Body": "How quickly the results are needed.", "comment_Score": 1, "comment_Author": "RevolutionStill4284", "comment_Link_id": "t3_1cb8yh8", "Create Date": "2024-04-24 03:43:51+00:00"}, {"comment_ID": "l0zunbl", "comment_Body": "Serious question, does this actually come up for anyone in their day-to-day? I've been doing database work with various titles (SQL programmer, BI Engineer, BI Developer, Data Analyst, Data Engineer, probably some other random ones) and I have literally never heard a single person mention normal forms outside of one graduate class I took years ago. In my professional life, no one has ever said 3NF to me. I've worked in education, retail, and healthcare. For state agencies, public and private companies. Worked with data sets in the tens of thousands up to the hundreds of billions. On-prem setups and cloud infrastructures. Literally, not once, has anyone mentioned this at work, and if someone were to ask me about it in an interview I couldn't tell you the definition of any of them.", "comment_Score": 2, "comment_Author": "Mononon", "comment_Link_id": "t3_1cb573s", "Create Date": "2024-04-24 03:41:25+00:00"}, {"comment_ID": "l0zsp1b", "comment_Body": "Love this channel. Why no mention of db2", "comment_Score": 1, "comment_Author": "Foodwithfloyd", "comment_Link_id": "t3_1cbef8q", "Create Date": "2024-04-24 03:26:11+00:00"}, {"comment_ID": "l0zshnp", "comment_Body": "I'm honestly still trying to figure out how all this works together. I've had enough experience (and read from others) that doing transformations in ADF is a nightmare. So, my thought would be to use ADF for orchestration but also for the extraction and loading of data from other systems, and then ADF would trigger stored procedures to transform and clean data and eventually kick data through to a gold layer.", "comment_Score": 2, "comment_Author": "e3thomps", "comment_Link_id": "t3_1cb7mqe", "Create Date": "2024-04-24 03:24:38+00:00"}, {"comment_ID": "l0zrwt1", "comment_Body": "I\u2019ve worked for a consultancy like this\u2026 Microsoft will often pay the consultancy a fee to build out engineering solutions on whatever the flavour of the day is.\n\nIf they can prove that X clients solution will cost \u00a3100k per year, they might offer \u00a350k to the consultancy to build it out in synapses\u2026", "comment_Score": 3, "comment_Author": "PumaPunku131", "comment_Link_id": "t3_1cb7mqe", "Create Date": "2024-04-24 03:20:17+00:00"}, {"comment_ID": "l0zr5hu", "comment_Body": "Also MLE here, unfortunately I\u2019ve heard business people say \u201cor something similar to that. Would much rather hear SQL than whatever the hell that is.", "comment_Score": 1, "comment_Author": "Der_Krsto", "comment_Link_id": "t3_1cb2ym3", "Create Date": "2024-04-24 03:14:33+00:00"}, {"comment_ID": "l0zo1k0", "comment_Body": "I second this.", "comment_Score": 1, "comment_Author": "SirGreybush", "comment_Link_id": "t3_1cbcfsv", "Create Date": "2024-04-24 02:51:59+00:00"}]