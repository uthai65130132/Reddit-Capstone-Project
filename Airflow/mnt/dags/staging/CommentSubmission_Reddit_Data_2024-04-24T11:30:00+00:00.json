[{"comment_ID": "l15aqbz", "comment_Body": "Start with collecting AWS certificates. They really help you with promotion and moving jobs.", "comment_Score": 1, "comment_Author": "Historical-Papaya-83", "comment_Link_id": "1cc6dnp", "Create Date": "2024-04-25 02:56:53+00:00"}, {"comment_ID": "l15ahfb", "comment_Body": "Try Iceberg! It is a new gen tech for a reason. Much faster, easy to deploy.", "comment_Score": 1, "comment_Author": "Historical-Papaya-83", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 02:55:04+00:00"}, {"comment_ID": "l15a8gd", "comment_Body": "Thirded (fourthed?) they are very collaborative and their tools have been solid. They are constantly improving these tools and I genuinely enjoy interactions with employees from that company.", "comment_Score": 1, "comment_Author": "Comprehensive_Tone", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:53:16+00:00"}, {"comment_ID": "l15a568", "comment_Body": "The production application that is the source of the transactions in your transactional database.", "comment_Score": 1, "comment_Author": "mmcalli", "comment_Link_id": "1ccfe3q", "Create Date": "2024-04-25 02:52:37+00:00"}, {"comment_ID": "l15a47v", "comment_Body": "You wanting to learn more is a good first step! It is common that we tend to be bound by whatever tech stack our company uses. But newer the tech, easier it gets to learn(usually). So start studying now, you will be fine.", "comment_Score": 1, "comment_Author": "Historical-Papaya-83", "comment_Link_id": "1ccd1ep", "Create Date": "2024-04-25 02:52:25+00:00"}, {"comment_ID": "l15a18r", "comment_Body": "Nerd is a term of endearment.  Been working with technical folks right out of college and have enjoyed my time with them.", "comment_Score": 1, "comment_Author": "babaisfun", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:51:50+00:00"}, {"comment_ID": "l159rvj", "comment_Body": "What do you like about them?", "comment_Score": 1, "comment_Author": "babaisfun", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:49:56+00:00"}, {"comment_ID": "l159qq4", "comment_Body": "What do you like most about these two?", "comment_Score": 1, "comment_Author": "babaisfun", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:49:42+00:00"}, {"comment_ID": "l159pez", "comment_Body": "That\u2019s kind of what sigma computing is trying to do. There is some buzz about them in the analytics/BI community. I haven\u2019t had a chance to try them yet, but plan to", "comment_Score": 1, "comment_Author": "RandomRandomPenguin", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 02:49:27+00:00"}, {"comment_ID": "l159p09", "comment_Body": "Love this feedback.  Exactly what I am looking for.", "comment_Score": 1, "comment_Author": "babaisfun", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:49:22+00:00"}, {"comment_ID": "l159n09", "comment_Body": "Stealing this.", "comment_Score": 1, "comment_Author": "babaisfun", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:48:58+00:00"}, {"comment_ID": "l159mlg", "comment_Body": "The best way of doing this IMO when fetching from an API is to do:\n\nCloud Function -> PubSub -> BQ with PubSub schema and automatic subscription.\n\nSo it's similar to your approach, it just removes a lot of the fluff and complexity while keeping the flexibility you need.\n\nThis gives many benefits not least schema migration and evolution capabilities ootb as it's basically writing the output to a json field in BQ.\n\nThen you can use whatever you want to handle ETL (DBT I would recommend) from this source table (basically a json field in BQ) as your raw layer to create the other layers in BQ directly using CTAS, this is the most efficient way.\n\nUse simple scheduling or Prefect to Orchestrate the API and DBT and voila \ud83d\udc4c", "comment_Score": 1, "comment_Author": "voycey", "comment_Link_id": "1cc0dx6", "Create Date": "2024-04-25 02:48:53+00:00"}, {"comment_ID": "l159kxf", "comment_Body": "Have spoke to some recruiters and this is a better answer then most they get haha.", "comment_Score": 1, "comment_Author": "babaisfun", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:48:33+00:00"}, {"comment_ID": "l1597n4", "comment_Body": "ORC is faster on Trino than Parquet (or at least it was a couple of years ago), so I tended to do most of my stuff on ORC.\n\nParquet is the standard and if you create the files correctly for the system and storage you are using is pretty much as fast as it comes and has widespread support.\n\nI tend to use Iceberg + Parquet for most things now but CSV won't ever truly disappear I don't think", "comment_Score": 1, "comment_Author": "voycey", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 02:45:55+00:00"}, {"comment_ID": "l1597ju", "comment_Body": "Thanks!  If I am going to sell I'd rather do it to some happy prospects/customers.", "comment_Score": 1, "comment_Author": "babaisfun", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:45:53+00:00"}, {"comment_ID": "l158hha", "comment_Body": "Congrats on the new role. Transitioning from Data Analysis to Data Engineering can be a solid move. Keep your skills sharp and stay updated with the latest tech in your field.\n\nBy the way, I used First 2 Apply when I was job hunting. It streamlined the process by monitoring job sites and sending notifications for new listings. Helped me cut down on the time I spent searching for jobs manually.\n\nGood luck with your new position. Keep learning and adapting.", "comment_Score": 1, "comment_Author": "Oblong_Mink", "comment_Link_id": "1camjik", "Create Date": "2024-04-25 02:40:45+00:00"}, {"comment_ID": "l158bkv", "comment_Body": "Ok thanks.  Can shoot you a pm and talk next steps.  Appreciate the reply.", "comment_Score": 1, "comment_Author": "reelznfeelz", "comment_Link_id": "1cca25z", "Create Date": "2024-04-25 02:39:36+00:00"}, {"comment_ID": "l1589d1", "comment_Body": "Thanks.  Will send a PM to ask more about your experience.  Much appreciated.", "comment_Score": 1, "comment_Author": "reelznfeelz", "comment_Link_id": "1cca25z", "Create Date": "2024-04-25 02:39:10+00:00"}, {"comment_ID": "l157qg4", "comment_Body": "why walmart when cvs is right there?", "comment_Score": 1, "comment_Author": "thomasutra", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 02:35:28+00:00"}, {"comment_ID": "l157hei", "comment_Body": "How big is the application, and is there any opportunity to separate it into independent projects? We ran into issues with parallel development and I split up our warehouse to be separate projects per table, working out the interdependencies. \n\nIs there any opportunity to enable local running of the pipelines? Or at least running in the feature branch? It seems like you have some issues with development environments to work out.", "comment_Score": 1, "comment_Author": "kaji823", "comment_Link_id": "1cbzuk2", "Create Date": "2024-04-25 02:33:43+00:00"}, {"comment_ID": "l156vbt", "comment_Body": "Learn about dbt and databricks", "comment_Score": 1, "comment_Author": "SophisticatedFun", "comment_Link_id": "1ccfhl8", "Create Date": "2024-04-25 02:29:32+00:00"}, {"comment_ID": "l156ht0", "comment_Body": "Databricks or Snowflake", "comment_Score": 2, "comment_Author": "WilhelmB12", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:27:00+00:00"}, {"comment_ID": "l156fye", "comment_Body": "Ironic username mate", "comment_Score": 7, "comment_Author": "pm_me_data_wisdom", "comment_Link_id": "1ccd1ep", "Create Date": "2024-04-25 02:26:39+00:00"}, {"comment_ID": "l1569j3", "comment_Body": "TL;DR: It's distributed workload.\n\nPicture your database like one of those tiny little coffin apartments in New York. If you want to use your kitchen, you have to fold up the bed and pull out the stove, but then you try to go to the bathroom so you have to drop your stove into the floor and lower the toilet from the ceiling.\n\nTechnically it works, but you are expecting the same space to perform multiple jobs when each should have their own. Imagine you really have to take a shit, but there's something cooking in the oven so you have to wait until it's done before you can stow the stove to use the toilet, otherwise you might ruin dinner or start a fire.\n\n-------------------------------\n\nYour transactional database is doing a lot of work managing those transactions coming in, between the CrUD operations, it's juggling all of the indexes, which have to be rebuilt every time a record is changed.\n\nWhen you try to read from it as well, especially constantly for something like a dashboard, you end up with bad performance as data in and data out fight for priority -- in extreme cases, you can chew up all of the memory and freeze everything out, especially if you have a very cheap setup and are not good at writing hyper efficient queries (No, I'm not bitter about my old employer's database).\n\nAnything that interferes with a transactional database should be extremely concerning -- missed inventory events, trades, sales, etc can be a nightmare to fix.\n\nA data warehouse reduces this load by quickly extracting data at low points, and focusing on speed and efficiency above all else. Because of this, there is bandwidth available for making the data cleaner and more lightweight and closer to what you need in your final reports. Not only that, if you have 10-100-1000 users running reports, they are all pulling from the dedicated data warehouse That is tuned to handle that load and isn't competing with constantly shifting indexes and processing all of the other transactions taking place across a huge number of tables.\n\nA data mart is just a stupid industry term for a view or stored procedure or table or some other form of curated final dataset. The data being served here already has all calculations, aggregations, and joins done so the reporting system that uses it doesn't have to do much work.\n\nThe point is, rather than straining your production database which could be catastrophic (and expensive), you spread the work out to an analytical database with a much lower overhead.", "comment_Score": 2, "comment_Author": "MakeoutPoint", "comment_Link_id": "1ccfe3q", "Create Date": "2024-04-25 02:25:26+00:00"}, {"comment_ID": "l155j8z", "comment_Body": "Bro didn't read the post \ud83e\udd23", "comment_Score": 1, "comment_Author": "IAMHideoKojimaAMA", "comment_Link_id": "1ccd1ep", "Create Date": "2024-04-25 02:20:34+00:00"}, {"comment_ID": "l1552sz", "comment_Body": "I mean isn\u2019t chatgpt just trained on a bunch of semi-competent blogs you wish were bullet points?  Or is it just me.", "comment_Score": 1, "comment_Author": "mossheadstone", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:17:32+00:00"}, {"comment_ID": "l154wcy", "comment_Body": "Annoyed isn\u2019t what I am. It\u2019s more depressing just watching this sub\u2019s \u201ccontent\u201d quality take a nose dive", "comment_Score": 3, "comment_Author": "picklesTommyPickles", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 02:16:20+00:00"}, {"comment_ID": "l154sg4", "comment_Body": "I inherited, but actually really like DOMO.  It\u2019s not quite data eng.  It\u2019s pretty much drag and drop etl (but well done).  Haven\u2019t loved my sales reps.  It\u2019s a but vendor-lock-in-y (but what isn\u2019t, aside from stuff that has super high TCO because you need amazing people on staff or the companies might go bust).\n\nBut with all that said, I\u2019ve done analytics, data engineering, data science, at a large telecom, a Silicon Valley 1,000 person company, a Google.org-funded nonprofit\u2026 DOMO would have been better tech selection for all the teams and applications I was on.  Except maybe the thing we home-grew at the one place ;)", "comment_Score": 1, "comment_Author": "mossheadstone", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 02:15:36+00:00"}, {"comment_ID": "l154qqb", "comment_Body": "Dataproc is of course \"managed\" Spark environment in the sense that it handles a lot of the heavy lifting for you. You don't need to worry about setting up and maintaining the underlying infrastructure like VMs, YARN, or HDFS.", "comment_Score": 1, "comment_Author": "Rude-Veterinarian-45", "comment_Link_id": "1c9zfu7", "Create Date": "2024-04-25 02:15:17+00:00"}, {"comment_ID": "l154pnr", "comment_Body": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "comment_Score": 1, "comment_Author": "AutoModerator", "comment_Link_id": "1ccghx9", "Create Date": "2024-04-25 02:15:05+00:00"}, {"comment_ID": "l153r39", "comment_Body": "You make me want to laugh and cry", "comment_Score": 3, "comment_Author": "coolnameright", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 02:08:47+00:00"}, {"comment_ID": "l153kvm", "comment_Body": "String schema parquet unless you know what you\u2019re doing\u2026.what are you doing dude.", "comment_Score": 1, "comment_Author": "OMG_I_LOVE_CHIPOTLE", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 02:07:39+00:00"}, {"comment_ID": "l153b5g", "comment_Body": "Yes you write a new parquet file instead of mutating it. If you need to update parquet consider delta", "comment_Score": 1, "comment_Author": "OMG_I_LOVE_CHIPOTLE", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 02:05:54+00:00"}, {"comment_ID": "l1533ad", "comment_Body": "Agree. Setting up 2 identical environments might not always be possible. We have a custom Astronomer Airflow image that scans the entire GCS bucket (configured in the image) to identify Airflow DAGs and their configs automatically and show them in the UI. So we sometimes re-use this same gcs bucket by creating separate directories with ACL(fine grained access) where setting up a new bucket is not an option - which saves some cost. \n\nBut the advantages that we saw with this strategy (Reduced Downtime,Improved Quality, Increased Confidence,Reduced Rollback Costs) overcome it's infra costs. If infra cost is an issue then using Docker containers could be an option!", "comment_Score": 1, "comment_Author": "Rude-Veterinarian-45", "comment_Link_id": "1cbzuk2", "Create Date": "2024-04-25 02:04:28+00:00"}, {"comment_ID": "l152yzd", "comment_Body": "Parquet for everything unless I need a static mapping in a code repository. Csv is good for that", "comment_Score": 1, "comment_Author": "OMG_I_LOVE_CHIPOTLE", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 02:03:39+00:00"}, {"comment_ID": "l152txx", "comment_Body": "What are live production apps?", "comment_Score": 2, "comment_Author": "Timely-Dimension9569", "comment_Link_id": "1ccfe3q", "Create Date": "2024-04-25 02:02:43+00:00"}, {"comment_ID": "l151vsb", "comment_Body": "try storing a quoted string in a csv and importing it seamlessly everywhere.", "comment_Score": 1, "comment_Author": "mamaBiskothu", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 01:56:33+00:00"}, {"comment_ID": "l151ngn", "comment_Body": "You lose street cred for not using ISO 8601", "comment_Score": 1, "comment_Author": "metaconcept", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 01:55:02+00:00"}, {"comment_ID": "l151m3n", "comment_Body": "Because you may have live production apps attached to the db and users need it to not fall to shit because an inter did select * and 6 full outer joins\u2026", "comment_Score": 2, "comment_Author": "puttyarrowbro", "comment_Link_id": "1ccfe3q", "Create Date": "2024-04-25 01:54:47+00:00"}, {"comment_ID": "l151he7", "comment_Body": "Ideas are dime a dozen, execution and domain expertise is what matters. And even if people get ideas through a public post why are you getting annoyed? Contribute or scroll.", "comment_Score": -1, "comment_Author": "glinter777", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 01:53:56+00:00"}]