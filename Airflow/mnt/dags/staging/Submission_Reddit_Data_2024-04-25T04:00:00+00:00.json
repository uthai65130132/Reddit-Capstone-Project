[{"submission_ID": "1ccian9", "submission_Title": "Parquet Format using for logs storage?", "submission_Score": 3, "submission_Author": "sebastiandang", "submission_selftext": "Hi, Is it common/good approach when I create/write logs with a minimum *low spec SparkApplication. Then store it in Parquet format and transfer it in to Bronze Layer?\nWhat do you think about this approach? Please let me know!", "Create Date": "2024-04-25 03:48:01+00:00"}, {"submission_ID": "1cchtjk", "submission_Title": "Enterprise ETL Tool recommendation. ", "submission_Score": 1, "submission_Author": "sidy66", "submission_selftext": "Hi All,\nI am an experienced ETL developer with 4 years of experience in Ab Initio. Due to some circumstances, I had to work mainly on SQL and pandas only for last 2 years and lost touch with Ab Initio. Now I feel like I have to start from the scratch. Also as companies are moving away from costly tools like Ab Initio and Informatica and the trend changed due to modern data lake architecture\u2026  What would be the one enterprise level ETL tool that you will recommend for learning to build data pipelines in 2024 at least for doing the EL in data integration. \n\nThanks!", "Create Date": "2024-04-25 03:22:04+00:00"}, {"submission_ID": "1ccfhl8", "submission_Title": "On-Prem trying to get cloud experience. What kind of cloud projects for realistic data engineer cloud experience?", "submission_Score": 5, "submission_Author": "findingjob", "submission_selftext": "I am currently a data engineer but mainly do on-premises work with legacy infrastructure (Autosys as our scheduler, IBM tools for Ci/Cd). We move data and process them using stored procedures, Linux, and PowerShell. I think these are dead-end technologies and would like to learn cloud development to apply to other jobs. \n\nMy company is in a hybrid model, so some teams are working with cloud services. Fortunately, I have access to the tools we use: GCP BigQuery and Gcp Composer (Airflow scheduler). \n\nI\u2019m currently dabbling with doing simple things like creating airflow DAGs on simple tasks like creating and deleting tables and copying data from one table to another. \n\nMy question is: \n\nDo you know of good projects I can do to get the concept down of these cloud tools, enough to put it on my LinkedIn? I won\u2019t have any professional projects so I\u2019ll have to create them myself. \n\nI want to do projects so I feel confident enough to apply for jobs using these technologies. I am currently just creating tables, copying, inserting, etc all using Python and big query.\n\nWhat kind of projects should I be doing to feel like I\u2019m getting a good grasp of real data engineering on the cloud? \n\nThank you in advance!", "Create Date": "2024-04-25 01:26:34+00:00"}, {"submission_ID": "1ccf7nm", "submission_Title": "Apache Beam - Pipeline unlimited input, limited output?", "submission_Score": 2, "submission_Author": "colour_doesnt_matter", "submission_selftext": "Hi, data engineers - I'm new to Apache Beam and I'm working out how it might apply to an unusual use case...\n\nLet's say I'd like to run a pipeline that will send exactly 10,000 email alerts (or as many as it can before running out of data), with the addresses to attempt stored in a BigQuery table. Sensibly, you would do this by just querying for 10,000 rows so your input PCollection had the correct number, and push them all through the pipeline.\n\nThe hangup is that in this situation, I can't tell whether an email can count as successful until after it's attempted to process. If processing didn't happen (because the email was on a blocklist, etc) I'd like to keep going until the maximum of 10,000 has been fulfilled.\n\nTo do this I think I need to do a couple of things:\n\n* Keep track of the number of successful processes across all the workers running the job. If we've reached the limit, process no more rows.\n* Pull more rows as needed until the limit is fulfilled (does some sort of streaming arrangement exist in Beam for BigQuery so that a query can pull more rows until the limit has been fulfilled?)\n\nIs Beam a suitable solution for this kind of arrangement, or am I thinking about it wrongly? Thanks for any advice!", "Create Date": "2024-04-25 01:13:09+00:00"}, {"submission_ID": "1ccdveg", "submission_Title": "What\u2019s one problem your organization is willing to spend money to solve?", "submission_Score": 12, "submission_Author": "glinter777", "submission_selftext": "It seems like all companies \u201cright now\u201d are in cost cutting mode. Most are trying to do the best with what they have, surgically hiring / backfilling. I\u2019m curious hear what types of problems / project are actually getting funded. ", "Create Date": "2024-04-25 00:09:20+00:00"}, {"submission_ID": "1ccd6rs", "submission_Title": "An Opinionated Guide to Layering your Analytics Pipelines ", "submission_Score": 3, "submission_Author": "nydasco", "submission_selftext": "I recently published an article on Medium discussing the layers of a reporting pipeline. I discuss the key steps in developing such a pipeline, breaking it down into distinct layers and explaining the processes that occur at each stage. \n\nI thought the folks in this sub might find it interesting. Keen to hear whether you agree or disagree with some of my thoughts.\n\n", "Create Date": "2024-04-24 23:38:04+00:00"}, {"submission_ID": "1ccd1ep", "submission_Title": "Feel like I\u2019m missing out on the big tools and it hurts the potential of my career", "submission_Score": 32, "submission_Author": "somebodyonc3toldm3", "submission_selftext": "I see a lot of topics on this subreddit and online with regards to Databricks, Dbt, snowflake, and kubernetes, for example. I see a lot of topics related to databases as well. I have not worked with any of these and rarely actually do work with databases. Mainly all I do is code with cloud microservices. Has this hurt my future prospects and career development? ", "Create Date": "2024-04-24 23:31:18+00:00"}, {"submission_ID": "1cccjwy", "submission_Title": "SQL preset for Raycast AI", "submission_Score": 1, "submission_Author": "0xIgnacio", "submission_selftext": "I\u2019ve created a Raycast preset for SQL to boost your database work. It can be used on Raycast AI if you have latest version.\n\nSystem instructions:\n\n    Act as a SQL expert. Your answers should include an explanation, SQL language using good practices and highlighting concepts.\n    These are the rules:\n    - Create efficient SQL queries that do not overload the server.\n    - Optimize performance through adjustments to indexes and database structures.\n    - Develop stored procedures and functions to handle complex operations.\n    - Implement SQL scripts that automate routine tasks.\n    - Manage transactions to maintain data integrity.\n    - Debug queries and scripts to fix errors.\n    - Integrate SQL with other programming languages to facilitate the creation of more robust applications.\n\n[Raycast link](https://presets.ray.so/shared?preset=%7B%22creativity%22:%22low%22,%22web_search%22:true,%22model%22:%22openai-gpt-4-turbo%22,%22name%22:%22SQL%20expert%22,%22instructions%22:%22Act%20as%20a%20SQL%20expert.%20Your%20answers%20should%20include%20an%20explanation,%20SQL%20language%20using%20good%20practices%20and%20highlighting%20concepts.%5Cn%5CnThese%20are%20the%20rules:%5Cn-%20Create%20efficient%20SQL%20queries%20that%20do%20not%20overload%20the%20server.%5Cn-%20Optimize%20performance%20through%20adjustments%20to%20indexes%20and%20database%20structures.%5Cn-%20Develop%20stored%20procedures%20and%20functions%20to%20handle%20complex%20operations.%5Cn-%20Implement%20SQL%20scripts%20that%20automate%20routine%20tasks.%5Cn-%20Manage%20transactions%20to%20maintain%20data%20integrity.%5Cn-%20Debug%20queries%20and%20scripts%20to%20fix%20errors.%5Cn-%20Integrate%20SQL%20with%20other%20programming%20languages%20to%20facilitate%20the%20creation%20of%20more%20robust%20applications.%22,%22image_generation%22:true%7D)", "Create Date": "2024-04-24 23:09:43+00:00"}, {"submission_ID": "1ccaaih", "submission_Title": "Delta format merge into question", "submission_Score": 3, "submission_Author": "DataDarvesh", "submission_selftext": "I am querying the source table with a filter greater than the last\\_update\\_time. My source (update) df has 940 distinct (deduped) rows (Databricks). I am merging into the target table (delta format) with when matched on the key, update set \\* and when not matched insert \\*. My target table does not have duplicates. 633 rows are matching. When I look at the Operation Metrics (in Databricks) of the target table on the \"merge\" operation, I see that 633 rows have been matched and updated, and 374 rows have been inserted, and the source df rows are 940. But 633 + 374 = 1007. Shouldn't my updated and inserted rows sum up to 940? What are those extra 67 rows? ", "Create Date": "2024-04-24 21:33:00+00:00"}, {"submission_ID": "1cca25z", "submission_Title": "Airbyte guru wanted", "submission_Score": 1, "submission_Author": "reelznfeelz", "submission_selftext": "I've got like 3 projects that require building or troubleshooting custom airbyte connectors.  I'm having a heck of a time.  If somebody has either mastered the UI Builder or worked with the CDK enough to be pretty comfortable with developing in it, hit me up and I'll pay you for a couple hours of assistance/mentoring.\n\nNot looking for somebody to do it for me, but rather just do a couple pairing sessions and see if I can get unstuck on a couple things.  And I don't expect somebody to do it for free. ", "Create Date": "2024-04-24 21:23:34+00:00"}, {"submission_ID": "1cc9hjf", "submission_Title": "Resources for Data Migrations", "submission_Score": 2, "submission_Author": "thisisnice96", "submission_selftext": "Hey everyone,\n\nI\u2019m seeking some advice and resources on data migrations as I transition into a potential data engineering role. My first project might involve migrating data, possibly from legacy systems to the cloud (AWS or Snowflake). While I don\u2019t have all the details yet, I want to be fully prepared for this task.\n\nAre there any recommended books, courses, or resources that cover the essentials of data migrations? I\u2019m particularly interested in learning about the staple steps, writing test cases, and ensuring a smooth transition of pipelines into the new destination.\n\nIt seems like there\u2019s a wealth of resources in the data world, but I\u2019ve found that information on data migrations is somewhat lacking. Any advice or pointers would be greatly appreciated. Thanks in advance!", "Create Date": "2024-04-24 21:00:25+00:00"}, {"submission_ID": "1cc9gh3", "submission_Title": "What data engineering product are you most excited to buy? Unemployed sales rep looking for the right company to work for.", "submission_Score": 30, "submission_Author": "babaisfun", "submission_selftext": "I know this is off topic but wanted to go to the source (you nerds).  \n\nI was laid off my Enterprise sales job late last year.  Have found myself wanting to jump into a role that serves data engineers for my next gig.  I have done a bit of advisory/consulting around DE topics but did not spend 100% of my time consulting in that area.\n\nCompanies like Monte Carlo Data, Red Panda, Grafana, and Cribl all look to be selling great products that move the needle in different ways.\n\nAny other products/companies I should be looking at? Want to help you all do your jobs better! ", "Create Date": "2024-04-24 20:59:14+00:00"}, {"submission_ID": "1cc89t4", "submission_Title": "managing dags with airflow", "submission_Score": 1, "submission_Author": "nelzon421", "submission_selftext": "Hi guys I recently started testing out airflow and I want to know if there is an easy way to handle all the dags with github. I only came across answers where you have one repo, but that's not what I want. I want to be flexible in my workflow where I can have different projects running in on airflow instance. \n\nDo you know of any good tips or trick, lmk!", "Create Date": "2024-04-24 20:11:22+00:00"}, {"submission_ID": "1cc83af", "submission_Title": "Frontend vs Backend", "submission_Score": 2, "submission_Author": "Emotional_Key", "submission_selftext": "My understanding of these terms is that a Frontend DE is dealing with the visualisation part(Building Reports, Dashboards, etc.) and the Backend DE is dealing with preparing the data to be visualised.\n\nBut are these 2 roles actually separated or a data engineer is supposed to know and do both? \n\nI am lacking on the Frontend part, and I am not really enjoying building SSRS reports and PowerBI dashboards.\n\nI want to understand if I should focus on it, or if I can live happily in my Backend world.\n\n", "Create Date": "2024-04-24 20:04:12+00:00"}, {"submission_ID": "1cc79rr", "submission_Title": "BS-Free Guide to Dominating the Movie Data Modeling Challenge\u2014and Beyond!", "submission_Score": 3, "submission_Author": "JParkerRogers", "submission_selftext": "With my Movie Data Modeling Challenge officially underway, I released a blog packed with insights and proven strategies designed to help data professionals dominate not only this challenge, but any data project.\n\nAll insights are drawn from extensive discussions with top performers from my recent NBA Data Modeling Challenge. They told me what works, and I just took notes! \ud83d\udcdd\n\nSneak peek of what you'll find in the blog:\n\n**A Well-Defined Strategy:** Master the art of setting clear objectives, formulating questions, embracing the 'measure twice, cut once' approach, and effectively telling stories with data.\n\n**Leveraging Paradime:** Learn how to maximize Paradime's robust features to enhance your analytics engineering productivity and streamline your SQL and dbt development processes. (This tool is required in the challenge)\n\nWhether you're aiming to dominate the Movie Data Modeling Challenge or seeking to refine your techniques in data projects, these insights are invaluable.\n\n[Dive into the full blog here!](https://www.paradime.io/blog/winning-strategies-movie-challenge)", "Create Date": "2024-04-24 19:31:33+00:00"}, {"submission_ID": "1cc6luv", "submission_Title": "Open Source SQL Databases - OLTP and OLAP Options", "submission_Score": 1, "submission_Author": "Data-Queen-Mayra", "submission_selftext": "Are you leveraging open source SQL databases in your projects?\n\nCheck out the article here to see the options out there: [https://www.datacoves.com/post/open-source-databases](https://www.datacoves.com/post/open-source-databases)\n\nAny experiences or questions about integrating these technologies into your tech stack would be appreciated! ", "Create Date": "2024-04-24 19:04:35+00:00"}, {"submission_ID": "1cc6dnp", "submission_Title": "What are steps a graduating college student can take to become a data engineer?", "submission_Score": 0, "submission_Author": "CupNo141", "submission_selftext": "I am graduating with a degree in Information Science from a decently well known university. I was interning as data analyst last summer and that was when I first got introduced to the concept of data engineering and learned more about the career and job functions. However, I was not offered a return offer and I have been struggling to find a job that is data related. \n\nI just completed a final round for a small finance company for a business analyst role, but the role is not really that technical. I've taken Intro programming classes and Data Structures and Algorithms, couple database classes, and a machine learning class, so I would say my programming skills are intermediate. I also obtained experience with Databricks, GCP, and SQL through my internship. I just feel extremely lost on what I should be doing to prepare myself to become a Data Engineer, especially if I end up working this non-technical role. Any advice is greatly appreciated.", "Create Date": "2024-04-24 18:55:36+00:00"}, {"submission_ID": "1cc5mfc", "submission_Title": "Seeking expert advice for a Data Project conundrum", "submission_Score": 1, "submission_Author": "Sorry-Concentrate580", "submission_selftext": "Calling all Data Engineers!:\n\nI'm in the process of setting up a table in my AWS RDS, which serves as a crucial data source for my BI tool. As part of the ETL process, I'm consolidating data from multiple tables into a single materialized view, then transforming it into a table (prod\\_table\\_temp), dropping the existing prod\\_table, and finally renaming prod\\_table\\_temp to prod\\_table.\n\nHowever, I'm aware this approach has its drawbacks. Is there a more efficient way to handle this process, considering our current data store is AWS RDS?\n\nLooking forward to your insights", "Create Date": "2024-04-24 18:25:13+00:00"}, {"submission_ID": "1cc5l23", "submission_Title": "Google Search Parameters (2024 Guide)", "submission_Score": 2, "submission_Author": "softcrater", "submission_selftext": "", "Create Date": "2024-04-24 18:23:42+00:00"}, {"submission_ID": "1cc3zjf", "submission_Title": "Dynamic SQL in Postgres", "submission_Score": 3, "submission_Author": "yoquierodata", "submission_selftext": "I\u2019ve got a use case where I have a table of \u201cconfigurations\u201d by ID and another table that holds the base data. The configurations table has an ID along with a string column which is a WHERE clause. My objective is to produce one table with the ID plus the results of a query based on the configuration.\n\nConfig Table \n\n|ID|CONFIG|\n|:-|:-|\n|ABC123|(region=\u2018A\u2019 and segment in (\u2018s1\u2019,s2\u2019))|\n|||\n\nBase Data Table \n\n\n\n|Region|Segment|Customer Type|\n|:-|:-|:-|\n|A|S1|T1|\n|B|S1|T9|\n\n\n\nWhen we did this in Snowflake and DBT we used a Jinja loop to build a SQL statement comprised of UNION statements for each ID. Now that we have thousands of ID values we are nearing the upper limit for the size of a single SQL statement/script. Now we want to port this to Postgres for a semi unrelated reason.\n\nIs porting this over to a Stored Proc that would be called for each ID the *only* solution here? Obviously performance is going to be a big factor, but I am struggling to come up with an alternative solution for the problem of dynamic SQL queries.\n\nTIA!", "Create Date": "2024-04-24 17:21:56+00:00"}]