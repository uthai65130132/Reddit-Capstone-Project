[{"submission_ID": "1cd8xbl", "submission_Title": "Data Catalogue Buy In", "submission_Score": 1, "submission_Author": "InsightByte", "submission_selftext": " Just delivered a new Data Platform with massive success.\n 36 months of hard work, within a global team spreading across multiple timezone.\n \n After delivery, we got huge positive feedback but we also got overwhelmed with question around provenance, glossary info, metadata desc, process flow, freshess, etc \n  Were data consumers want to plug in there process into our data streams and consume them, all in all fair questions.\n\n So that is the problem.\n\n Solution Data Catalogue solution.\n\nMe as a definitive geek, went and implement one as POC.  In this case  I used datahub. \n\nIssue i have. \nGetting pushback from leadership on this, and i am super confused about it.\nThey prefer doing lineage in drawio in confluence.\nTransformation mapping in excell in some obscure sharepoint folder.\nAnd the governance manager does not even know tools like datahub, open metadata, Amundsen even exists... wtf.\n\nI love where i work but this kinda raised an alarm. \n\nAnyway,  i still wanna add Datahub to our stack and prove is worth it. \n\nHow woukd you tackle this?", "Create Date": "2024-04-26 01:33:07+00:00"}, {"submission_ID": "1cd8sem", "submission_Title": "Query engine to combine Graph and NoSQL Data", "submission_Score": 1, "submission_Author": "Hephaestite", "submission_selftext": "I'm essentially looking for a way to run SQL like queries across our data, that consists of graph data in neo4j (a representation of users, groups they belong to, etc) and MongoDB data that contains richer data about those graph nodes.\n\nWe want to be able to design queries that for example build a list of Users based on conditions that might exist in either the graph or the mongo data store. As a software engineer I'm a bit out of my depth when it comes to data engineering.\n\nOpen question, is there something out there that will let us achieve this? or do I need to start considering building a relational database representation of our data? I'm a bit reluctant to go down the traditional relational SQL avenue because the data structures and types of data we are storing are likely to change quite a bit over time and possibly from customer to customer.", "Create Date": "2024-04-26 01:26:30+00:00"}, {"submission_ID": "1cd8bs7", "submission_Title": "Noob learning resources ?", "submission_Score": 1, "submission_Author": "rantedranter", "submission_selftext": "Hi everyone, complete Newbie to data Engineering working in a junior role. What book recs do you have to master Microsoft SQL specifically. \n\nI\u2019m currently practicing with sqlbolt, watched a bunch of YouTube videos but I think reading, might really help too. \n\nAny recs?\n\nThanks in advance ", "Create Date": "2024-04-26 01:04:33+00:00"}, {"submission_ID": "1cd89wr", "submission_Title": "Data pipeline for a new blogs/ articles notification of Tech blogs", "submission_Score": 1, "submission_Author": "FriendshipEastern291", "submission_selftext": "Hi everyone, I currently want to do a data pipeline for an app that users can insert any news url(tech blog site, news site,...) that they want to get notification if that site upload a new blog. What Im going to do is:\n\n\\- use scrapy to scrape all url that appear on the input url (my scale is going to be 1000s of url a day). Output of 1 scraped url is all the urls that appear on that site(example. Link to home, link to blogs, links to pages,...)   \n\\- after scrape through 1000s of url I will store the output in S3 bucket (output of scraped url), I will process that to get what is the blogs url in the output and I will save it into database Ex. Reddit. 'com has 20 blog links  \n \\- i will run that loop each day to refresh if there are any site that post new blogs and update my database\n\n\\- in addition, i have to serve data for my DS to build a model to recommend users another blogs base on there urls subscription they want to get notified  \nWhat will you do if you approach this problem, is my plan for this problem optimized ?  \nWhat should I do to serve the data for the DS smoothly?  \nAppreciate every advice !", "Create Date": "2024-04-26 01:02:08+00:00"}, {"submission_ID": "1cd6v3c", "submission_Title": "Data Engineering Requirements vs. Data Science - Career Decision", "submission_Score": 7, "submission_Author": "motiontrading", "submission_selftext": "Hi Data Engineers,\n\nI hope to get some insight here as I am making a career transition decision. I have formerly been a systems administrator but I quit because of the on-call requirements. I am currently enrolled in University to get my Data Science degree and I just recently got interested about a bootcamp to become a Data Engineer. My question is do you have to be on-call as a Data Engineer? Asking the bootcamp they told me that most Data Engineer jobs do not require you to be on-call and that you work only during normal business hours.\n\nThanks in advance for your answers!", "Create Date": "2024-04-25 23:58:18+00:00"}, {"submission_ID": "1cd5gzc", "submission_Title": "How should I start?", "submission_Score": 1, "submission_Author": "danyhero101", "submission_selftext": "I recently started getting interested in data engineering. I graduated last year from a good school with a BA in Anthropology.  How should I start studying if I want a job in data engineering? I know a bit of python and have a ccp cert from AWS. Currently learning SQL.", "Create Date": "2024-04-25 22:58:57+00:00"}, {"submission_ID": "1cd4p69", "submission_Title": "How do you handle changes upstream to field names that affect your scripts downstream?", "submission_Score": 3, "submission_Author": "miserablywinning", "submission_selftext": "Hi everyone,\n\nA system migration project I\u2019m working on right now has an issue where changes are being made to field names and shifting fields from one table to another as they continue to design the system while we are building curated datasets/implementing at the same time. \n\nThe problem is, they\u2019ll change a field name from say, field_name_10 to field_name_20, or shift a field from table_4 to table_8, without letting our team or other downstream teams know. As a result, our scripts will fail and we have to find where the change was made to. There are multiple changes being made during the week/month, and we have over 50 tables in our data warehouse so we cannot check every single table for changes. \n\nMy question is, how have some of you dealt with this? How are changes communicated to you? Any particular tools or apps that we can use to help with this? I\u2019m looking for something that can maybe notify us of changes made if it directly affect us/our tables in the DW. \n\nI thought of using the github versioning stuff, but it doesn\u2019t seem like the best option since we aren\u2019t all using the same schemas. \n\n\n", "Create Date": "2024-04-25 22:28:10+00:00"}, {"submission_ID": "1cd4ouc", "submission_Title": "Cloud Function to trigger Cloud composer DAG", "submission_Score": 2, "submission_Author": "brittlet", "submission_selftext": "I need some help in coding/building the following scenario since I'm new to this profession,\n\nI have a DAG which should be triggered when a file is dropped in a GCS bucket folder. This can be achieved by an event driven cloud Function. Is it possible to suppress the firing of the cloud function until the DAG run is complete and make it active once the DAG run is complete. A step by step explanation of the solution will be greatly appreciated.", "Create Date": "2024-04-25 22:27:48+00:00"}, {"submission_ID": "1cd3xyl", "submission_Title": "Sync tables from Mysql to any OLAP", "submission_Score": 4, "submission_Author": "diceHots", "submission_selftext": "Hi everyone, that's say we have some operational database (mysql) and to do some ELT to directly dump the data into a OLAP.  I am not sure what's the proper tool to use here. I have played around with some potential tools here,\n\n* Airbyte (no code solution, it works for how's the scalability?)\n* Flink and flink cdc\n* Meltano (yaml-based but not a lot of people use it)\n* any other recommendation?\n\nJust trying to see the pro and cons here. Thanks", "Create Date": "2024-04-25 21:52:28+00:00"}, {"submission_ID": "1cd3saf", "submission_Title": "Database Access Solution", "submission_Score": 2, "submission_Author": "sdsmith0610", "submission_selftext": "Not sure I'm in the right place, I checked out the database sub but it doesn't seem to be as active.  I started a job a few months (accounting role), in previous places of employment had few problems accessing data via power query for use in excel and for use in power bi.  As background, I know enough basic coding and sql to be dangerous and google is my friend.  Current company uses Sage  with ODBC.  They have the US Sage application and the UK Sage application stored on an Azure cloud server.  Access is via dns router (i think).  To query data and refresh reports I have to use a terminal server and the dns connection is set up as a \"silent\" (??) Connection to the ODBC database, anyone that wants to refresh reports also has to have this access point.  They don't want more than one person doing this as it puts excessive load on the server.  Thus, we are meeting next week to discuss transition to Sage with SQL database, but from what I have heard this will not solve the problems due to the way it is set up.  My question is what other alternatives are there to easily access data without use of terminal server and having the ability for access by multiple people?  Is there a solution that could potentially employ a gateway connection, or a repository that can be auto updated?  I have never had so many problems with accessing data for use in reports.  FYI, our IT group seems to have very little experience in setting this up, so far their solution has been there isnt one and we just can't do it unless we get Sage with SQL.   Any help or solutions I can research would be a lifesaver!!!!", "Create Date": "2024-04-25 21:46:01+00:00"}, {"submission_ID": "1cd2hq4", "submission_Title": "Question regarding relation denormalization ", "submission_Score": 2, "submission_Author": "xyzb206", "submission_selftext": "I was toying around with the stack overflow data dumps ( in my case Law Exchange since it was smaller ) mainly trying to implement TF-IDF in plpgsql. The data came in these big denormalized XML files, and I while its clear why they would decide to denormalize the relations, I also kinda was wondering if its posible to have the cake and both eat it with a implementation like this: \n\n \u2022 Data is stored in a normalized form\n\n \u2022 The database has a materialized view with a query that denormalizes this data into itself.\n\n \u2022 Any read query is sent to the materialized form, where it will be a lot faster to query all the relevant data without extra joins.  \n\n \u2022 Any writes are sent to the Normalized relationships who will perform a easy write that can actually be given efficient Foreign Key constraints, from there a trigger will be fired up after the write and this trigger will modify the materialized view apropriatly ( not rerun the base query but rather insert/update a row )\n\n \u2022 Database backups will truncate the materialized view and due to the normalized data structure backups will become smaller, in the case of a need to restore from backup, the materialized view query can be rerun.\n\nFor me this seems like a sitatuion without a downside,\n\n \u2022 You get smaller file sizes (since for every prod database Im assuming there are at least a few in backup). \n\n \u2022 Intuitive consistency checks that dont need to be implimented in the application layer. \n\n \u2022 A much more logical and easy to work with schema.\n\n \u2022 Slight performance improvments on writes ( if I'm not wrong, since the extra write to the view could eat that, either way for such a read intensive app the benifit is pretty meaningless )\n\n \u2022 Cott is happy\n\nOf course this dosnt change the fact that those guys know what they are doing and probably have a good reason to do it this way, My question is, is such a implementatiom valid? Am I missing something here?\n", "Create Date": "2024-04-25 20:54:59+00:00"}, {"submission_ID": "1cd1ce6", "submission_Title": "Neo4j as a NoSQL database for large data and location data", "submission_Score": 3, "submission_Author": "__bdude", "submission_selftext": "Hi all,\n\nI've been exploring various NoSQL databases that effectively handle large datasets (1 entry per second) and provide robust support for location data and big data sets. Recently, I've been delving into Neo4j, and I got inspired by its graph-based model, which seems to offer significant advantages for complex queries that involve relationships and spatial data.\n\nMy thought is in between neo4j and elastic search. Is the approach of Neo4J a logical one? What are your experiences?\n\nKind regards,\n\n\\_\\_bdude", "Create Date": "2024-04-25 20:11:39+00:00"}, {"submission_ID": "1cd15m2", "submission_Title": "How to leverage my Pyspark skills?", "submission_Score": 5, "submission_Author": "Irachar", "submission_selftext": "I wanna improve my programming skills and to optimize loading large files of data in Pyspark but I feel if you dont work in a company with Databricks, big paid clusters... it's so hard to practice it.\n\n&#x200B;\n\nBut at the same time I need to practice Pyspark for the interviews... what I can do?\n\n&#x200B;\n\nthanks", "Create Date": "2024-04-25 20:04:40+00:00"}, {"submission_ID": "1cd0tmv", "submission_Title": "Easiest and cheapest approach for daily fetch of data", "submission_Score": 9, "submission_Author": "LouisDeconinck", "submission_selftext": "I've written a Python script that pulls data from a JSON API. I would like to automate this so the data gets pulled every day and stored in a SQL database.\n\nAbout 5,000 rows every day.\n\nWhat would be the easiest and cheapest approach to handle this?", "Create Date": "2024-04-25 19:52:21+00:00"}, {"submission_ID": "1cd0tii", "submission_Title": "Easiest and cheapest approach for daily fetch of data", "submission_Score": 3, "submission_Author": "LouisDeconinck", "submission_selftext": "I've written a Python script that pulls data from a JSON API. I would like to automate this so the data gets pulled every day and stored in a SQL database.\n\nAbout 5,000 rows every day.\n\nWhat would be the easiest and cheapest approach to handle this?", "Create Date": "2024-04-25 19:52:14+00:00"}, {"submission_ID": "1cd0r1h", "submission_Title": "I need to optimise my workflow extracting mongo data and uploading it to SQL", "submission_Score": 1, "submission_Author": "Drogen24", "submission_selftext": "The problem:  \nWe receive nightly backups of our data from a third party solution we're using. The backups are mongodumps and we get the zipped up version. It needs to be downloaded and loaded to some form of data warehouse.\n\nThe current solution:  \nUsing data bricks to download, unzip, store in ADLS and then convert bson/json to CSV so data factory can copy the CSVs to SQL server\n\nThis works, but it's expensive and slow, and I want a resilient solution that can eventually integrate with other systems to ingest more data.\n\nI'm thinking of using Cosmos DB as a stop-gap so the conversion to CSV and copy to SQL server can be bypassed. As yet I haven't been able to get data bricks to upload the BSON files stored in ADLS to cosmos.   \nI've tried using both the Spark version and the Python version but they only seem to copy direct from the source MongoDB, and a 'mongorestore' as I can do on desktop doesn't seem possible because Mongo Tools can't be installed on the data bricks cluster.  \nI've tried setting up a data migration service (that I'm hoping can be triggered daily further down the pipeline) but there seems to be issues with RUs and configs that I can't get past.\n\nIs using Cosmos DB as a stop-gap a plausible solution? And if so, what else can I try to get this working?   ", "Create Date": "2024-04-25 19:49:38+00:00"}, {"submission_ID": "1cczo1s", "submission_Title": "Extracting information from a Word Document to CSV", "submission_Score": 1, "submission_Author": "willereid", "submission_selftext": "Sorry if this type of post isn't allowed, mod's can delete if that is the case.  \nI have a problem at my job where I need to manually read microsoft word reports created by consultants and extract their info into a CSV file. I wanted to do this via python and the docx package but this proved difficult as the reports ended up differing too much from report to report to extract them using the same code.   \nI was wondering if it was possible to use a Generative AI tool to automatically extract the information I need into a CSV from instructions. \n\nAre there any recommendations for new tools that can do this job? I would prefer tools that are free/open source but I can pay if there aren't any free options. I know of some tools like Google's Document AI or Amazon Textract but I was wondering if there were others that would work easier/cheaper", "Create Date": "2024-04-25 19:07:57+00:00"}, {"submission_ID": "1ccy0s6", "submission_Title": "Implementing a cloud data architecture (Azure) at a small/medium healthcare NPO.", "submission_Score": 4, "submission_Author": "Special-Salamander10", "submission_selftext": "I'm a Data Analyst/Engineer at a relatively small (500 employee) healthcare organization. At first a large portion of my duties were to write custom SQL queries to extract SSMS data from a 3rd party org that houses our data. This either goes into operational/grant reporting or dashboards in PowerBI.\n\nFrankly, I literally cannot make another dashboard at this point. Call it burn out call it what you will but I find my eyes glazing over whenever someone talks about a shiny custom dashboard that they won't bother to learn to use and will just forget about 20 minutes after I present it. Strangely enough though I'm not sick of SQL and unfortunately what little Python I know is getting rusty.\n\nI approached our CTO about creating a cloud data infrastructure in Azure. We're an NPO so we get $3000 for the platform annually. He pretty much gave me the keys to the kingdom to work up some POC but frankly I don't know where the hell to start.\n\nSo far all I've done is automate a few pipelines to pull/transform data from SSMS with ADF and then use Logic Apps to drop csv in an external SFTP server. I've started curating data marts by creating Azure SQL Db tables to extract and house only our data from the 3rd party org in semi-functional Datamarts but these seem lateral at best.\n\nThere are several external orgs that house data that we don't have access to in our Epic  system/3rd party SSMS and I think extracting this and enriching our data would be a great starting point. I'm trying to build something useful and scalable (while also being a portfolio builder) and really trying to justify using Databricks in some capacity.\n\nIs there anyone that has had this experience or would be willing to give some advice on how to kick this off/where to start. I took a course for the DP-203 exam but all they really did was fellate Synapse which I'm not even about to start using.\n\n  \n\\*I should rephrase this and say a large majority of our data goes into an EHR (Epic) and realistically there are only like 30-40 providers + support staff inputting any data in Epic. The vast majority of of staff are in operations in other capacities that don't have much to do with any data accessibility. Our data team is 3 Analysts.", "Create Date": "2024-04-25 18:07:33+00:00"}, {"submission_ID": "1ccx5yc", "submission_Title": "Databricks - Lakehouse setup - How to organize Unity Catalog + Medallions and ADLS?", "submission_Score": 4, "submission_Author": "Allstarbowser", "submission_selftext": " For my company, I want to build a lakehouse with Databricks in Azure. However, I am not sure how to name all my unity catalogs and organize the dev/prod workspaces. \n\n&#x200B;\n\nMy question to you is: **Am I doing this right?** I'm looking for best practices in organizing my workspace/Databricks structure without having to re-do it in half a year because I missed something important. I'm half-sure why I'm doing it like this and I want to start off correctly.\n\n# Background\n\n* We have two seperate businesses with their own data: Digital and Sales.\n* Currently I have a resourcegroup named rg-lakehouse-dev.\n   * I have a dev storage account in there and a dev Databricks workspace.\n\n**Current setup ADLS:** 4 containers:\n\n* 01-landing (raw data, any format)\n* 02-bronze (raw data to delta)\n* 03-silver (delta tables with better column types etc)\n* 04-gold (end data)\n* Each container has a folder named \"Digital\" and \"Sales\" with external locations linked to each folder in Databricks for Unity Catalog (so 8 external locations - digital & sales, dev & prod, landing + bronze + silver + gold, so **adls-digital-landing** for example, or **adls-sales-silver**)\n\n&#x200B;\n\nhttps://preview.redd.it/tszh22r1wnwc1.png?width=176&format=png&auto=webp&s=482d4a82ce1245e05e4833f46f564d7f9b914d7c\n\n**Current setup DataBricks, dev:** 6 catalogs:\n\n* digital\\_dev\\_bronze\n* digital\\_dev\\_silver\n* digital\\_dev\\_gold\n* sales\\_dev\\_bronze\n* sales\\_dev\\_silver\n* sales\\_dev\\_gold\n\n&#x200B;\n\nhttps://preview.redd.it/sri17z13wnwc1.png?width=189&format=png&auto=webp&s=4c57dc5fee7bf909cde04b6247ff8d929c6920a7\n\nThen I want to fill these catalogs with data for dev. The catalogs for production will not be linked to this workspace account.\n\n&#x200B;\n\nIs this a best-practice approach? Would you advice me to do it differently?\n\n&#x200B;\n\nThanks! Looking forward to your ideas.", "Create Date": "2024-04-25 17:37:46+00:00"}, {"submission_ID": "1ccwob5", "submission_Title": "Faster Postgres Migrations", "submission_Score": 1, "submission_Author": "saipeerdb", "submission_selftext": "", "Create Date": "2024-04-25 16:40:26+00:00"}]