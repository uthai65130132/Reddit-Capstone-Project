[{"submission_ID": "1ce2ojv", "submission_Title": "Delta Live Tables for Gold Layer", "submission_Score": 1, "submission_Author": "Mr_Nickster_", "submission_selftext": "Snowflake just relased clustering support for Dynamic tables. DTs are equivalent of DLTs in Databricks. I know with frequent upserts where older rows are updated, tables tend to get declustered, causing degradation of analytics performance. (Most queries end up doing full or big table scans)  DT Clustering parameter is designecld to solve this issue for Dynamic tables on Snowflake side.\n\nI am curious to see how DLTs deal with declustering effect of frequent updates to old rows. I tried to search but couldn't find any info. Is there equivalent functionality for Clustering for DLTs?", "Create Date": "2024-04-27 01:42:16+00:00"}, {"submission_ID": "1ce0ohq", "submission_Title": "Why do companies use Snowflake if it is that expensive as people say ? ", "submission_Score": 51, "submission_Author": "Normal-Inspector7866", "submission_selftext": "Same as title", "Create Date": "2024-04-27 00:04:14+00:00"}, {"submission_ID": "1cdybju", "submission_Title": "Databricks Architecture: Data Plane and control plane", "submission_Score": 2, "submission_Author": "Extra-Cancel3086", "submission_selftext": "Hi All,  \n\n\nI am seeking explanation and guidance on databricks architecture as a budding data engineer and want to understand the concepts thoroughly!   \nFor the purpose of discussion, let's assume that databricks is deployed on azure and it's for customer A.   \nI have been reading the documentation and have a very conflicting information on where these two components exists for the customer.  \nI am aware that the data plane will reside in customers cloud account or in this case customer A azure account and databricks is tagged to customers azure subscription but where is control plane deployed? Is that managed by databricks and therefore not available in customers azure account?  \n\n\nIf someone can help me with explanation,I would be grateful! Thanks", "Create Date": "2024-04-26 22:17:44+00:00"}, {"submission_ID": "1cdxnbv", "submission_Title": "Two Things to Keep in Mind Before You Start Building Another Database System", "submission_Score": 3, "submission_Author": "dmagda7817", "submission_selftext": "", "Create Date": "2024-04-26 21:48:58+00:00"}, {"submission_ID": "1cdxdh4", "submission_Title": "Databricks Architecture: Control plane and data plane", "submission_Score": 2, "submission_Author": "Extra-Cancel3086", "submission_selftext": "Hi All,  \nI have been reading documentation and going over the databricks partner academy videos, there has been a bit of conflicting information as to where two components exists for say a customer account, that is Control plane and Data Plane?  \nIf we take Azure as the cloud provider in this scenario, are both control plane and data plane in customers azure account managed via subscription or the control plane is managed by databricks with their own subscription ? If so would that mean the control plane is not directly accessible for the customer?  \nBelow are my references:\n\n1. [https://premvishnoi.medium.com/data-engineer-git-versioning-with-databricks-repos-f3b4592e2c92](https://premvishnoi.medium.com/data-engineer-git-versioning-with-databricks-repos-f3b4592e2c92) The below diagram explicitly calls out the Control Plane in databricks -- manage customer accounts, datasets, cluster\n\nhttps://preview.redd.it/xajuhft38wwc1.png?width=1394&format=png&auto=webp&s=8b4682c12100ae6790c0c13d1ac2c4160b12b6f6\n\n\n\nThe above documentation says that control plane is in customer databricks account.  \nCan anyone help me understand the architecture better?  \nThank you!", "Create Date": "2024-04-26 21:37:44+00:00"}, {"submission_ID": "1cdx354", "submission_Title": "Devops Engineer or Data Engineer Career choice", "submission_Score": 6, "submission_Author": "Wild_Manufacturer105", "submission_selftext": "Hi guys, so I've been working as a DevOps engineer for just 2 months now after I graduated last Dec 2023. I find it really challenging, which I like, and it's also quite new to me. Before this, I interned as a Data Engineer and enjoyed it as well. However, after graduating, I couldn't land a job as a DE, but I was lucky to receive an offer as a DevOps Engineer.\n\nNevertheless, I'm still focusing on the Data Engineer side because I've done a lot of projects in DE and even earned a certification in Azure DP-900, with DP-203 pending. Therefore, I feel like I'm wasting the skills I've built for my DE career since my current DevOps Engineer role doesn't involve using the cloud. My company operates everything on-premises, so my daily tasks revolve around Linux, SQL, NoSQL, deploying applications and databases, maintaining servers, cleaning disk space, and troubleshooting operational issues.\n\nDo you guys think I should change jobs after a year and focus on becoming a Data Engineer, or should I continue focusing on DevOps Engineering and try to learn something new, perhaps by getting certifications in Terraform, Kubernetes, Docker, AWS, or Azure? Thanks, guys!!!", "Create Date": "2024-04-26 21:25:46+00:00"}, {"submission_ID": "1cdwlww", "submission_Title": "AI/ML for data engineering ", "submission_Score": 2, "submission_Author": "Jkk_geek", "submission_selftext": "Hey everyone,\n\n I'm gearing up to conduct the first round of screening for a data engineer/scientist position focusing on AI/ML model building for my client. \n\nWhile I'm comfortable with the surface level things, I want to deepen my understanding about model building before the screening. Any suggestions for online courses or sample projects I can dive into within a week? I don't want to start from scratch, just aiming to grasp the workings. Also, I'd appreciate your input on must-ask questions for the screening.\n Thanks!", "Create Date": "2024-04-26 21:06:05+00:00"}, {"submission_ID": "1cdvxow", "submission_Title": "My videography business' file storage is getting really expensive. Any ideas on reducing costs for someone who's not tech-savvy?", "submission_Score": 3, "submission_Author": "PianoCharged", "submission_selftext": "My event videography hobby is transitioning to a full-time profession; and I'm becoming overwhelmed with the simple challenge of data storage. My most popular product is 360\u00b0 video which is livestreamed and concurrently recorded in 11K resolution at 60fps with one to two hours of total footage. The raw files on the camera's SD cards alone are huge. Then I want to keep my post-production work and the final cut. Most projects end up being 1TB to 2TB^(+).\n\nMy Mac Studio's hard drive has 2TB (a regretful underestimate), so I do post-production from a 6TB external SSD. For backup of the external SSD and archival of finished projects, I've been using Dropbox because it's user-friendly, but it's also getting really expensive.\n\nI've looked into other options, but anything significantly cheaper is buggy and clunky, not nearly as user-friendly as Dropbox.\n\nI've thought about just keeping everything local and buying more and more external hard drives as necessary, but (i) this requires renting a climate-controlled storage unit for the backup copies, (ii) I'd still need to sync/backup files in progress, and, years from now when I have petabytes' worth of archives, (iii) I wonder if the hardware would end up costing just as much as a cloud-based service.\n\nAny suggestions on reducing my costs? I'm not particularly tech-savvy, FYI. Thank you in advance!", "Create Date": "2024-04-26 20:38:35+00:00"}, {"submission_ID": "1cdurxf", "submission_Title": "\"Defer to Prod\" concept", "submission_Score": 1, "submission_Author": "Ownards", "submission_selftext": "Hello everyone,\n\nI came across this article which explains the concept of \"Defer to Prod\" in Dbt, but I must say I really don't understand the use-case.\n\nIt says that the feature enables a developer to connect to a production model (environment) without having to build the dependent elements in the development environment :\n\n [More time coding, less time waiting: Mastering defer in dbt | dbt Developer Blog (getdbt.com)](https://docs.getdbt.com/blog/defer-to-prod) \n\nhttps://preview.redd.it/9tder4kjovwc1.png?width=785&format=png&auto=webp&s=390de216188f15e44ddbfdaa034077cedd18a97b\n\nBut what kind of development environment is that ? In all projects I did, the development environment was a copy from the production environment with all dependencies. What is this development environment with a single model\\_f floating in the middle of nowhere? I don't get it ...", "Create Date": "2024-04-26 19:52:35+00:00"}, {"submission_ID": "1cdudj9", "submission_Title": "I don\u2019t know if it\u2019s enough for a first job\u2026", "submission_Score": 4, "submission_Author": "Puzzleheaded-Sun3107", "submission_selftext": "I have 5 years of work experience in civil engineering, did a software engineering bootcamp and have a GIS certificate.\nI learned from the bootcamp that I\u2019m don\u2019t enjoy building UI, backend (build api set up database) was fine and at most making a cool graph with data. My civil engineering (simulation modeling) role gave me the experience in data analysis which I used python to do ETL or a portion of it. Most recently I created 2 ETLs at my last company using purely python pulling data from an api, using database data to query the api, applied web scraping to write to the database (I was not in an IT role so I did not have write access to their database plus the database was huge and very confusing think random column names) and using windows task scheduler to run the script every 5 minutes. Essentially reproducing what I did in a previous work experience using Azure Function App and Logic Apps. Is this enough to get a Data Engineering job or do I need to sit down and pick up AWS? I\u2019d rather learn on the job as I\u2019ve done in previous experiences (I\u2019m getting tired of using my personal time, I don\u2019t mind but I\u2019d like to be more strategic with my learning approach)\n\nAny advice would be welcomed!  ", "Create Date": "2024-04-26 19:35:58+00:00"}, {"submission_ID": "1cds5yo", "submission_Title": "Would it be cringe to connect with people on LinkedIn?", "submission_Score": 8, "submission_Author": "Single-Sound-1865", "submission_selftext": "I don't study cs so I am not in the circles of people in software industry so it would be logical to connect with people in DE in different career levels to make connections and possible future referrals but the thing is these people are complete strangers as we don't have anything in common\n\nI won't contact seniors or HR folks because iam not ready yet ( still learning)", "Create Date": "2024-04-26 18:04:57+00:00"}, {"submission_ID": "1cds2vy", "submission_Title": "What is your favorite Postgres extension and why?", "submission_Score": 1, "submission_Author": "AMDataLake", "submission_selftext": "What are your favorite parts of the Postgres ecosystem", "Create Date": "2024-04-26 18:01:32+00:00"}, {"submission_ID": "1cdrhdu", "submission_Title": "3NF and dimensional modeling ", "submission_Score": 7, "submission_Author": "yeykawb", "submission_selftext": "Why is it always so that these often stand in contrast to each other? When searching for \u201c3NF data warehouse\u201d there is a lot of \u201c3NF vs dimensional\u201d. Why is it so?\n\nWe\u2019re using three layers, landing -> 3NF -> star schema data marts (dimensional modeling). For me they complement each other rather than compete.\n\nAm I completely missing the point?", "Create Date": "2024-04-26 17:37:07+00:00"}, {"submission_ID": "1cdr3zx", "submission_Title": "Reving Warehouse DDL", "submission_Score": 3, "submission_Author": "ReporterNervous6822", "submission_selftext": "Wondering what tooling/solutions others use to manage warehouse schemas? My org was using alembic for a bit but as our warehouse size grows, migrations become expensive in terms of time.  Our current solution is one piece of DDL in git per table that recreates a table from an existing one using a select * into it, and we modify whatever we need to there in terms of new columns or indexes or whatever. This is fine but the worst part is finding a runtime to run migrations that might take 20+ hours\u2026.so what are you all using for migrations/DDL changes on big data warehouses? ", "Create Date": "2024-04-26 17:21:58+00:00"}, {"submission_ID": "1cdr2va", "submission_Title": "Using DBT Expectations for a FK check", "submission_Score": 2, "submission_Author": "slcgayoutdoors", "submission_selftext": "New to dbt:\n\nIe does the value from Table1.Column exist in Table2.Column.\n\nIt doesn't look like dbt expectations has something of that nature? It should be simple to make as a generic, but I was sort of expecting one to exist already and don't see it.", "Create Date": "2024-04-26 17:20:45+00:00"}, {"submission_ID": "1cdqz5r", "submission_Title": "Python vs Bash as a glue language: what are the pros and cons? And which one do you use?", "submission_Score": 5, "submission_Author": "knightfall0", "submission_selftext": "TL;DR: Planning on switching from bash to Python for wrapper scripts around the Pyspark application. What are the pros and cons to this?\n\n\nI'm currently working in a company where we use bash scripts as a wrapper around the Pyspark pipelines. The job scheduling tool invokes a shell script, which does a few operations like date control, configuration setup etc. and then runs spark-submit to run a pyspark pipeline. Finally the bash scripts also catch exit codes and send out emails. \n\nThis works, but it's getting difficult to maintain. We have 150+ lines of bash scripts for each application and I find it extremely difficult to work with. \n\nI've been thinking of working on shifting some of the processing to a python script instead. In my mind, python being a general purpose programming language gives it certain powers that are hard to replicate in bash. Error handling, ease of debugging, and complex conditional statements are a few. \n\n\nMy question is, is there a certain benefit to bash that I'm not seeing? What are the pros and cons of switching from bash to Python?", "Create Date": "2024-04-26 17:16:30+00:00"}, {"submission_ID": "1cdq1cp", "submission_Title": "LLMs are Commoditized; Data is the Differentiator", "submission_Score": 3, "submission_Author": "something_cleverer", "submission_selftext": "", "Create Date": "2024-04-26 16:38:20+00:00"}, {"submission_ID": "1cdodnu", "submission_Title": "Is Snowflake micro-partitions a rebranding or Parquet row-groups?", "submission_Score": 8, "submission_Author": "royondata", "submission_selftext": "One of the big performance benefits of Snowflake is micro-partitions and it's being sold by Snowflake as a major differentiator, compared to configuring static partition keys for tables. But the more I read about it and use it, the more I'm being convinced it's no different than Parquet row-groups.\n\nFrom the Snowflake documentation, the benefits of micro-partitions are:\n\n* In contrast to traditional static partitioning, Snowflake micro-partitions are derived automatically - \\[MY COMMENT\\] in Parquet you also don't partition the data, it's broken into row group and column chunks with lots of metadata/stats\n* As the name suggests, micro-partitions are small in size (50 to 500 MB, before compression), which enables extremely efficient DML and fine-grained pruning for faster queries. \\[MY COMMENT\\] that's simply a row-group size configuration in Parquet and you get the same benefit\n* Micro-partitions can overlap in their range of values, which, combined with their uniformly small size, helps prevent skew. \\[MY COMMENT\\] row-groups in Parquet can be sorted so the values are contiguous across groups. Or if unsorted, they will overlap in value as well.\n* Columns are stored independently within micro-partitions, often referred to as *columnar storage*. This enables efficient scanning of individual columns; only the columns referenced by a query are scanned. \\[MY COMMENT\\] Exactly what Parquet does. row-groups contain column chunks that are stored exactly like this.\n* Columns are also compressed individually within micro-partitions. Snowflake automatically determines the most efficient compression algorithm for the columns in each micro-partition. \\[MY COMMENT\\] Parquet does this by default. row-groups are compressed independently with appropriate metadata so that engines can find the right groups to read based on specific filter criteria.\n\nIt sounds like Snowflake micro-partitions are just a marketing rebranding of a feature that you get for free with Parquet\n\nSo am I missing something here?  ", "Create Date": "2024-04-26 15:31:33+00:00"}, {"submission_ID": "1cdod9o", "submission_Title": "How good programing skills does one need to get into DE? I am pursuing Cloud Computing PG. ", "submission_Score": 0, "submission_Author": "Guilty-Direction-808", "submission_selftext": "I am from non IT background and know Excel, SQL, Data Visualization (currently working on Python skills for analytics). \n\nCurrently pursuing Post Graduation in cloud computing. \n\nI am not sure if I can break into DE field and hence considering getting into DS/AI.\n\nAlso, which complements cloud more - DE or DS/AI.\n\n", "Create Date": "2024-04-26 15:31:07+00:00"}, {"submission_ID": "1cdnvfi", "submission_Title": "HOT updates in PostgreSQL for better performance", "submission_Score": 2, "submission_Author": "theporterhaus", "submission_selftext": "I found a cool little PostgreSQL optimization for update-heavy workloads.\n\nTL;DR HOT updates - you can allow PostgreSQL to update tuples in a block without updating the index AND dead tuples automatically get removed without needing VACUUM.\n\nDownside is you can't then use indexes on the columns that are being updated.\n\nAnyone who has used this technique I would love to hear your experience.", "Create Date": "2024-04-26 15:10:40+00:00"}]