[{"comment_ID": "l1a9pjp", "comment_Body": "Works as designed. Design sucks.", "comment_Score": 1, "comment_Author": "lzwzli", "comment_Link_id": "1ccv2r3", "Create Date": "2024-04-26 01:16:47+00:00"}, {"comment_ID": "l1a90rr", "comment_Body": "Or both", "comment_Score": 1, "comment_Author": "lzwzli", "comment_Link_id": "1ccv2r3", "Create Date": "2024-04-26 01:12:39+00:00"}, {"comment_ID": "l1a7o1i", "comment_Body": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "comment_Score": 1, "comment_Author": "AutoModerator", "comment_Link_id": "1cd8bs7", "Create Date": "2024-04-26 01:04:34+00:00"}, {"comment_ID": "l1a76xv", "comment_Body": "Depends on the team for DE. Sometimes there's on-call but the load won't be as high as Ops / Sysadmin, because you're just responsible for your own infra, not the whole company's infra.\n\nDS analysts generally don't have to be on call since they usually don't write critical production code.", "comment_Score": 1, "comment_Author": "dravacotron", "comment_Link_id": "1cd6v3c", "Create Date": "2024-04-26 01:01:33+00:00"}, {"comment_ID": "l1a6qwo", "comment_Body": "Interested in DE so I really hope so", "comment_Score": 1, "comment_Author": "snailspeed25", "comment_Link_id": "1ccsf39", "Create Date": "2024-04-26 00:58:47+00:00"}, {"comment_ID": "l1a6qpu", "comment_Body": "More face time with leaders who don't have the slightest understanding of the complexity under the hood. All they see is fancy dashboards and presentations and assume it's easy", "comment_Score": 1, "comment_Author": "Hackerjurassicpark", "comment_Link_id": "1ccsf39", "Create Date": "2024-04-26 00:58:45+00:00"}, {"comment_ID": "l1a6l5o", "comment_Body": "Given these requirements and the consensus on postgres as a DB I think Mageai is worth consideration. \n\n1. It's easy to get started as it's just a docker image.\n2. It has a lot of out of the box loaders, transformers and exporters for common tools ie postgres.\n3. You just drag blocks onto the canvas, connect them by dragging lines between blocks and schedule it with a trigger. \n4. They have a pretty good slack community for help and support. It also has a bot that you can ask questions.", "comment_Score": 1, "comment_Author": "wannabe-DE", "comment_Link_id": "1ccs3vs", "Create Date": "2024-04-26 00:57:47+00:00"}, {"comment_ID": "l1a6eb0", "comment_Body": "This is the way. Also doable in Azure and GCP.", "comment_Score": 1, "comment_Author": "corecursion0", "comment_Link_id": "1cd0tii", "Create Date": "2024-04-26 00:56:36+00:00"}, {"comment_ID": "l1a69e6", "comment_Body": "It\u2019s easy if you want to teach English.  If not, not so much.  I make a fair amount more than my wife teaching but we have been here a long time and want to get out.  Thanks", "comment_Score": 1, "comment_Author": "Bare_arms", "comment_Link_id": "1bt7iv2", "Create Date": "2024-04-26 00:55:46+00:00"}, {"comment_ID": "l1a5pbl", "comment_Body": "Also, this thread details how I've circumvented the coupling of both selecting a group of dbt models and defining how they're to be computed.\n\n[https://github.com/dagster-io/dagster/discussions/20761](https://github.com/dagster-io/dagster/discussions/20761)\n\nTo provide an example of the problem:\n\n* you load in a group of upstream models (staging for example) with one dbt\\_assets decorator\n* you load in a second group of downstream models (marts for example) with one dbt\\_assets decorator.\n* If ANY of the upstream models (assets in dagster) fail, NONE of the downstream models (assets) will run\n* This is because each dbt\\_assets decorator constructs only one dbt command to be executed.\n* Thus, with the current dagster\\_dbt API, the dbt command to be executed is not dynamic and cannot subset models to run\n* To circumvent this, I used a asset factory to create one dbt\\_assets decorator per model by parsing the dbt\\_manifest.json manually", "comment_Score": 1, "comment_Author": "coreytrevorlahey69", "comment_Link_id": "1aqvhcj", "Create Date": "2024-04-26 00:52:20+00:00"}, {"comment_ID": "l1a4k0q", "comment_Body": "get out if you can", "comment_Score": 1, "comment_Author": "jaylen_browns_beard", "comment_Link_id": "1ccv2r3", "Create Date": "2024-04-26 00:45:14+00:00"}, {"comment_ID": "l1a47ye", "comment_Body": "Sorry, edited the comment above.", "comment_Score": 1, "comment_Author": "coreytrevorlahey69", "comment_Link_id": "1aqvhcj", "Create Date": "2024-04-26 00:43:06+00:00"}, {"comment_ID": "l1a4691", "comment_Body": "Your contract example is \\*very\\* interesting.  Is that something you've worked with before?", "comment_Score": 1, "comment_Author": "CalendarSpecific1088", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-26 00:42:47+00:00"}, {"comment_ID": "l1a45vh", "comment_Body": "Would love a link to that thread! \n\nEdit: Amazing username BTW", "comment_Score": 1, "comment_Author": "noah-ford", "comment_Link_id": "1aqvhcj", "Create Date": "2024-04-26 00:42:43+00:00"}, {"comment_ID": "l1a40s6", "comment_Body": "Funny you mention.  I've had long arguments with Copilot about how it outright lies describing what my company does, which it apologizes for and then doubles down the mis-statement.\n\nI'll concede effectiveness on tonal language assistance (you can sound much more professional), but that's about it.  In other words, LLM's are good at assisting with the English language (assuming that's your LLM's language); anything else is patchy.  \n\nAI's give you a statement that's statistically likely to be said.  Since the internet has a high degree of bullshit, you can infer the inevitable results.", "comment_Score": 1, "comment_Author": "CalendarSpecific1088", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-26 00:41:48+00:00"}, {"comment_ID": "l1a40hu", "comment_Body": "I've actually made lots of progress with automaterialization. I created this thread yesterday on the dagster github that was quickly answered by a maintainer. \n\n[https://github.com/dagster-io/dagster/discussions/21400](https://github.com/dagster-io/dagster/discussions/21400)\n\nThe thread outlines at a high level how I'm using automaterialization with both non-partitioned and partitioned assets. I actually don't specify anything about automaterialization in the dbt yml files, and entierly handle it within the dagster code base.\n\nI'm also happy to share any knowledge if you have any questions!", "comment_Score": 1, "comment_Author": "coreytrevorlahey69", "comment_Link_id": "1aqvhcj", "Create Date": "2024-04-26 00:41:45+00:00"}, {"comment_ID": "l1a3rtl", "comment_Body": "> C-suite wants some performance and financial data\n\nBefore implementing, some things to consider:\n- how frequently will the data be updated?\n- who is the end consumer of the Data? Usually Finance just wants the raw data and wants to manipulate/chart themselves.\n- what\u2019s the volume of data? \n\nHonestly, from my experience, based on what you said, your implementation seems like overkill. If you wanna develop the skills, go for it. Otherwise, just (export to CSV) and import into Google Sheets. [Here\u2019s a link to SO on how to export](https://stackoverflow.com/questions/8119297/postgresql-export-resulting-data-from-sql-query-to-excel-csv). Just cron this and dump to a shared folder. \n\nIf you have Excel, even better. There are commercial ODBC drivers that will let you connect directly to Postgres.", "comment_Score": 1, "comment_Author": "jawabdey", "comment_Link_id": "1ccs3vs", "Create Date": "2024-04-26 00:40:13+00:00"}, {"comment_ID": "l1a3pw2", "comment_Body": "Can you give an example?", "comment_Score": 1, "comment_Author": "icysandstone", "comment_Link_id": "1ccsf39", "Create Date": "2024-04-26 00:39:52+00:00"}, {"comment_ID": "l1a3m93", "comment_Body": "What\u2019s after DE so I can jump the line?", "comment_Score": 1, "comment_Author": "icysandstone", "comment_Link_id": "1ccsf39", "Create Date": "2024-04-26 00:39:14+00:00"}, {"comment_ID": "l1a3j8k", "comment_Body": "This is somewhat surprising to me. Why do you think DAs get more praise?", "comment_Score": 1, "comment_Author": "icysandstone", "comment_Link_id": "1ccsf39", "Create Date": "2024-04-26 00:38:42+00:00"}, {"comment_ID": "l1a386n", "comment_Body": "Just did auto materialization about 10 minutes ago and was also scared, but it works amazingly. Unfortunately can't speak too much to partitioning. \n\nThe only code change I made was below, but you will also have to turn auto-materialization on in the UI\n\n    ###dbt_project.yml###\n    models:\n      [project_name]:\n        +dagster_freshness_policy:\n            maximum_lag_minutes: !!float 1440 # Every day\n        +dagster_auto_materialize_policy: \n            type: lazy # If upstream asset runs sill wait for the policy and don't run just yet\n\nLet me know if you need an example for specific tables and I can write one up!", "comment_Score": 1, "comment_Author": "noah-ford", "comment_Link_id": "1aqvhcj", "Create Date": "2024-04-26 00:36:44+00:00"}, {"comment_ID": "l1a35z0", "comment_Body": "Thanks. Really appreciate it. I am kinda inclined towards snowflake or red shift for compute with dbt with transformation logic. But at the end of day, the cheapest wins out. Gonna see the which vendor gives better promotion at end of the day. Thank you", "comment_Score": 1, "comment_Author": "diceHots", "comment_Link_id": "1cd3xyl", "Create Date": "2024-04-26 00:36:20+00:00"}, {"comment_ID": "l1a2y7j", "comment_Body": "Oh I completely agree on chat gpt being the handy \u201cask-it\u201d tool. You definitely need to know what to ask it. \nMy disagreement is only on the part of not expecting to be assessed on spark and pyspark knowledge by hands-on exercise. It is getting common in interviews, has been my experience.", "comment_Score": 1, "comment_Author": "barely_functional_de", "comment_Link_id": "1cd15m2", "Create Date": "2024-04-26 00:34:57+00:00"}, {"comment_ID": "l1a1o6b", "comment_Body": "It\u2019s extraordinarily effective, I believe Snowflake does some optimization on their end to save metadata about the json nesting structure that makes it really fast to retrieve fields.", "comment_Score": 1, "comment_Author": "mistanervous", "comment_Link_id": "1ccsms9", "Create Date": "2024-04-26 00:26:55+00:00"}, {"comment_ID": "l1a1bzl", "comment_Body": "I find that platforms focused on data engineering disappoint me when I want to build event-driven, low-latency data pipelines and don't want a crazy rats-nest of inter-pipeline dependencies, and can leverage kubernetes, ecs, lambda, etc for scaling workers.\n\nAlso, most orchestration tools look like hell when I take a look at their data pipelines, how they're scheduled, and how their dependencies are handled.", "comment_Score": 2, "comment_Author": "kenfar", "comment_Link_id": "1ccueqx", "Create Date": "2024-04-26 00:24:43+00:00"}, {"comment_ID": "l1a10h8", "comment_Body": "holy smokes - near best in class to dumpster", "comment_Score": 1, "comment_Author": "CAPHILL", "comment_Link_id": "1ccv2r3", "Create Date": "2024-04-26 00:22:39+00:00"}, {"comment_ID": "l19zz16", "comment_Body": "A good interview isn't necessarily looking for just the right syntax.  That's one of the main pitfalls technical folks fall into - hammering away at one approach they see, without fully addressing the problem.  When I ask a question like that, I'm looking to see how you understand data relationships, are you going to ask a clarifying question like what are you considering as the top, how do you want to handle ties, etc.   If you don't know the answer, the talking it through your problem solving approach is really helpful.  We're going to run into real world problems that we won't know how to solve right away, I want to understand how you think about decomposing something into a way you eventually will be able to solve it.\n\nWindowing questions I think are also a good way to see if someone can visualize how processes can be done within a sub group.  What if I wanted to bucket larger elements, how would you assign buckets without a window function?  Or if you wanted to compare ordered values within a set...  I don't care too much if you know the exact syntax, but that you know how to think in terms of subgroups.", "comment_Score": 1, "comment_Author": "skinnychef312", "comment_Link_id": "1cb2ym3", "Create Date": "2024-04-26 00:15:55+00:00"}, {"comment_ID": "l19z1l9", "comment_Body": "I'll agree that it isn't there for development yet, buts 100% useable for business users to ask business and data questions. Vectorize a repo of contracts and ask questions and statistics about them. Get a BRD and a roles and permissions matrix loaded up and let the knowledge transfer become way easier.", "comment_Score": 2, "comment_Author": "Swirls109", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-26 00:09:51+00:00"}, {"comment_ID": "l19yrks", "comment_Body": "99% of my problems as an employed data engineer have nothing to do with the chosen orchestrator.\n\nat this point debugging issues with dag development/deployment are the most enjoyable parts of my job.\n\nif things are different for you, consider yourself lucky.", "comment_Score": 5, "comment_Author": "zazzersmel", "comment_Link_id": "1ccueqx", "Create Date": "2024-04-26 00:08:02+00:00"}, {"comment_ID": "l19yr5q", "comment_Body": "We\u2019ve been using Mage in production for about 6 months now. We were hesitant as it\u2019s relatively new but we\u2019ve enjoyed it. It has an active community, the devs are quick to respond to any issues we\u2019ve had, and it works well for our use case. However, we don\u2019t do anything particular complicated and we\u2019re an on-prem shop. I\u2019ve not tried deploying it to the cloud and can\u2019t speak to how it functions in a more modern environment.\n\nEdit: I don\u2019t know what an IC Data Engineer is but we found Mage by accident. It wasn\u2019t even on our list to evaluate until I saw a random post on this subreddit. Maybe I fell victim to an influencer campaign but it works so I\u2019m not too mad about it. Our other top choice was Dagster, it seemed very promising as well.\n\nEdit 2: Not sure why the downvotes. Is there something about Mage I don\u2019t know? What\u2019d I miss?", "comment_Score": 0, "comment_Author": "Lord_Lloydd", "comment_Link_id": "1ccueqx", "Create Date": "2024-04-26 00:07:58+00:00"}, {"comment_ID": "l19yk4w", "comment_Body": "Does it though?  My experience with it has been deeply mixed, and the hallucination rate sending younger devs down blind alleys because they lack the chops to call bullshit on responses has been very, very high.", "comment_Score": 1, "comment_Author": "CalendarSpecific1088", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-26 00:06:43+00:00"}, {"comment_ID": "l19y38u", "comment_Body": "This seems more like a problem of using a database directly to do analytics/reporting. Databases are generally used for what is called OLTP (Online Transaction Processing) workloads so they are fast at writing data and fast at reading data according to clearly defined query patterns (i.e. get all transactions for a customer). They can be used for complex queries (which are generally needed for analytics/reporting) but are not designed for it as you may put too much workload onto them, causing issues like the one you described above.\n\nSo most companies move to push this workload onto OLAP (Online Analytics Processing) solutions which are designed for analytics/reporting workloads where complex queries can be executed with multiple users at any one time. Data is usually extracted from the source database and pushed into offline storage (usually into files like Parquet) so that these OLAP solutions read data from the offline storage for the complex queries.", "comment_Score": 1, "comment_Author": "Pitah7", "comment_Link_id": "1cd3saf", "Create Date": "2024-04-26 00:03:46+00:00"}, {"comment_ID": "l19xu2d", "comment_Body": "This.  SQLite, in case you are dealing w/ small sets, is also pretty good at parsing JSON.  Postgres is a joy, though.", "comment_Score": 1, "comment_Author": "CalendarSpecific1088", "comment_Link_id": "1ccsms9", "Create Date": "2024-04-26 00:02:05+00:00"}, {"comment_ID": "l19xo36", "comment_Body": "This approach is far more performant than I believed it could be before I tried it.", "comment_Score": 2, "comment_Author": "CalendarSpecific1088", "comment_Link_id": "1ccsms9", "Create Date": "2024-04-26 00:01:00+00:00"}, {"comment_ID": "l19xg0c", "comment_Body": "Maybe but chatGPT is pretty solid when it comes to actually writing the code, from there it comes down to what to write and how to optimize. Especially true if using pyspark and pandas together. I have witnessed some very complex pipelines that could have been easily simplified if the author actually had a grip on how the processing works under the hood.", "comment_Score": 1, "comment_Author": "Desperate-Walk1780", "comment_Link_id": "1cd15m2", "Create Date": "2024-04-25 23:59:32+00:00"}, {"comment_ID": "l19xdp2", "comment_Body": "This is meant to be ironic.  Please put down the gun.", "comment_Score": 4, "comment_Author": "CalendarSpecific1088", "comment_Link_id": "1cd0tmv", "Create Date": "2024-04-25 23:59:07+00:00"}, {"comment_ID": "l19xaok", "comment_Body": "while \\[ 1 \\]; do ./pullJson.py; sleep 86400; done.", "comment_Score": 6, "comment_Author": "CalendarSpecific1088", "comment_Link_id": "1cd0tmv", "Create Date": "2024-04-25 23:58:36+00:00"}, {"comment_ID": "l19x8zt", "comment_Body": "Are you interested in transitioning into Data Engineering? Read our community guide: https://dataengineering.wiki/FAQ/How+can+I+transition+into+Data+Engineering\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "comment_Score": 1, "comment_Author": "AutoModerator", "comment_Link_id": "1cd6v3c", "Create Date": "2024-04-25 23:58:18+00:00"}, {"comment_ID": "l19wc8t", "comment_Body": "Download docker, pull image, run container and practice in a notebook. \n\nhttps://hub.docker.com/r/jupyter/pyspark-notebook", "comment_Score": 2, "comment_Author": "hblock44", "comment_Link_id": "1cd15m2", "Create Date": "2024-04-25 23:52:26+00:00"}, {"comment_ID": "l19vesz", "comment_Body": "When I was working at a bank, we would have \"contracts\" (some word document that defines how data will flow and what responsibilities each team has including notifying each other of changes) with our upstream/downstream systems. Usually, these are just a formality and a way to blame someone when something goes wrong and we never actually used them for their intended purpose. Us data engineers didn't care either because there wasn't anything in the contract that was actionable or enforcable. This is where the data engineering industry have started to resolve problems like this by defining data contracts where producers and consumers adhere to the what is defined in the data contract. For reference, I'm part of the TSC for ODCS (Open Data Contract Standard https://github.com/bitol-io/open-data-contract-standard) where we are trying to define a standard contract that everyone could understand and use.\n\nThis idea has been in use for a long time already with APIs (OpenAPI standard https://spec.openapis.org/oas/latest.html) where upstream/downstream teams just share OpenAPI/Swagger docs between each other to understand what data is available to use, how to retrieve it and what format it comes in.", "comment_Score": 1, "comment_Author": "Pitah7", "comment_Link_id": "1cd4p69", "Create Date": "2024-04-25 23:46:22+00:00"}]