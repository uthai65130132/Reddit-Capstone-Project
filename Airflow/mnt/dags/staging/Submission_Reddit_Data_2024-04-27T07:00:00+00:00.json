[{"submission_ID": "1ce86hh", "submission_Title": "How to get into Data science with a good package(not that decent) when u're completely new to this field and only have a Bcom degree with IBM data analytics professional certification and 3 years of accounts experience ", "submission_Score": 0, "submission_Author": "d_guy_u_cant_buy", "submission_selftext": "What are you're suggestions?", "Create Date": "2024-04-27 06:54:43+00:00"}, {"submission_ID": "1ce7ly4", "submission_Title": "What do you think about a company using informatica cloud, snowflake , palantir foundry? ", "submission_Score": 2, "submission_Author": "sneekeeei", "submission_selftext": "My client uses all 3 and I don\u2019t understand how does it make sense cost wise. ", "Create Date": "2024-04-27 06:18:46+00:00"}, {"submission_ID": "1cdwj1d", "submission_Title": "Need Advice: \"AI-driven\" Excel Template Converter", "submission_Score": 1, "submission_Author": "whyAlwaysAi", "submission_selftext": "Hi everyone,\n\nFirst of all I hope that is the right subreddit.\n\nI recently graduated and started my first job as a data engineer at a small startup. I've been tasked with developing an \"AI-driven Excel converter.\" The idea is that we have a standardized Excel format for inputs that works well for the product. Now, the goal is to create a system that can take any Excel file, regardless of how the data is structured or formatted (e.g. different column names, unnecessary data in the input, etc), and transform it into our required template format.\n\nThe product owner is really pushing me to use the OpenAI API, believing that their AI capabilities can handle the conversion process. I'm a bit skeptical about relying on AI for this, especially since I'm not very familiar with AI technolgy. Plus, I'm concerned about our ability to handle potentially weird or complex file types that our customers might use.\n\nSo, I'd really appreciate any insights or advice.\n\nDo you think it\u2019s viable to use AI for this kind of data transformation?\nDo you know of any tools that I could use for that conversion instead?\n\nI am a bit concert that the po does not accept any non Ai tooling, I have the feeling that he just wants to stamp ai on the product :/\n\nSo, I am really looking forward to your thoughts. Thanks a lot! :)", "Create Date": "2024-04-26 21:02:50+00:00"}, {"submission_ID": "1ce2ojv", "submission_Title": "Delta Live Tables for Gold Layer", "submission_Score": 1, "submission_Author": "Mr_Nickster_", "submission_selftext": "Snowflake just relased clustering support for Dynamic tables. DTs are equivalent of DLTs in Databricks. I know with frequent upserts where older rows are updated, tables tend to get declustered, causing degradation of analytics performance. (Most queries end up doing full or big table scans)  DT Clustering parameter is designecld to solve this issue for Dynamic tables on Snowflake side.\n\nI am curious to see how DLTs deal with declustering effect of frequent updates to old rows. I tried to search but couldn't find any info. Is there equivalent functionality for Clustering for DLTs?", "Create Date": "2024-04-27 01:42:16+00:00"}, {"submission_ID": "1ce0ohq", "submission_Title": "Why do companies use Snowflake if it is that expensive as people say ? ", "submission_Score": 99, "submission_Author": "Normal-Inspector7866", "submission_selftext": "Same as title", "Create Date": "2024-04-27 00:04:14+00:00"}, {"submission_ID": "1cdybju", "submission_Title": "Databricks Architecture: Data Plane and control plane", "submission_Score": 2, "submission_Author": "Extra-Cancel3086", "submission_selftext": "Hi All,  \n\n\nI am seeking explanation and guidance on databricks architecture as a budding data engineer and want to understand the concepts thoroughly!   \nFor the purpose of discussion, let's assume that databricks is deployed on azure and it's for customer A.   \nI have been reading the documentation and have a very conflicting information on where these two components exists for the customer.  \nI am aware that the data plane will reside in customers cloud account or in this case customer A azure account and databricks is tagged to customers azure subscription but where is control plane deployed? Is that managed by databricks and therefore not available in customers azure account?  \n\n\nIf someone can help me with explanation,I would be grateful! Thanks", "Create Date": "2024-04-26 22:17:44+00:00"}, {"submission_ID": "1cdxnbv", "submission_Title": "Two Things to Keep in Mind Before You Start Building Another Database System", "submission_Score": 3, "submission_Author": "dmagda7817", "submission_selftext": "", "Create Date": "2024-04-26 21:48:58+00:00"}, {"submission_ID": "1cdxdh4", "submission_Title": "Databricks Architecture: Control plane and data plane", "submission_Score": 2, "submission_Author": "Extra-Cancel3086", "submission_selftext": "Hi All,  \nI have been reading documentation and going over the databricks partner academy videos, there has been a bit of conflicting information as to where two components exists for say a customer account, that is Control plane and Data Plane?  \nIf we take Azure as the cloud provider in this scenario, are both control plane and data plane in customers azure account managed via subscription or the control plane is managed by databricks with their own subscription ? If so would that mean the control plane is not directly accessible for the customer?  \nBelow are my references:\n\n1. [https://premvishnoi.medium.com/data-engineer-git-versioning-with-databricks-repos-f3b4592e2c92](https://premvishnoi.medium.com/data-engineer-git-versioning-with-databricks-repos-f3b4592e2c92) The below diagram explicitly calls out the Control Plane in databricks -- manage customer accounts, datasets, cluster\n\nhttps://preview.redd.it/xajuhft38wwc1.png?width=1394&format=png&auto=webp&s=8b4682c12100ae6790c0c13d1ac2c4160b12b6f6\n\n\n\nThe above documentation says that control plane is in customer databricks account.  \nCan anyone help me understand the architecture better?  \nThank you!", "Create Date": "2024-04-26 21:37:44+00:00"}, {"submission_ID": "1cdx354", "submission_Title": "Devops Engineer or Data Engineer Career choice", "submission_Score": 9, "submission_Author": "Wild_Manufacturer105", "submission_selftext": "Hi guys, so I've been working as a DevOps engineer for just 2 months now after I graduated last Dec 2023. I find it really challenging, which I like, and it's also quite new to me. Before this, I interned as a Data Engineer and enjoyed it as well. However, after graduating, I couldn't land a job as a DE, but I was lucky to receive an offer as a DevOps Engineer.\n\nNevertheless, I'm still focusing on the Data Engineer side because I've done a lot of projects in DE and even earned a certification in Azure DP-900, with DP-203 pending. Therefore, I feel like I'm wasting the skills I've built for my DE career since my current DevOps Engineer role doesn't involve using the cloud. My company operates everything on-premises, so my daily tasks revolve around Linux, SQL, NoSQL, deploying applications and databases, maintaining servers, cleaning disk space, and troubleshooting operational issues.\n\nDo you guys think I should change jobs after a year and focus on becoming a Data Engineer, or should I continue focusing on DevOps Engineering and try to learn something new, perhaps by getting certifications in Terraform, Kubernetes, Docker, AWS, or Azure? Thanks, guys!!!", "Create Date": "2024-04-26 21:25:46+00:00"}, {"submission_ID": "1cdwlww", "submission_Title": "AI/ML for data engineering ", "submission_Score": 2, "submission_Author": "Jkk_geek", "submission_selftext": "Hey everyone,\n\n I'm gearing up to conduct the first round of screening for a data engineer/scientist position focusing on AI/ML model building for my client. \n\nWhile I'm comfortable with the surface level things, I want to deepen my understanding about model building before the screening. Any suggestions for online courses or sample projects I can dive into within a week? I don't want to start from scratch, just aiming to grasp the workings. Also, I'd appreciate your input on must-ask questions for the screening.\n Thanks!", "Create Date": "2024-04-26 21:06:05+00:00"}, {"submission_ID": "1cdvxow", "submission_Title": "My videography business' file storage is getting really expensive. Any ideas on reducing costs for someone who's not tech-savvy?", "submission_Score": 6, "submission_Author": "PianoCharged", "submission_selftext": "My event videography hobby is transitioning to a full-time profession; and I'm becoming overwhelmed with the simple challenge of data storage. My most popular product is 360\u00b0 video which is livestreamed and concurrently recorded in 11K resolution at 60fps with one to two hours of total footage. The raw files on the camera's SD cards alone are huge. Then I want to keep my post-production work and the final cut. Most projects end up being 1TB to 2TB^(+).\n\nMy Mac Studio's hard drive has 2TB (a regretful underestimate), so I do post-production from a 6TB external SSD. For backup of the external SSD and archival of finished projects, I've been using Dropbox because it's user-friendly, but it's also getting really expensive.\n\nI've looked into other options, but anything significantly cheaper is buggy and clunky, not nearly as user-friendly as Dropbox.\n\nI've thought about just keeping everything local and buying more and more external hard drives as necessary, but (i) this requires renting a climate-controlled storage unit for the backup copies, (ii) I'd still need to sync/backup files in progress, and, years from now when I have petabytes' worth of archives, (iii) I wonder if the hardware would end up costing just as much as a cloud-based service.\n\nAny suggestions on reducing my costs? I'm not particularly tech-savvy, FYI. Thank you in advance!", "Create Date": "2024-04-26 20:38:35+00:00"}, {"submission_ID": "1cdurxf", "submission_Title": "\"Defer to Prod\" concept", "submission_Score": 2, "submission_Author": "Ownards", "submission_selftext": "Hello everyone,\n\nI came across this article which explains the concept of \"Defer to Prod\" in Dbt, but I must say I really don't understand the use-case.\n\nIt says that the feature enables a developer to connect to a production model (environment) without having to build the dependent elements in the development environment :\n\n [More time coding, less time waiting: Mastering defer in dbt | dbt Developer Blog (getdbt.com)](https://docs.getdbt.com/blog/defer-to-prod) \n\nhttps://preview.redd.it/9tder4kjovwc1.png?width=785&format=png&auto=webp&s=390de216188f15e44ddbfdaa034077cedd18a97b\n\nBut what kind of development environment is that ? In all projects I did, the development environment was a copy from the production environment with all dependencies. What is this development environment with a single model\\_f floating in the middle of nowhere? I don't get it ...", "Create Date": "2024-04-26 19:52:35+00:00"}, {"submission_ID": "1cdudj9", "submission_Title": "I don\u2019t know if it\u2019s enough for a first job\u2026", "submission_Score": 2, "submission_Author": "Puzzleheaded-Sun3107", "submission_selftext": "I have 5 years of work experience in civil engineering, did a software engineering bootcamp and have a GIS certificate.\nI learned from the bootcamp that I\u2019m don\u2019t enjoy building UI, backend (build api set up database) was fine and at most making a cool graph with data. My civil engineering (simulation modeling) role gave me the experience in data analysis which I used python to do ETL or a portion of it. Most recently I created 2 ETLs at my last company using purely python pulling data from an api, using database data to query the api, applied web scraping to write to the database (I was not in an IT role so I did not have write access to their database plus the database was huge and very confusing think random column names) and using windows task scheduler to run the script every 5 minutes. Essentially reproducing what I did in a previous work experience using Azure Function App and Logic Apps. Is this enough to get a Data Engineering job or do I need to sit down and pick up AWS? I\u2019d rather learn on the job as I\u2019ve done in previous experiences (I\u2019m getting tired of using my personal time, I don\u2019t mind but I\u2019d like to be more strategic with my learning approach)\n\nAny advice would be welcomed!  ", "Create Date": "2024-04-26 19:35:58+00:00"}, {"submission_ID": "1cds5yo", "submission_Title": "Would it be cringe to connect with people on LinkedIn?", "submission_Score": 10, "submission_Author": "Single-Sound-1865", "submission_selftext": "I don't study cs so I am not in the circles of people in software industry so it would be logical to connect with people in DE in different career levels to make connections and possible future referrals but the thing is these people are complete strangers as we don't have anything in common\n\nI won't contact seniors or HR folks because iam not ready yet ( still learning)", "Create Date": "2024-04-26 18:04:57+00:00"}, {"submission_ID": "1cds2vy", "submission_Title": "What is your favorite Postgres extension and why?", "submission_Score": 1, "submission_Author": "AMDataLake", "submission_selftext": "What are your favorite parts of the Postgres ecosystem", "Create Date": "2024-04-26 18:01:32+00:00"}, {"submission_ID": "1cdrhdu", "submission_Title": "3NF and dimensional modeling ", "submission_Score": 8, "submission_Author": "yeykawb", "submission_selftext": "Why is it always so that these often stand in contrast to each other? When searching for \u201c3NF data warehouse\u201d there is a lot of \u201c3NF vs dimensional\u201d. Why is it so?\n\nWe\u2019re using three layers, landing -> 3NF -> star schema data marts (dimensional modeling). For me they complement each other rather than compete.\n\nAm I completely missing the point?", "Create Date": "2024-04-26 17:37:07+00:00"}, {"submission_ID": "1cdr3zx", "submission_Title": "Reving Warehouse DDL", "submission_Score": 3, "submission_Author": "ReporterNervous6822", "submission_selftext": "Wondering what tooling/solutions others use to manage warehouse schemas? My org was using alembic for a bit but as our warehouse size grows, migrations become expensive in terms of time.  Our current solution is one piece of DDL in git per table that recreates a table from an existing one using a select * into it, and we modify whatever we need to there in terms of new columns or indexes or whatever. This is fine but the worst part is finding a runtime to run migrations that might take 20+ hours\u2026.so what are you all using for migrations/DDL changes on big data warehouses? ", "Create Date": "2024-04-26 17:21:58+00:00"}, {"submission_ID": "1cdr2va", "submission_Title": "Using DBT Expectations for a FK check", "submission_Score": 2, "submission_Author": "slcgayoutdoors", "submission_selftext": "New to dbt:\n\nIe does the value from Table1.Column exist in Table2.Column.\n\nIt doesn't look like dbt expectations has something of that nature? It should be simple to make as a generic, but I was sort of expecting one to exist already and don't see it.", "Create Date": "2024-04-26 17:20:45+00:00"}, {"submission_ID": "1cdqz5r", "submission_Title": "Python vs Bash as a glue language: what are the pros and cons? And which one do you use?", "submission_Score": 5, "submission_Author": "knightfall0", "submission_selftext": "TL;DR: Planning on switching from bash to Python for wrapper scripts around the Pyspark application. What are the pros and cons to this?\n\n\nI'm currently working in a company where we use bash scripts as a wrapper around the Pyspark pipelines. The job scheduling tool invokes a shell script, which does a few operations like date control, configuration setup etc. and then runs spark-submit to run a pyspark pipeline. Finally the bash scripts also catch exit codes and send out emails. \n\nThis works, but it's getting difficult to maintain. We have 150+ lines of bash scripts for each application and I find it extremely difficult to work with. \n\nI've been thinking of working on shifting some of the processing to a python script instead. In my mind, python being a general purpose programming language gives it certain powers that are hard to replicate in bash. Error handling, ease of debugging, and complex conditional statements are a few. \n\n\nMy question is, is there a certain benefit to bash that I'm not seeing? What are the pros and cons of switching from bash to Python?", "Create Date": "2024-04-26 17:16:30+00:00"}, {"submission_ID": "1cdq1cp", "submission_Title": "LLMs are Commoditized; Data is the Differentiator", "submission_Score": 3, "submission_Author": "something_cleverer", "submission_selftext": "", "Create Date": "2024-04-26 16:38:20+00:00"}]