[{"comment_ID": "l15s5x0", "comment_Body": "\"you nerds\" said here, doesn't mean what it meant in the high school cafeteria in the US. I actually really appreciate the invitation to share our technical opinions.", "comment_Score": 1, "comment_Author": "MeditatingSheep", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 05:26:15+00:00"}, {"comment_ID": "l15s2o1", "comment_Body": "Is this via databricks? The log files will make sure they merge in the correct order. It is all done automatically, easy enough to test though to be sure.", "comment_Score": 1, "comment_Author": "Additional-Maize3980", "comment_Link_id": "1ccjshm", "Create Date": "2024-04-25 05:25:19+00:00"}, {"comment_ID": "l15rkte", "comment_Body": "We deliver data downloads that ranges from 35 MB to 6 GB to thousands of customers. Our formats are gzipped CSV, gzipped NDJSON and a special type of binary data format. This just works.", "comment_Score": 1, "comment_Author": "reincdr", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 05:20:24+00:00"}, {"comment_ID": "l15rkps", "comment_Body": "We are using ADF to copy the data into parquet, it always defaults the date", "comment_Score": 1, "comment_Author": "passing_marks", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 05:20:23+00:00"}, {"comment_ID": "l15rcmz", "comment_Body": "https://github.com/apache/parquet-format/blob/master/LogicalTypes.md#common-considerations\n\nUnless you are using nanosecond precision timestamps you shouldn\u2019t have any issue representing the year 1000 in parquet according to the spec. Nanoseconds for data that far back doesnt make sense anyway", "comment_Score": 1, "comment_Author": "exergy31", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 05:18:10+00:00"}, {"comment_ID": "l15qd7b", "comment_Body": "Please do! If it\u2019s good, I\u2019ll actually look into it for our stack.", "comment_Score": 1, "comment_Author": "JohnPaulDavyJones", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 05:08:43+00:00"}, {"comment_ID": "l15prx1", "comment_Body": "Your post/comment was removed because it violated rule #3 (Do a search before asking a question). The question you asked has been answered in the wiki so we remove these questions to keep the feed digestable for everyone.", "comment_Score": 1, "comment_Author": "dataengineering-ModTeam", "comment_Link_id": "1cc6dnp", "Create Date": "2024-04-25 05:03:06+00:00"}, {"comment_ID": "l15papl", "comment_Body": "What library package are you using to send emails (e.g. imaplib)? Assuming the package sends an error code if the message fails, my suggestion would be to build a custom PTransform that takes in a PCollection of your email addresses and has two outputs, one for emails that succeed and one that fails after x number of retries. For outputs, create a key value pair where the key is some hardcode value like 1. In the next step you can do operations like combine by key if you wanted to get the total number of successful jobs or group and bulk uploads to BigQuery the failed jobs.\n\nOne things to note on your post. You won\u2019t be able to stream effectively if you are using BigQuery as your input as it is a bounded source. You maybe better off triggering batch jobs periodically and reading between the timestamp values.", "comment_Score": 1, "comment_Author": "sassypantsuu", "comment_Link_id": "1ccf7nm", "Create Date": "2024-04-25 04:58:36+00:00"}, {"comment_ID": "l15p9pr", "comment_Body": "Should I tell HR or the manager about this? Or just put my actual title (product safety specialist) on my background check and see if they comment on it?", "comment_Score": 1, "comment_Author": "seikoalpinist197", "comment_Link_id": "1ccf9zr", "Create Date": "2024-04-25 04:58:20+00:00"}, {"comment_ID": "l15p9br", "comment_Body": "This post was flagged as not being related enough to data engineering. In order to keep the quality and engagement high, we sometimes remove content that is unrelated or not relevant enough to data engineering.", "comment_Score": 1, "comment_Author": "dataengineering-ModTeam", "comment_Link_id": "1ccf9zr", "Create Date": "2024-04-25 04:58:14+00:00"}, {"comment_ID": "l15p7ws", "comment_Body": "Please see our rules about this topic in the sidebar.", "comment_Score": 1, "comment_Author": "dataengineering-ModTeam", "comment_Link_id": "1ccghx9", "Create Date": "2024-04-25 04:57:51+00:00"}, {"comment_ID": "l15p67f", "comment_Body": "This was true but five years back", "comment_Score": 1, "comment_Author": "king_booker", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 04:57:24+00:00"}, {"comment_ID": "l15oe88", "comment_Body": "Iceberg supports parquet, orc and avro", "comment_Score": 1, "comment_Author": "madness_of_the_order", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 04:50:10+00:00"}, {"comment_ID": "l15odd6", "comment_Body": "Official company titles are not very important.   \nThe rule of thumb is to put your functional role as your title (as you have done) to make it clear what your role was, regardless what some \"creative\" person at your company deiced your title should be.\n\nThat said, \"Product Safety Specialist\" is a pretty strange title for someone doing analytics.\n\nIt is unlikely the background check will go that deep. If they do, they'll ask you about this (the recruiter or HM) and you can just explain that your day to day work was in analytics using x, y, z tool. But the company doesn't have an official \"Analytics Engineer\" role.\n\nI get why you are concerned but I wouldn't sweat this at all.", "comment_Score": 1, "comment_Author": "NoUsernames1eft", "comment_Link_id": "1ccf9zr", "Create Date": "2024-04-25 04:49:56+00:00"}, {"comment_ID": "l15n9ar", "comment_Body": "Well, here is my take. I have been a DE for close to 5 years now.\nEvery big org wants to move to the cloud but they don't realise how expensive the cloud is.\nFor starters I think having a way to manage cloud costs ( cloud provider agnostic) can be a good hit.\nThen if the solution can provide good cost saving recommendations then even better.", "comment_Score": 1, "comment_Author": "TheRedRoss96", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 04:39:39+00:00"}, {"comment_ID": "l15lhkh", "comment_Body": "paywall.", "comment_Score": 1, "comment_Author": "PeaDifficult1128", "comment_Link_id": "1ccd6rs", "Create Date": "2024-04-25 04:23:47+00:00"}, {"comment_ID": "l15kydz", "comment_Body": "What and run into the Medium paywall, no thanks.", "comment_Score": 1, "comment_Author": "MachineParadox", "comment_Link_id": "1ccd6rs", "Create Date": "2024-04-25 04:19:03+00:00"}, {"comment_ID": "l15kwam", "comment_Body": "I'll add it to thousands of articles I have yet to read.", "comment_Score": 1, "comment_Author": "jerrie86", "comment_Link_id": "1cbx9ki", "Create Date": "2024-04-25 04:18:32+00:00"}, {"comment_ID": "l15kqzh", "comment_Body": "You don't need to know exactly all the trendy tools, but having experience with at least one member of their category is important to open your doors. Given that you probably already know SQL well, I think trying on your side some free-tier cloud SQL and dbt could be a fairly easy way for you to open more doors.\n\n> Mainly all I do is code with cloud microservices.\n\nThis may give you some interesting experience that some \"SQL monkeys\" here are not having, so learn to sell that too!", "comment_Score": 5, "comment_Author": "sib_n", "comment_Link_id": "1ccd1ep", "Create Date": "2024-04-25 04:17:14+00:00"}, {"comment_ID": "l15kpfa", "comment_Body": "Dbt and Apache air flow", "comment_Score": 0, "comment_Author": "SophisticatedFun", "comment_Link_id": "1cchtjk", "Create Date": "2024-04-25 04:16:51+00:00"}, {"comment_ID": "l15knbo", "comment_Body": "Yeah, the IPO should be aaaany day now.", "comment_Score": 1, "comment_Author": "SintPannekoek", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 04:16:20+00:00"}, {"comment_ID": "l15kixn", "comment_Body": "Hey Brad,\n\n  \nThank you so much for the feedback. It means a lot!\n\nI used a template from Tally to build that, and the email form was there. I felt the same way but still decided to experiment.\n\nAfter considering your feedback, I decided to remove the email form at the end of the article. I agree with you that it could be perceived as unnecessary.\n\nI don't want this article to look like a promotion. All I want is to help people. In fact, the quiz has two more alternatives to Meltano, which makes the chance of recommending it quite low.\n\n  \nThank you again!", "comment_Score": 0, "comment_Author": "ivanovyordan", "comment_Link_id": "1cbutrc", "Create Date": "2024-04-25 04:15:15+00:00"}, {"comment_ID": "l15kakg", "comment_Body": "Never heard of Hex before but looks great. Looks similar to Mode at first glance.", "comment_Score": 1, "comment_Author": "JBalloonist", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 04:13:14+00:00"}, {"comment_ID": "l15k4lt", "comment_Body": "Hahaha well I\u2019m checking out their demo in a few weeks - happy to report back :p", "comment_Score": 2, "comment_Author": "RandomRandomPenguin", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 04:11:48+00:00"}, {"comment_ID": "l15k1o6", "comment_Body": "RFC: rename to `big_datum_mike`.", "comment_Score": 4, "comment_Author": "sib_n", "comment_Link_id": "1ccd1ep", "Create Date": "2024-04-25 04:11:06+00:00"}, {"comment_ID": "l15i9sl", "comment_Body": "You can find a list of community-submitted learning resources here: https://dataengineering.wiki/Learning+Resources\n\n*I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/dataengineering) if you have any questions or concerns.*", "comment_Score": 1, "comment_Author": "AutoModerator", "comment_Link_id": "1cchtjk", "Create Date": "2024-04-25 03:56:05+00:00"}, {"comment_ID": "l15g3c5", "comment_Body": "Databricks is the champion in data right now, everything else is mostly niche. There's dbt, SQL mesh and so on, but if you ever listen to any other rep, i.e. Microsoft, Amazon, etc. they always admit that \"this\" and \"that\" was championed by Databricks. \n\nThey (and Apache, obviously) are what made this industry what it is, in my opinion. Others can, of course, disagree.", "comment_Score": 1, "comment_Author": "Hear7y", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 03:38:13+00:00"}, {"comment_ID": "l15fjzy", "comment_Body": "Resource constraints.", "comment_Score": 1, "comment_Author": "ithinkiboughtadingo", "comment_Link_id": "1ccfe3q", "Create Date": "2024-04-25 03:34:00+00:00"}, {"comment_ID": "l15fgzy", "comment_Body": "Transactional databases are designed to support the transactions of operations. They handle high concurrency simple queries like SIUD kind of transactions. The structure of this data, often in some kind of normal form, is incredibly difficult to answer questions and deliver insights from, not to mention the databases are often not performant at aggregations and complex joins especially while maintaining operations. \n\nA proper data warehouse uses a database appropriate for aggregation, like OLAP. It also denormalizes data structures and calculates metrics for reporting purposes. This both reduces reporting risk (everyone derives the same things differently) and reduces time to insights delivery (faster returning queries, metrics available, etc). \n\nIn my data warehouse (p&c claims data for a t1 carrier), one table may source 20-30 operational tables, have 20 metrics, and/or hundreds of dimensions. These are stored at the transaction level (microsecond) so they can be aggregated any needed way. We\u2019ll end up with 30-40 tables as it nears completion.", "comment_Score": 1, "comment_Author": "kaji823", "comment_Link_id": "1ccfe3q", "Create Date": "2024-04-25 03:33:21+00:00"}, {"comment_ID": "l15f3yx", "comment_Body": "You may have multiple systems that you need to query together. That might include an ERP that is on-prem with a SQL database, plus a CRM that you can get data from via an API, and spreadsheets with budgets that you store in SharePoint. You need to be able to bring all of these sources together in a unified way, and expose them to the business through a \u2018single window\u2019 to bake it easy to get insights.", "comment_Score": 1, "comment_Author": "nydasco", "comment_Link_id": "1ccfe3q", "Create Date": "2024-04-25 03:30:26+00:00"}, {"comment_ID": "l15eoc7", "comment_Body": "I think Grafana has one of the highest growth potential, though it's not really data engineering specific. There's an industry wide focus on security which involves centralized, comprehensive loggine solutions and it's not stopping any time soon. And the current solutions are expensive AF. \n\nSnowflake is a great product but they have a [class action lawsuit ](https://www.rgrdlaw.com/cases-snowflake-inc-class-action-lawsuit-snow.html)accusing them of overselling credits to cook their revenue books, so I would be weary to join as a sales rep potentially dealing with that shit.\n\nDatabricks is also an great product, and especially if you could get equity could be pretty cool as I think they'll almost certainly ipo or get acquired soon-ish.\n\nI'd avoid micro-solutions like Monte Carlo (observabiilty) and Atlan (data catalog). They are great, but imho they'll never bring enough value alone to justify the cost of implementation/maintenance.", "comment_Score": 1, "comment_Author": "poopybutbaby", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 03:27:00+00:00"}, {"comment_ID": "l15e922", "comment_Body": "Man, the only thing I\u2019ve heard about Sigma Computing is that the platform blows from two friends who experimented with it a while ago, and then their constant emails from account managers who figured out my work email and want to get a meeting to sell me on their tool.\n\nTheir tool could have progressed lightyears from wherever it was when it sucked, but I\u2019d still be turned off by how many emails I get from their salespeople. Block one, and another one is in my inbox two weeks later.", "comment_Score": 1, "comment_Author": "JohnPaulDavyJones", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 03:23:40+00:00"}, {"comment_ID": "l15dred", "comment_Body": "Once again, Gartner appears to be doing their research on this sub...", "comment_Score": 1, "comment_Author": "SintPannekoek", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 03:19:51+00:00"}, {"comment_ID": "l15ddck", "comment_Body": "People are fighting tooth and nail to get into that place", "comment_Score": 2, "comment_Author": "IAMHideoKojimaAMA", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 03:16:51+00:00"}, {"comment_ID": "l15d8g4", "comment_Body": "\ud83d\udc80", "comment_Score": 1, "comment_Author": "IlIlIl11IlIlIl", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 03:15:48+00:00"}, {"comment_ID": "l15d6ej", "comment_Body": "I would have used the word \u201c[delve](https://www.businessinsider.com/y-combinator-paul-graham-delve-ai-chatgpt-giveaway-email-pitch-2024-4)\u201d if I was a bot. Beep. Boop.", "comment_Score": 1, "comment_Author": "miqcie", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 03:15:22+00:00"}, {"comment_ID": "l15cxdc", "comment_Body": "Just you. Chatgpt is solely based off your past and present internet experience. Thank you for your service.", "comment_Score": 1, "comment_Author": "IlIlIl11IlIlIl", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 03:13:27+00:00"}, {"comment_ID": "l15crdj", "comment_Body": "RARLY do I come across domo companies or contracts. And when I do it's like huh I forgot about domo. Haven't even used it yet. I'm sure it's as good as you say. But you know how it is, just used to using what's mostly out there", "comment_Score": 1, "comment_Author": "IAMHideoKojimaAMA", "comment_Link_id": "1cc9gh3", "Create Date": "2024-04-25 03:12:12+00:00"}, {"comment_ID": "l15blpm", "comment_Body": "I try to save the trees", "comment_Score": 1, "comment_Author": "defnotjec", "comment_Link_id": "1cbx8bb", "Create Date": "2024-04-25 03:03:22+00:00"}, {"comment_ID": "l15azw7", "comment_Body": "Fancy dinners for the boss. I wish I was joking.", "comment_Score": 1, "comment_Author": "Visionexe", "comment_Link_id": "1ccdveg", "Create Date": "2024-04-25 02:58:51+00:00"}]