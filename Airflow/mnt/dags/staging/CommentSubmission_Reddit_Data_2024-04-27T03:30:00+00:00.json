[{"comment_ID": "l1giuqk", "comment_Body": "It\u2019s almost always because of three things:\n\n1) Poor data architecture without separation of concerns, eg like letting people query raw data directly. \n\n2) an unnecessarily large amount of transformations and models (I\u2019m looking at you, dbt)\n\n3) Poor SQL syntax because people don\u2019t understand columnar storage of data warehouses.\n\nSnowflake works just fine and with the right set up will be fast and efficient. But it\u2019s also very forgiving and will just throw compute at the three issues above so that your bill will just keep racking up. \n\nSo get a data engineer who understands setting up Snowflake correctly, set guardrails for your transformation layer and teach your analysts good SQL.", "comment_Score": 1, "comment_Author": "Current_Doubt_8584", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 04:28:25+00:00"}, {"comment_ID": "l1gikj3", "comment_Body": "You can limit that with a warehouse and resource monitor\u00a0", "comment_Score": 1, "comment_Author": "Fantastic-Schedule15", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 04:25:51+00:00"}, {"comment_ID": "l1ghwnm", "comment_Body": "Not weird at all. I run a consulting firm, which has a data and IA verticle. I get heaps of randos sending me requests, I accept all of them (unless I can smell some kinda scam). \n\nI've since given advise to people all over the world, all walks of life and all ages. But they all share a common love of working with data, that I can tell.", "comment_Score": 1, "comment_Author": "Additional-Maize3980", "comment_Link_id": "1cds5yo", "Create Date": "2024-04-27 04:19:56+00:00"}, {"comment_ID": "l1ghgdq", "comment_Body": "I haven't done that, but it should work. It's a straightforward process:\n\n* Add the CSV to the seeds\n* Reference the seed model as you would the SQL model", "comment_Score": 1, "comment_Author": "ivanovyordan", "comment_Link_id": "198w13u", "Create Date": "2024-04-27 04:16:01+00:00"}, {"comment_ID": "l1gge2b", "comment_Body": "Cheaper than GBQ. But I don\u2019t like it as much\u2026", "comment_Score": 1, "comment_Author": "UnrealizedLosses", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 04:06:24+00:00"}, {"comment_ID": "l1gg95m", "comment_Body": "Depends on where you are starting from", "comment_Score": 1, "comment_Author": "deemerritt", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 04:05:12+00:00"}, {"comment_ID": "l1gg7vw", "comment_Body": "Wow TIL.", "comment_Score": 1, "comment_Author": "icysandstone", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 04:04:52+00:00"}, {"comment_ID": "l1gg2t5", "comment_Body": "You mean like no star schema? All 3NF?", "comment_Score": 1, "comment_Author": "icysandstone", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 04:03:39+00:00"}, {"comment_ID": "l1gfu8w", "comment_Body": "Until someone runs a rampant query and sucks up all your monthly credit.", "comment_Score": 2, "comment_Author": "MachineParadox", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 04:01:34+00:00"}, {"comment_ID": "l1gfoqs", "comment_Body": "Probably so but DE definitely has on-call as well. I doubt many jobs in this field don\u2019t have on-call.", "comment_Score": 1, "comment_Author": "khaili109", "comment_Link_id": "1cdx354", "Create Date": "2024-04-27 04:00:14+00:00"}, {"comment_ID": "l1gffk0", "comment_Body": "# \u201cIf the only tool you have is a hammer, you tend to see every problem as a nail.\u201d", "comment_Score": 1, "comment_Author": "Sufficient_Exam_2104", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 03:57:57+00:00"}, {"comment_ID": "l1gfem9", "comment_Body": "Python for working with scripting, data, api' and generally anything in data engineering. You need shell scripts for a few things, but keep them simple. Performance wise your pinching pennies using bash and paying dollars for engineers to do the work", "comment_Score": 1, "comment_Author": "doinnuffin", "comment_Link_id": "1cdqz5r", "Create Date": "2024-04-27 03:57:43+00:00"}, {"comment_ID": "l1gfcfc", "comment_Body": "FYI Snowflake employee:\n\nThere are numerous ways to control costs per account, clusters and user levels.\n\nYou can put hard(Shutoff) or soft(warning only) limits x$ per week/month/year:\n1. At account level \n2. At Warehouse or combo of warehouses\n\n\nYou can also put query timeout limits:\n1. Account Level(applies to all users & clusters)\n2. Warehouse Level ( diff timeouts for engineering  vs. Analytics clusters)\n3. User Level \n\nEach lower level will override the others.\n\nThese will prevent excess usage by avoiding run away queries & stopping compute if gets overused.\n\nThere is also BUDGETS feature that you can use to track costs against compute and storage per project.\n\nAccounts have no limits by default. It is in our onboarding deck that we go over with new customers where putting account & query timeout limits is the first thing we recommend. \n\nWe give tools for controlling, reporting & preventing unwanted usage. You just have to put those controls in place.", "comment_Score": 1, "comment_Author": "Mr_Nickster_", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:57:12+00:00"}, {"comment_ID": "l1gfbf8", "comment_Body": "You can enable auto management of your delta tables in DLT, which will automatically vacuum and optimize with z-order. It runs as a separate maintenance job with job compute once per day.", "comment_Score": 1, "comment_Author": "SimpleSimon665", "comment_Link_id": "1ce2ojv", "Create Date": "2024-04-27 03:56:57+00:00"}, {"comment_ID": "l1gev21", "comment_Body": "We had a similar tension at our company with BQ billing, and I made it work without imposing any real rules on analysts or stakeholders using the BI tooling.\n\nI was able to cut our BQ bill in about half a while back with constant usage. Basically without talking to anyone or affecting any work, by building custom tooling into our util library that wraps all of our data infra and switching some stuff out in place.\n\nMostly it was dynamically running queries against the alternative billing methods depending on the expected resource consumption profile of that job (memory vs CPU heavy), and designing an abstraction for partially materialized views on top of big log streaming tables that had a lot of different kinds of events that needed real time reporting.\n\nI also added some really basic constraints for partition filter requirements, Bi tool max byte processing limits, but pretty high. Biggest inflections where those two tools up there, switching out billing methods at run time and these weird partially materialized views that were kind of a pain at first but I wrote as a framework and then could just stamp it.\n\nI'm just saying that because those don't necessarily have to be irreconcilable. It's just that, if your leadership really wants both, they have to make the right investments. In my case the CTO was leaning on me and I didn't want the shit to roll downhill and mess up our culture and productivity, so I just told him that hobby horse would cost him a month of messed up productivity, and dealt with it myself to avoid distracting the team.", "comment_Score": 2, "comment_Author": "melodyze", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:53:01+00:00"}, {"comment_ID": "l1getjj", "comment_Body": "Company uses it due to sunk cost fallacy.", "comment_Score": 1, "comment_Author": "Sufficient_Exam_2104", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:52:40+00:00"}, {"comment_ID": "l1ges6r", "comment_Body": "Expensive compared to what? Not having an extremely valuable DWH that helps drive revenue generating decisions?", "comment_Score": 1, "comment_Author": "yeetsqua69", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:52:20+00:00"}, {"comment_ID": "l1geh73", "comment_Body": "It\u2019s also cloud agnostic, just about every tool/connector works with snowflake across all public clouds", "comment_Score": 2, "comment_Author": "Sp00ky_6", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:49:42+00:00"}, {"comment_ID": "l1gedv1", "comment_Body": "your coworker is full of crap. I work for a decent sized game company. My job is a lot of SQL. We're using snowflake primarily as our primary database. I just wrote a 700 line stored procedure that curates data from a bunch of different sources into a shiny, relatively simple table. SQL is the language to do this kinda stuff in, especially since it's going to perform pretty well since it runs directly in snowflake.", "comment_Score": 1, "comment_Author": "seleniumdream", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 03:48:54+00:00"}, {"comment_ID": "l1ge8uj", "comment_Body": "doesn\u2019t Dev ops involve being on call more than DE? That would make a difference in career path", "comment_Score": 2, "comment_Author": "Professional_War2996", "comment_Link_id": "1cdx354", "Create Date": "2024-04-27 03:47:44+00:00"}, {"comment_ID": "l1ge84b", "comment_Body": "This is the way. Defend your team like family, but aggressively weed out the untrustworthy and unreliable. Once everyone trusts each other, up, down and sideways, the team flourishes and then deserves a brave advocate.", "comment_Score": 1, "comment_Author": "AncientClumps", "comment_Link_id": "1cdmsrt", "Create Date": "2024-04-27 03:47:33+00:00"}, {"comment_ID": "l1ge4z9", "comment_Body": "which DWH it was ?", "comment_Score": 3, "comment_Author": "Sufficient_Exam_2104", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:46:48+00:00"}, {"comment_ID": "l1gcmas", "comment_Body": "Unpacking nested json using python dictionaries is my go to", "comment_Score": 1, "comment_Author": "superduperkylee", "comment_Link_id": "1ccsms9", "Create Date": "2024-04-27 03:33:56+00:00"}, {"comment_ID": "l1gcllu", "comment_Body": "I don't doubt that Snowflake can work well for folks.  But it's hard to keep those costs down - especially if you want frequent data updates.", "comment_Score": 0, "comment_Author": "kenfar", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:33:47+00:00"}, {"comment_ID": "l1gchvf", "comment_Body": "It is expensive but the alternative of DIY is more expensive.\n\nNow it might be more expensive than their competitors, but then we would need to debate the feature set, capabilities, and stability first.", "comment_Score": 1, "comment_Author": "tanin47", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:32:56+00:00"}, {"comment_ID": "l1gcgds", "comment_Body": "Could anyone compare this with BigQuery?  I feel BigQuery has most of the above mentioned features", "comment_Score": 2, "comment_Author": "alone_drone", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:32:34+00:00"}, {"comment_ID": "l1gc51c", "comment_Body": "As a person who works both as Data Engineer and DevOps, I\u2019d say it highly depends on the project itself. My experience as DevOps varied from routine polishing of huge Jenkins jobs integrated with outdated technologies to building cutting-edge GitOps IaC systems on K8. As DE it were hundreds of hours debugging and optimizing Spark jobs as well as architecting analytical streaming processes on Kinesis. Both directions can offer you either absolutely boring stuff or super interesting experince. I\u2019m personally willing to develop more as DE as this role is often tied with Data Science and ML projects so in my opinion will be more promissing and demanded in future.", "comment_Score": 1, "comment_Author": "Last_from_Mogican", "comment_Link_id": "1cdx354", "Create Date": "2024-04-27 03:29:58+00:00"}, {"comment_ID": "l1gbxjn", "comment_Body": "Surely you don't actually believe this? The closer truth is people are experienced in one and it's easier to validate what you know. \n\nThese experiences used to be more prevalent but these days everyone is copying each other's homework. With so much feature parity and competitive pricing models, the biggest difference between platforms is naming.", "comment_Score": 3, "comment_Author": "drinknbird", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:28:15+00:00"}, {"comment_ID": "l1g9w85", "comment_Body": "Senior management loves the buzzword \"cloud database\"", "comment_Score": 1, "comment_Author": "foresttrader", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:11:50+00:00"}, {"comment_ID": "l1g9ul2", "comment_Body": "This is actually a propaganda, instead of fact comparison. Take \"stateless broker\" as example, AutoMQ vendored kafka brokers are definitely stateful given that immediately acknowledged data are only accessible to the owner broker node before getting uploaded to S3.\n\nIn addition, their solution suffers in terms of service reliability. RTO, in case of  power-off crashes, will be several minitutes. Measure the time it takes to force detach EBS from a panic EC2 instance and the time to recover a kafka broker with dozens of partitions. These issues are better handled in Apache Kafka with quorum replication.", "comment_Score": 1, "comment_Author": "rust_cn", "comment_Link_id": "1ccodiy", "Create Date": "2024-04-27 03:11:28+00:00"}, {"comment_ID": "l1g9oox", "comment_Body": "Not that expensive if you know how to use it.", "comment_Score": 1, "comment_Author": "Traditional-Ad-8670", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:10:10+00:00"}, {"comment_ID": "l1g9h9y", "comment_Body": "Disclaimer: I don\u2019t work for snowflake, I just like the platform. IMO It\u2019s not as expensive as people make it out to be. Any product based on compute-second for billing could be called \u201cexpensive\u201d. There are certain use cases that would be crazy expensive (real-time aka < 1min of latency), but I think with their new external storage solution they close the gap there.\u00a0\n\nTLDR: I think it speaks to the divide in the data engineering community, the traditional DBA/ETL/DW folks vs the flashy software engineering background folks.\u00a0", "comment_Score": 1, "comment_Author": "AnnualDepth8843", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 03:08:32+00:00"}, {"comment_ID": "l1g7ugo", "comment_Body": "Not cringe but probably less effective than you think it is", "comment_Score": 1, "comment_Author": "NightflowerFade", "comment_Link_id": "1cds5yo", "Create Date": "2024-04-27 02:55:50+00:00"}, {"comment_ID": "l1g7pwx", "comment_Body": "I love hearing these war stories. My only on prem experience was with vertical but our bill was in the million+ range for 72 nodes / year. Really loved that cluster, worked wonderfully. \n\nAs a snowflake customer I can give you some ideas of what it would cost though. The issue isn't really the total dataset size, it's totally reasonable to have 10tb of data and still keep your costs less than say $40k/yr. Storage is cheap. The issue is their computation pricing. It's pretty steep. \n\nWe run a trino cluster with snowflake on top of that as the analysts interface. Snowflake is nice because the Rbac and resource contention model is clean as fuck. Rough estimate is they're markup on storage is 1x and on compute it's 6-10x. It's expensive but not crazy especially if you use incremental logic", "comment_Score": 2, "comment_Author": "Foodwithfloyd", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:54:51+00:00"}, {"comment_ID": "l1g7eu5", "comment_Body": "S3 has both a front end (via AWS console) and cli that\u2019s easy to use.", "comment_Score": 2, "comment_Author": "DisappearCompletely", "comment_Link_id": "1cdvxow", "Create Date": "2024-04-27 02:52:30+00:00"}, {"comment_ID": "l1g6d7l", "comment_Body": "Fair point but not always. In true data companies where the data itself is the main selling point (like Neilson) democratizing data does work.", "comment_Score": 3, "comment_Author": "mamaBiskothu", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:44:39+00:00"}, {"comment_ID": "l1g6163", "comment_Body": "A lot of folks have been using that Architecture for a while:  Landing/Staging -> Data Warehouse (3NF) -> Data Marts (business specific stuctures).\n\nThis is a valid architecture if you have an organization that has a requirement for a central collection of atomic data that is business neutral (fairly-normalized data warehouse), and also business specific data structures that are supported in the business specific data marts.  These business specific data structures could be dimensional, observational format, marketing lists, data structures that support specific reports, etc.\n\nMost organizations that require this architecture might have a central repository of data and has to maybe support 15 different business user groups that each want to see data their way.'\n\nFor a lot of organizations, this might be overkill.  If building something for a department and are not burdened by the requirements of others in the organization, might be good for you to just go from source (staging) to your business specific structures which might be a dimensional model.  Lots of organizations do this.  Whether this is good or bad will depend on what your enterprise organization's data strategy might be.  There are plusses and minuses of this approach.  This can be a huge discussion all on it's own.\n\nIf it's a small company with solid focus on it's particular data needs, going from source to dimensional models might be the way to go if they want 'speed of business' and time to market.\n\nA 3NF structure is very flexible and meets the needs of businesses that have different data needs in different parts of the organization.  But 3NF tends to be more difficult for business users to query (this can be argued either way buy different experts) and dimensional modeling tends to be easier for business users and query generators in UI tools to generate SQL.\n\nIf your organization has requirements that fit the Staging -> 3NF -> Dimensional model scenario then that would be a valid approach.  But if the the 3NF or the Dimensional are just being thrown in with the other, might not be the best way to go.\n\nNOTE: When I mention the word 'fairly normalized' I'm saying this because I've seen folks model in 1NF, 2NF, 3NF, and believe it or not 4NF for their data models. Sometimes it's on purpose and can be explained why.  Other times it's what ended up from the data modelers attempt at data modeling and that was their best effort...\n\nA thing with 3NF in the real world is that there are some companies that will build a 'logical data model' in 3NF.  But when it gets to implementation there might be some performance optimizations that are put in place on the physical data model to accommodate the in(capabilities) of some of the databases.  Some of these optimizations might be:\n\n*  pushing super-types down into the sub-types to eliminate joins. \n   * Example is the party model.  A party might be a customer, vendor, employee, etc that have common attributes that would be in the party super-type.  One way to reduce joins is to take those party attributes and push down to the subtype entities.\n* Pre-joining things that usually go together\n   * example Order Header to order Line-item\n   * This reduces joins on things that will normally go together anyway.\n\nA problem that occurs when building the logical model with business input is that when it's physically implemented, and if the business users are not part of the physical database implementation, the physical model ends up being different that the logical model that the business understands. I've seen this happens which is why I adopted what I call physio-logical modeling.  \n\nWhat I mean by this is if we know the database we will be working with, and we are building a 3NF with business user input, we'll build the model as if it's going to be the physical model and model with the supertype push-downs and the prejoins during the meetings with the business user so that the model they help build is the model they query.  I use the name physio-logical modeling as it's a play on logical and physical models, being built as the same, at the same time.  \n\nA reason to do this is most business users are not going to want to sit through a bunch of logical data modeling sessions, then later sit through a bunch of physical data modeling sessions.  Just do it once.  This is going to be heresy to some or many, as it was in the company I worked for 25 years, but I've found putting the business users through one round of sessions is much better than two rounds for what they see is the same thing.\n\nIn dimensional modeling the logical model worked out with the business users (dimensional model) ends up being very close if not the exact same as the physical model (the star schema). So that when the business users see the end product it is the model that they expect.", "comment_Score": 1, "comment_Author": "GotSeoul", "comment_Link_id": "1cdrhdu", "Create Date": "2024-04-27 02:42:10+00:00"}, {"comment_ID": "l1g5y93", "comment_Body": "Use an LLM to do the translations for you, have a whiskey.", "comment_Score": 1, "comment_Author": "PizzaCatAm", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 02:41:33+00:00"}, {"comment_ID": "l1g5oac", "comment_Body": "It just shows the cost of inefficient queries in the monthly charges with out effecting the availability while it\u2019s the other way around with out snowflake", "comment_Score": 1, "comment_Author": "Any_Check_7301", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:39:28+00:00"}, {"comment_ID": "l1g5du3", "comment_Body": "As someone who barely knew Spark (or Scala) when I started my current (first) DE position... can confirm. I'm pretty comfortable with SQL but in my previous positions we always had DBAs who would go over our SQL to optimize it. Now I have to do the optimization myself and I'm seeing something new every day.", "comment_Score": 1, "comment_Author": "BenjaminGeiger", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 02:37:21+00:00"}]