[{"comment_ID": "l1h3z3u", "comment_Body": "It's mostly a design flaw", "comment_Score": 1, "comment_Author": "SierraBravoLima", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 08:22:02+00:00"}, {"comment_ID": "l1h3u3g", "comment_Body": "Snowflake does its job pretty well and you are able to buy a solution for a lot of problems on the one hand. On the other hand, you need to be really cautious. Their sales team is trying to upsell you constantly in a really aggressive manner. Also, the \"everybody can manage the platform\" claim is kind of wrong. You really need to dig into Snowflake to use it properly.\n\nHere's the story I experienced:\n\nI had a manager in the past who wasn't technical at all, but thought he was very technical. That was a perfect match for the Snowflake salespeople. He went crazy about the product and they started to upsell us like crazy. Soon we switched from standard to enterprise without any reason. We didn't gain any advantages, we just paid for features we didn't even need. Because he never gave the team time to maintain the warehouse properly to optimize cost efficiency, the bill went through the roof. He started to increase the amount of yearly credits up to $130k per year for a pretty small team of 5 full-time analysts and around 1TB of data. When the company started to struggle due to COVID, he quit his job to wriggle out of it. After that, we were forced to drastically reduce costs because otherwise, we would be forced to start laying off our analysts to get our cost target. At this point, we didn't even know what kind of contracts the ex-manager signed. So we started to rework our infrastructure around Snowflake and were able to save 1/3 of the bill pretty quickly. After some time, we realized that the best approach for our situation would be to switch back from Snowflake Enterprise to Standard, which would cut our bill for another 30%. This was the moment we noticed that none of our savings would get us any advantage. The money is gone as soon as we would change the contract. In the end, we settled with Snowflake around $30k and switched back to Standard. In this process, I had multiple meetings with the Snowflake sales team and technical consultants, and after that, I could kind of understand why the clueless manager signed the contracts that were absolutely over our scale. There was a point where we started to ghost the boss of our Snowflake account manager and just did our thing. At the moment, we are doing the same job for around $22k per year with the side effect that our analysts have the feeling that our Snowflake warehouses are performing better than before.\n\nFor us, that was a really hard time. We were really afraid of having to lay off technically good people with their own personal stories and all of that because of a manager who didn't know what he was doing and an insane amount of upselling.", "comment_Score": 1, "comment_Author": "erwagon", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 08:20:17+00:00"}, {"comment_ID": "l1h3jpj", "comment_Body": "I know what you're talking about.\nI am working on shifting from teradata to snowflake. Right now it costs almost half million dollars yearly and snowflake seems to be a good option.\nIf you want, do you mind telling me the company via pm? Maybe we can set up a call if you'd like.", "comment_Score": 1, "comment_Author": "vinchent_PSP", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 08:16:40+00:00"}, {"comment_ID": "l1h248a", "comment_Body": "It was built on Teradata", "comment_Score": 1, "comment_Author": "HighPitchedHegemony", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 07:58:49+00:00"}, {"comment_ID": "l1h1zjq", "comment_Body": "I had an engineer bring down our Snowflake costs by 50% by implementing a new processing architecture. \n\nThe company gave him a petty raise when it came to performance evaluations. No wonder people don't feel motivated to do it.", "comment_Score": 3, "comment_Author": "sluuurpyy", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 07:57:11+00:00"}, {"comment_ID": "l1h1mxb", "comment_Body": "The readability of (simple) SQL queries is better than having multiples lines of doing the same with spark. You can have the query in a separate file, which makes it easier to maintain instead of having the logic of reading and transforming in the same file. \nI wouldn't write complex SQL queries (talking about hundreds of lines of SQL with subqueries and temp tables) because the following week no one except your past self and god know what you did.\nYou can also define a function that parametrizes queries for you so you can dynamically query the statement without having to write the same query with fixed parameters over and over again (you can also do that in spark tbf). \n\nIn summary, splitting the logic of reading a table and transforming the data is advantageous in my opinion. But that's just me, I don't like my script clogged.", "comment_Score": 1, "comment_Author": "IIDraxII", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 07:52:43+00:00"}, {"comment_ID": "l1h12oa", "comment_Body": "Note he says Scala/Spark. IME in those cases scaais used only as \"\"plumbing\" tool, while data manipulation are done in  Spark.", "comment_Score": 1, "comment_Author": "perverse_sheaf", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 07:45:38+00:00"}, {"comment_ID": "l1h0np1", "comment_Body": "If they had that, they wouldn't be complaining about their DWH not having workload isolation. That is like one of the first thing you arrange on a DWH.", "comment_Score": 1, "comment_Author": "JonPX", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 07:40:23+00:00"}, {"comment_ID": "l1h0i0w", "comment_Body": "Isn't it though still cheaper than Redshift?", "comment_Score": 1, "comment_Author": "alexchambana", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 07:38:26+00:00"}, {"comment_ID": "l1h09df", "comment_Body": "Sorry, I don't get that point. In what aspects can sql query the initial data better?\n\nAside: I would expect a setup with a catalog and using spark.read.table(tableName), which seems to me a perfectly fine alternative to sql.", "comment_Score": 1, "comment_Author": "perverse_sheaf", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 07:35:26+00:00"}, {"comment_ID": "l1gze59", "comment_Body": "i see, thanks", "comment_Score": 1, "comment_Author": "tablmxz", "comment_Link_id": "1cdqz5r", "Create Date": "2024-04-27 07:24:38+00:00"}, {"comment_ID": "l1gypq7", "comment_Body": "Full agreement. We recently migrated a >50-CTE-behemoth (a year old,  is going to be in production for some years to come) from Hive to Spark. It is so much easier to maintain, I am not looking back. That said, I still use a lot of SQL for simple and ad hoc queries.", "comment_Score": 1, "comment_Author": "perverse_sheaf", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 07:16:19+00:00"}, {"comment_ID": "l1gychr", "comment_Body": "I'm in devops looking to get into data engineering cuz i love working with data and i feel like it's more chill", "comment_Score": 1, "comment_Author": "z420a", "comment_Link_id": "1cdx354", "Create Date": "2024-04-27 07:11:50+00:00"}, {"comment_ID": "l1gy8kb", "comment_Body": "My leader the same, everything done via python libraries.\n\nI spend 10 times longer forking about writing python than just writing the forking sql.\n\nI think it\u2019s insane but I\u2019m not in charge so as long as I get paid I don\u2019t care anymore as have copped shit for debating it in the past", "comment_Score": 1, "comment_Author": "Ok_Relative_2291", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 07:10:31+00:00"}, {"comment_ID": "l1gxh5r", "comment_Body": ">It's just draining for me to connect with new people as an introvert and i don't want to look desperate for a job only my work and projects talks for me\u00a0\n\nI too felt the same few months ago, but I forced myself to better my social skills(not just in professional but all ways), and connected with as many people as I can and saw how I could help them in both professional and personal space. And it is working for me, getting new friends, getting interviews scheduled and many more. \n\nDon't ever believe your work will talk for you, it will do the talking but you have to start the conversation first.", "comment_Score": 1, "comment_Author": "iamthatmadman", "comment_Link_id": "1cds5yo", "Create Date": "2024-04-27 07:01:23+00:00"}, {"comment_ID": "l1gwt28", "comment_Body": "They must have a lot of money. Best case they use each tool only for the part they are best at.\n\nWorst case, it's a mixed architecture by use case / department / team.", "comment_Score": 2, "comment_Author": "tzt1324", "comment_Link_id": "1ce7ly4", "Create Date": "2024-04-27 06:53:30+00:00"}, {"comment_ID": "l1gwihm", "comment_Body": "I know all that in theory - I understand the function of these services, but I feel I lack a deep understanding how networking/ sec work.", "comment_Score": 1, "comment_Author": "user2401372", "comment_Link_id": "1cdgi6u", "Create Date": "2024-04-27 06:50:03+00:00"}, {"comment_ID": "l1gwafm", "comment_Body": "Hey stranger glad to connect just DM me", "comment_Score": 1, "comment_Author": "Thinker_Assignment", "comment_Link_id": "1cds5yo", "Create Date": "2024-04-27 06:47:25+00:00"}, {"comment_ID": "l1gw5pe", "comment_Body": "What are the typical queries (access patterns) planned in the system ?", "comment_Score": 1, "comment_Author": "rasviz", "comment_Link_id": "1ccoerb", "Create Date": "2024-04-27 06:45:51+00:00"}, {"comment_ID": "l1gvyeh", "comment_Body": "Definitely interested in the data you sell", "comment_Score": 1, "comment_Author": "freebird348", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 06:43:28+00:00"}, {"comment_ID": "l1gvk50", "comment_Body": "I would day it depends on the size of data.  If your scheduled jobs data size  wont grow and hover less than 1 GB or so  Airflow might be able to do it with in its task itself. If the size of data to be processed is growing exponentially or if its massive, use any big data processing engine and use airflow to orchestrate it. Like Spark or flink. Sharing data between tasks should be generally by the storage layer like S3 or GCS. Xcom is not designed to transfer the data itself. You can exchange the location of data (S3 path) through xcom. There is a way to map xcom to S3, its in a config  called xcom\\_backend. I will not explore it at this stage.", "comment_Score": 1, "comment_Author": "mortal-psychic", "comment_Link_id": "1cc0mqb", "Create Date": "2024-04-27 06:38:38+00:00"}, {"comment_ID": "l1gvjkj", "comment_Body": "We had a consultancy firm come in to help with reducing costs and it cut our bill in half.  \nI am worried about the enshittification phase in a few years, though.", "comment_Score": 2, "comment_Author": "dessmond", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 06:38:27+00:00"}, {"comment_ID": "l1gvj5a", "comment_Body": "Jesus,  I feel for you, man. I haven't handled replication with SAP. I mostly set up data pipelines in Power Bi with the on-prem gateway and .NET connector. The worst part of SAP is the decentralized nature of products they offer. Working in a large corporate environment, there are many issues that show up around access control and roles. It took me 2-3 months just to get a GAC driver installed, to then get the SAP connector to work. When it was finally set up, I found I could only connect to pre-written Bex queries. When I asked where to find the master data, I had to download all the designer apps only to get denied write access for query designer. Now, if I need data from BW, I have to load all of the pre-written queries and comb through them to see if there is a dataset broad enough to work. \n\nFront end and back end UX is hot garbage!", "comment_Score": 1, "comment_Author": "kkessler1023", "comment_Link_id": "1ccv2r3", "Create Date": "2024-04-27 06:38:19+00:00"}, {"comment_ID": "l1guswc", "comment_Body": "You say you have 50TB now, but to how far must it scale? And how fast is your internet service, down and up? 400mbit UP will allow you to upload 144 GB per hour. If you are working on 2 projects and deltas run into TBs the upload will take 10 to 30 hours.", "comment_Score": 1, "comment_Author": "dessmond", "comment_Link_id": "1cdvxow", "Create Date": "2024-04-27 06:29:52+00:00"}, {"comment_ID": "l1guil5", "comment_Body": "Well, IMHO the modelling (DS or analytics for that matter) is where the magic happens... So it's rarely a case of just using tools, else we wouldn't provide any value as data teams (I mean if that's the case: just use the tools, right?)\n\nAnd that's kind of the whole point of what I was saying with \"fullstack data engineering\" => in such a role you're not \"just\" technical. You also have to get to understand the business needs and hence how you might go about it from the data (available or not...), and only then you get into the technical reflexion on how to actually do it.", "comment_Score": 1, "comment_Author": "briceluu", "comment_Link_id": "1ccsf39", "Create Date": "2024-04-27 06:26:30+00:00"}, {"comment_ID": "l1gtpgb", "comment_Body": "Hi\n\n3NF is just a common term used to describe the design methods for OLTP systems.  As a general practice, 3NF is the way most OLTPs are designed (but you can go to higher in normalization).  \n\nA classical dimensional model (each dimension is a single table) is more like 1NF.   However, if you design a dimensional model and incorporate snowflaking you can end up with a 3NF design which is dimensional.", "comment_Score": 1, "comment_Author": "GreyHairedDWGuy", "comment_Link_id": "1cdrhdu", "Create Date": "2024-04-27 06:17:06+00:00"}, {"comment_ID": "l1gtdqk", "comment_Body": "Could you also dm me please", "comment_Score": 1, "comment_Author": "WallResponsible9918", "comment_Link_id": "1b4r198", "Create Date": "2024-04-27 06:13:28+00:00"}, {"comment_ID": "l1gs55z", "comment_Body": "Yeah, if I get a message explaining why they want to connect I might accept. If you ask some questions I normally take the time to answer them, and converse. Maybe a couple times a year I converse with strangers who are figuring out how to get started, but I never accept invites from strangers with nothing in common if they don\u2019t explain why.", "comment_Score": 2, "comment_Author": "dr_craptastic", "comment_Link_id": "1cds5yo", "Create Date": "2024-04-27 05:59:53+00:00"}, {"comment_ID": "l1grqq0", "comment_Body": "RAID is \"Redundant Array of Independent Disks\". It can be either a hardware (RAID controller) or software level configuration. You basically take multiple drives, depending on the RAID level you're using, and the hardware or software divvies up the drives as needed to support the RAID level you've assigned. For instance, RAID 1 is mirroring, which means when you write to a logical volume, you're actually writing it in two physical locations. Your RAID configuration actually handles the mirroring, so you only see one location, but in the background, everything is mirrored on both logical volumes. One drive goes down, you still have a access to everything because of the mirrored volume. But that also means you halve your total storage. This is the most basic level of fault tolerance, and you should aim for a more complete solution like RAID 5 or 6 or 10.\n\nS3 costs are in egress and time. Something stored in glacier is going to be cheaper to just sit there than in standard storage, but if you need access to the file, it will take time to \"thaw\" and make it available. And AWS charges you for network egress (data leaving AWS) to pull it down from S3. 1-2 TB/mo of egress from S3 will cost between $100 and $200 per month.\n\nYour best bet might be looking at one of those cloud storage providers like Backblaze or pics.io. They still use public cloud storage, but larger entities usually get discounts from cloud providers for their business, so they might give you \"Unlimited\" options that are more affordable, but still profitable for them. I would reach out to a few of them directly and ask them about your specific use case and get certain things in writing. Most of them have \"Fair Usage\" guidelines that do put some practical limits on ingress and egress or file sizes or might throttle transfers above a certain amount. So be very explicit about your needs and make sure you read all the fine print.", "comment_Score": 1, "comment_Author": "BuildingViz", "comment_Link_id": "1cdvxow", "Create Date": "2024-04-27 05:55:35+00:00"}, {"comment_ID": "l1gr7ih", "comment_Body": "What kind of data do you sell?", "comment_Score": 2, "comment_Author": "Martekk_", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 05:49:51+00:00"}, {"comment_ID": "l1gqtbz", "comment_Body": "\"personally identifiable information\", to avoid problems with the General Data Protection Regulation - GDPR", "comment_Score": 3, "comment_Author": "Kaze_Senshi", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 05:45:43+00:00"}, {"comment_ID": "l1goqhc", "comment_Body": "for the rest of us mere mortals though... ;)", "comment_Score": 1, "comment_Author": "mailed", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 05:24:07+00:00"}, {"comment_ID": "l1goeev", "comment_Body": "People always fight for the stack they\u2019re most comfortable with and have a background in. It honestly has nothing to do with SQL or Scala or Rust or whatever. The only thing that matters is getting the right data and quality data for data science, analytics and BI.\n\nUnfortunately in this situation, it looks like your coworker had more influence and he won. This is not an uncommon scenario. In my previous job, a senior engineer who had more of a backend engineering background came to our DE team and completely revamped the entire code base that we had written in PySpark into Scala Spark because HE liked functional languages. He was at the company for 25 years and management didn\u2019t question him. So we had to learn Scala. SMH!!", "comment_Score": 1, "comment_Author": "MotherCharacter8778", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 05:20:49+00:00"}, {"comment_ID": "l1gn1nq", "comment_Body": "Agreed", "comment_Score": 1, "comment_Author": "soundboyselecta", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 05:07:27+00:00"}, {"comment_ID": "l1gn148", "comment_Body": "How did you cold call/message people? What was your strategy? I can shoot the shit with anyone but haven't found the best way to get peoplw on the phone/conversing regularly on linkedin", "comment_Score": 2, "comment_Author": "johnsonfrusciante", "comment_Link_id": "1cds5yo", "Create Date": "2024-04-27 05:07:17+00:00"}, {"comment_ID": "l1gmlcv", "comment_Body": "I believe that is build off their data share technology. That shit is lightning fast. I leverage a transactional platform where we purchase media in real time, and I can see my campaign data via a Snowflake AWS datashare in as little as 10 minutes whereas the platforms themselves take hours to update with aggregate indicators. I\u2019ve built dashboards off of this data to QA our buys faster than we could natively in the companies own software.\n\nIt requires almost 0 ETL, just tell SF where to point the table and query away", "comment_Score": 9, "comment_Author": "JimmyTango", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 05:02:59+00:00"}, {"comment_ID": "l1gmf4m", "comment_Body": "They have some differences, but they\u2019re mostly comparable. Some people just don\u2019t want to be on GCP, and Snowflake will deploy anywhere.", "comment_Score": 6, "comment_Author": "thrav", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 05:01:18+00:00"}, {"comment_ID": "l1glk7b", "comment_Body": "This rings true for me. They've got a winning combo for extracting maximum cash from clients:\n\n1. Ready to go solution that cuts down on the number of pesky employees you have to depend on\n2. Cleverly obfuscated pricing that \\*looks\\* like you know what it's going to cost you, when actually you've no idea what the bill is going to be from one month to the next", "comment_Score": 2, "comment_Author": "bree_dev", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 04:53:11+00:00"}, {"comment_ID": "l1glfks", "comment_Body": "Other on-prem versions are even costlier with less performance.", "comment_Score": 0, "comment_Author": "VolTa1987", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 04:51:58+00:00"}, {"comment_ID": "l1gkfw6", "comment_Body": "I'm just starting to work with spark,  but this guy sounds like a pretentious twat. Is he suggesting to eliminate sql from any part of the ingestion process? It seems like a good way to over complicate things.", "comment_Score": 1, "comment_Author": "kkessler1023", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 04:42:50+00:00"}]