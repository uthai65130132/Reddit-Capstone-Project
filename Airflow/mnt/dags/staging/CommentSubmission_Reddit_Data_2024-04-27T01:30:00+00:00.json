[{"comment_ID": "l1g7ugo", "comment_Body": "Not cringe but probably less effective than you think it is", "comment_Score": 1, "comment_Author": "NightflowerFade", "comment_Link_id": "1cds5yo", "Create Date": "2024-04-27 02:55:50+00:00"}, {"comment_ID": "l1g7pwx", "comment_Body": "I love hearing these war stories. My only on prem experience was with vertical but our bill was in the million+ range for 72 nodes / year. Really loved that cluster, worked wonderfully. \n\nAs a snowflake customer I can give you some ideas of what it would cost though. The issue isn't really the total dataset size, it's totally reasonable to have 10tb of data and still keep your costs less than say $40k/yr. Storage is cheap. The issue is their computation pricing. It's pretty steep. \n\nWe run a trino cluster with snowflake on top of that as the analysts interface. Snowflake is nice because the Rbac and resource contention model is clean as fuck. Rough estimate is they're markup on storage is 1x and on compute it's 6-10x. It's expensive but not crazy especially if you use incremental logic", "comment_Score": 1, "comment_Author": "Foodwithfloyd", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:54:51+00:00"}, {"comment_ID": "l1g7eu5", "comment_Body": "S3 has both a front end (via AWS console) and cli that\u2019s easy to use.", "comment_Score": 1, "comment_Author": "DisappearCompletely", "comment_Link_id": "1cdvxow", "Create Date": "2024-04-27 02:52:30+00:00"}, {"comment_ID": "l1g6d7l", "comment_Body": "Fair point but not always. In true data companies where the data itself is the main selling point (like Neilson) democratizing data does work.", "comment_Score": 2, "comment_Author": "mamaBiskothu", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:44:39+00:00"}, {"comment_ID": "l1g6163", "comment_Body": "A lot of folks have been using that Architecture for a while:  Landing/Staging -> Data Warehouse (3NF) -> Data Marts (business specific stuctures).\n\nThis is a valid architecture if you have an organization that has a requirement for a central collection of atomic data that is business neutral (fairly-normalized data warehouse), and also business specific data structures that are supported in the business specific data marts.  These business specific data structures could be dimensional, observational format, marketing lists, data structures that support specific reports, etc.\n\nMost organizations that require this architecture might have a central repository of data and has to maybe support 15 different business user groups that each want to see data their way.'\n\nFor a lot of organizations, this might be overkill.  If building something for a department and are not burdened by the requirements of others in the organization, might be good for you to just go from source (staging) to your business specific structures which might be a dimensional model.  Lots of organizations do this.  Whether this is good or bad will depend on what your enterprise organization's data strategy might be.  There are plusses and minuses of this approach.  This can be a huge discussion all on it's own.\n\nIf it's a small company with solid focus on it's particular data needs, going from source to dimensional models might be the way to go if they want 'speed of business' and time to market.\n\nA 3NF structure is very flexible and meets the needs of businesses that have different data needs in different parts of the organization.  But 3NF tends to be more difficult for business users to query (this can be argued either way buy different experts) and dimensional modeling tends to be easier for business users and query generators in UI tools to generate SQL.\n\nIf your organization has requirements that fit the Staging -> 3NF -> Dimensional model scenario then that would be a valid approach.  But if the the 3NF or the Dimensional are just being thrown in with the other, might not be the best way to go.\n\nNOTE: When I mention the word 'fairly normalized' I'm saying this because I've seen folks model in 1NF, 2NF, 3NF, and believe it or not 4NF for their data models. Sometimes it's on purpose and can be explained why.  Other times it's what ended up from the data modelers attempt at data modeling and that was their best effort...\n\nA thing with 3NF in the real world is that there are some companies that will build a 'logical data model' in 3NF.  But when it gets to implementation there might be some performance optimizations that are put in place on the physical data model to accommodate the in(capabilities) of some of the databases.  Some of these optimizations might be:\n\n*  pushing super-types down into the sub-types to eliminate joins. \n   * Example is the party model.  A party might be a customer, vendor, employee, etc that have common attributes that would be in the party super-type.  One way to reduce joins is to take those party attributes and push down to the subtype entities.\n* Pre-joining things that usually go together\n   * example Order Header to order Line-item\n   * This reduces joins on things that will normally go together anyway.\n\nA problem that occurs when building the logical model with business input is that when it's physically implemented, and if the business users are not part of the physical database implementation, the physical model ends up being different that the logical model that the business understands. I've seen this happens which is why I adopted what I call physio-logical modeling.  \n\nWhat I mean by this is if we know the database we will be working with, and we are building a 3NF with business user input, we'll build the model as if it's going to be the physical model and model with the supertype push-downs and the prejoins during the meetings with the business user so that the model they help build is the model they query.  I use the name physio-logical modeling as it's a play on logical and physical models, being built as the same, at the same time.  \n\nA reason to do this is most business users are not going to want to sit through a bunch of logical data modeling sessions, then later sit through a bunch of physical data modeling sessions.  Just do it once.  This is going to be heresy to some or many, as it was in the company I worked for 25 years, but I've found putting the business users through one round of sessions is much better than two rounds for what they see is the same thing.\n\nIn dimensional modeling the logical model worked out with the business users (dimensional model) ends up being very close if not the exact same as the physical model (the star schema). So that when the business users see the end product it is the model that they expect.", "comment_Score": 1, "comment_Author": "GotSeoul", "comment_Link_id": "1cdrhdu", "Create Date": "2024-04-27 02:42:10+00:00"}, {"comment_ID": "l1g5y93", "comment_Body": "Use an LLM to do the translations for you, have a whiskey.", "comment_Score": 1, "comment_Author": "PizzaCatAm", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 02:41:33+00:00"}, {"comment_ID": "l1g5oac", "comment_Body": "It just shows the cost of inefficient queries in the monthly charges with out effecting the availability while it\u2019s the other way around with out snowflake", "comment_Score": 1, "comment_Author": "Any_Check_7301", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:39:28+00:00"}, {"comment_ID": "l1g5du3", "comment_Body": "As someone who barely knew Spark (or Scala) when I started my current (first) DE position... can confirm. I'm pretty comfortable with SQL but in my previous positions we always had DBAs who would go over our SQL to optimize it. Now I have to do the optimization myself and I'm seeing something new every day.", "comment_Score": 1, "comment_Author": "BenjaminGeiger", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 02:37:21+00:00"}, {"comment_ID": "l1g5cxj", "comment_Body": "I learned it a long time ago and I'm trying to make it everyone's problem.", "comment_Score": 1, "comment_Author": "AnubisJcakal", "comment_Link_id": "1cdqz5r", "Create Date": "2024-04-27 02:37:10+00:00"}, {"comment_ID": "l1g4m8d", "comment_Body": "my company has now spent 2 years focusing on reducing snowflake spend whilst simultaneously endorsing a culture from the very, very top that developer agency is paramount so there are no guardrails when it comes to querying\n\nshockingly, that reduction has never born fruit", "comment_Score": 2, "comment_Author": "Steamsalt", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:31:46+00:00"}, {"comment_ID": "l1g4m4n", "comment_Body": "in my experience, expanding data access without expanding data \"sense\" does not lead to improve efficiency. It makes things worse not better. Now you have more people making more wrong decisions, except now they wrongly use the data to justify it.", "comment_Score": 2, "comment_Author": "naijaboiler", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:31:45+00:00"}, {"comment_ID": "l1g4gns", "comment_Body": "Literally just had something in my daily data pipeline broke because of a new patch", "comment_Score": 2, "comment_Author": "iRemjeyX", "comment_Link_id": "1cdjiby", "Create Date": "2024-04-27 02:30:39+00:00"}, {"comment_ID": "l1g4e63", "comment_Body": "I read somewhere that we trust/respect managers that can do our work more.\nThe catch - technical manager can be overwhelming if they are more technical than you.", "comment_Score": 1, "comment_Author": "LaserToy", "comment_Link_id": "1cdmsrt", "Create Date": "2024-04-27 02:30:09+00:00"}, {"comment_ID": "l1g4cl2", "comment_Body": "Sounds like a configuration and permissions error. Why let power users determine warehouse size? RBAC is built into place to ensure power users or developers dont abuse the compute scaling you indicated here as \"putting an end to a $50k query\". \n\nThere are actually configurations in place to prevent this. Read the white pages on resource monitors. They prevent situations/events such as this.", "comment_Score": 1, "comment_Author": "HorseCrafty4487", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:29:51+00:00"}, {"comment_ID": "l1g487p", "comment_Body": "Nah keep your posts bruh. I literally told you how you could limit the spend and you couldn\u2019t see that part of the message could you? \n\nAnswer the simpler question of how an analyst was allowed to spend 50k on a query. Sounds like an unhinged org with no controls or DBAs.", "comment_Score": 1, "comment_Author": "mamaBiskothu", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:28:57+00:00"}, {"comment_ID": "l1g45v7", "comment_Body": "And this is why someone like you will never be head of anything in a big company.", "comment_Score": -1, "comment_Author": "allenasm", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:28:28+00:00"}, {"comment_ID": "l1g3vhx", "comment_Body": "Automated PII masking!?", "comment_Score": 2, "comment_Author": "TehMoonRulz", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:26:22+00:00"}, {"comment_ID": "l1g3p96", "comment_Body": "You\u2019re head of architecture and couldn\u2019t any lackey to put a max warehouse size and/or timeout to limit queries beyond 500 bucks? Sounds like a you problem mate.\n\nAlso if someone cost you 50k on a query it suggests you let them lose on a 5xl or a 6xl specially provisioned warehouse. To an ANALYST. That\u2019s like buying a drunk spoilt teenager a Ferrari and asking to drive into a hospital. Don\u2019t blame the tools huh.", "comment_Score": 0, "comment_Author": "mamaBiskothu", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:25:05+00:00"}, {"comment_ID": "l1g3msr", "comment_Body": "Snowflake is top down approach while other companies, Databricks, is bottom up. I did see there are memes that Snowflake purchases are decided by company executives playing golf with Snowflake sales people. For executives eyes, as long as it fulfills their tech stack transitioning into cloud based new gen solutions, Snowflake should be fine. It doesn't mean Snowflake is a bad product. Just hypothesizing why companies purchase despite it being more expensive than other solutions.", "comment_Score": 2, "comment_Author": "Historical-Papaya-83", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:24:35+00:00"}, {"comment_ID": "l1g3d1l", "comment_Body": "Ha ha first time I\u2019m hearing a bad interpretation of the CEO.  Like what? Snowflake still is doing what it promised, it\u2019s as unadulterated as it could be (except the Neeva acquisition lol), its stock price is pretty good.  The CEO has done another successful IPO and I assumed he just wanted to quit at this point. Do you have any other discussions to point otherwise?\n\nAlso I\u2019m sorry Databricks sucks. It\u2019s a good tool for hardcore DE teams _maybe_ but not at all a replacement for snowflake for where it truly shines, in the _it just works_ department.", "comment_Score": 2, "comment_Author": "mamaBiskothu", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:22:38+00:00"}, {"comment_ID": "l1g2tvy", "comment_Body": "It is not more expensive, but it instantly democratizes the company\u2019s data to a much larger population of employees since all they need is basic sql skills. Thus the costs explode for two reasons - more people are exploring your data to actually do business and you\u2019re likely doing well because of this , and these people are inexperienced and hence end up writing really inefficient sql which snowflake will happily execute without erroring out because it\u2019ll just scale up the clusters.", "comment_Score": 1, "comment_Author": "mamaBiskothu", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:18:47+00:00"}, {"comment_ID": "l1g1jg9", "comment_Body": "why do people smoke crack cocaine if it is as expensive as people say?", "comment_Score": 1, "comment_Author": "dalmutidangus", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:09:20+00:00"}, {"comment_ID": "l1g19kq", "comment_Body": "I'm kinda trolling too with the kill redshift 4x. Semi structured data is really its downfall. I've worked at many companies that have used Redshift, and the engineering teams at all said at some point for some subset of the front end data model that we'll just handle it on the application side so a JSON blob in postgres is the data model. Unnesting that shit makes redshift shit the bed, postgres can go way bigger than redshift. Jsonb pisses off DMS too", "comment_Score": 1, "comment_Author": "flatulent1", "comment_Link_id": "1cdjiby", "Create Date": "2024-04-27 02:07:21+00:00"}, {"comment_ID": "l1g0mht", "comment_Body": "Yeah, I'm starting to learn that you're always going to pay for it somewhere. There is no magic solution that has all the functionality that you want, and a good easy to use interface, and cheaply, if there was we'd all use that.\n\nCan you build it in house? Sure, but then you have to pay someone to build it, and maintain it, and then you just added a bunch of institutional knowledge that you can lose when someone leaves.", "comment_Score": 3, "comment_Author": "Nyjinsky", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 02:02:42+00:00"}, {"comment_ID": "l1fzzce", "comment_Body": "Adding random people on social media is not \u201dnetworking\u201d.", "comment_Score": 1, "comment_Author": "KOREANWALMART", "comment_Link_id": "1cds5yo", "Create Date": "2024-04-27 01:58:11+00:00"}, {"comment_ID": "l1fzsfu", "comment_Body": "Not sure what you were running for a $360k price tag but that seems excessive. Ive noticed properly architected data models and ensuring your queries/workloads are designed efficiently reduce compute costs.\n\nSnowflake has measures to ensure warehouses arent online 24/7/365", "comment_Score": 1, "comment_Author": "HorseCrafty4487", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 01:56:50+00:00"}, {"comment_ID": "l1fzqrk", "comment_Body": "Devops skills pair really well with Data Engineering. A lot of companies don\u2019t like to hire sole DevOps Engineers for their data stack so it\u2019s either the Wild West or they hire Data Engineers with DevOps skills like Terraform or CI.", "comment_Score": 1, "comment_Author": "sassypantsuu", "comment_Link_id": "1cdx354", "Create Date": "2024-04-27 01:56:30+00:00"}, {"comment_ID": "l1fzpqc", "comment_Body": "It\u2019s really not, though. Connecting with strangers definitely isn\u2019t the etiquette, and people generally don\u2019t like random people reaching out to them.", "comment_Score": 2, "comment_Author": "KOREANWALMART", "comment_Link_id": "1cds5yo", "Create Date": "2024-04-27 01:56:17+00:00"}, {"comment_ID": "l1fzjsb", "comment_Body": "20 TB hard drive on Amazon is like $250.", "comment_Score": 1, "comment_Author": "SlightEmergency9131", "comment_Link_id": "1cdvxow", "Create Date": "2024-04-27 01:55:08+00:00"}, {"comment_ID": "l1fzcn2", "comment_Body": ">RA Nodes - great feature, but if you need to scale to a level which isn't a elastic resize your SOL.\n\nRecover from snapshot onto a new cluster and classic resize the new one? Or break it off into separate clusters and data share? Or think about the node size and types when you create the cluster. There's also concurrency scaling...\n\n>There was a significant amount of time when it would give new nodes a different version than the rest of the cluster. AWS Support isn't so keen to admit fault, so them telling me it's their fault tells me it's them.\n\nI have never had that happen. Were you not allowing maintenance mode to happen or something? Weird and not cool regardless..\n\n>SUPER can be good but it's still inferior to Snowflake Variant. Super says 16mb, but it's really 1mb, and max string size is still 64k.\n\nI don't think I've intentionally stored semi structured data in RS since it's usually better to just tabularize it. The oddities with 1MB I bet are due to RS block size being 1MB. But for semi structured data, you could use spectrum would be my guess. \n\nThe data type problem sucks / can't do much about it. The version mixing is weird and idk what happened there. It could be bad releases or scaling or something on your side.. no idea. For the scaling, when creating a cluster it's important to check if the configuration can be scaled. That might have happened before you got there but with data sharing cross cluster, it should be less of a problem since you can break different work loads into different clusters.\n\nDidn't mean to rub you the wrong way. I was kinda trolling initially anyways.", "comment_Score": 1, "comment_Author": "data_addict", "comment_Link_id": "1cdjiby", "Create Date": "2024-04-27 01:53:42+00:00"}, {"comment_ID": "l1fzaor", "comment_Body": "And sometim\u00e9s (or most of the time in my limited experience), the cost is much smaller than the human resources needed to man the inhouse tech.\n\nSometimes it just need a data analyst to write useful but badly optimized script, then a data engineer to fix those scripts. On the other hand you need a team of infrastructure engineers to manage the machines (or cloud engineers in case of cloud), platform engineers to develop and maintain the stack, and then a data analyst to write useful but badly optimized script, then a data engineer to fix those scripts :)", "comment_Score": 5, "comment_Author": "dreamingfighter", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 01:53:18+00:00"}, {"comment_ID": "l1fy737", "comment_Body": "Honestly?  As head of architecture in fortune 5 I\u2019d have divisions coming to me and they would already have a POC working. They would get buy off from the bean counters in their division.  It wasn\u2019t a bad solution, just friggin expensive. If their ceo was willing to pay for it\u2026. I guess\u2026.  I did however put an end to all snowflake after we had a BA execute a $50k running query.  When j met with Microsoft and snowflake they were incredulous that I wanted to STOP any query that cost over $500.  They would only offer up alerts, nothing to stop it. So I guess that\u2019s both how they get in and get shut down.", "comment_Score": -3, "comment_Author": "allenasm", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 01:45:30+00:00"}, {"comment_ID": "l1fy6un", "comment_Body": "Funny how LLMs took off before DE. \u201cWait\u2026 we need engineers to develop pipelines to feed our models!? I thought AI was magic.\u201d", "comment_Score": 1, "comment_Author": "Expensive_Map9356", "comment_Link_id": "1ccsf39", "Create Date": "2024-04-27 01:45:27+00:00"}, {"comment_ID": "l1fwkut", "comment_Body": "TIRES NO LONGER HAVE A PLACE IN VEHICLES.  FROM NOW ON EVERYTHING NEEDS TO BE HOVERCRAFTS.", "comment_Score": 1, "comment_Author": "Evening_Chemist_2367", "comment_Link_id": "1cdjtn4", "Create Date": "2024-04-27 01:34:05+00:00"}, {"comment_ID": "l1fwj3v", "comment_Body": "Non tech manager: excel sheets everywhere. Even we have a dashboard but need to have excel version of it. I can\u2019t even automate the excel process due to heavily written excel functions and abundance of dependent excel sheets. Plus no push for making things properly built with best engineering practices. Pure focus is having dashboardand and excel ready on time.\nMiss the time I worked with manager with tech background", "comment_Score": 1, "comment_Author": "dev_anon", "comment_Link_id": "1cdmsrt", "Create Date": "2024-04-27 01:33:45+00:00"}, {"comment_ID": "l1fvxbk", "comment_Body": "That's absolutely true that you had to scale for peak compute, though with some systems you had workload managers that could slow down some queries, give priority to others, etc.\n\n> Slower? Not at TB/PB scale. For GBs of data, yes.\n\nAnd yeah, faster 15 years ago.  Though it took work.  I had a db2 database on a small linux cluster with a lot of memory, and a ton of extremely fast disk & extremely fast solid state storage on a bunch of fast io channels.\n\nIIRC that was about 10 TB in size, had 50 billion row tables, and our average query response time was about 1.9 seconds.  I was also able to tell that users that they could hit it with as many ad hoc queries as they wanted - they would not be able to hurt performance for anyone or knock it over.   The entire system cost $150k, and we usually only had a part-time dba.  This system ran 7x24, with a ton of users hammering it.  We also had a fall-back system in a separate data center.  Users could query both.   The company still uses that system today, though they added a new cluser every 5 years or so.    \n\nSnowflake would have cost 20x as much at a minimum.", "comment_Score": 1, "comment_Author": "kenfar", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 01:29:31+00:00"}, {"comment_ID": "l1fvjxj", "comment_Body": "1. Dimensional models aren\u2019t designed to persist transactional level data forever. They\u2019re designed to support analytics and reporting. So you don\u2019t need 30+ years of transactional grain data in them. That sits in the underlying warehouse, which could be 3NF or DV or something else.\n2. There might be complicated logic required to build out your metrics (or attributes in a dimension). Rather than a 2,000 line SQL query, you might want to break this up into steps. Each of those steps may end up being persistent in a table. You should apply a consistent architecture and design approach to these tables, and dimensional modeling doesn\u2019t work here.", "comment_Score": 2, "comment_Author": "nydasco", "comment_Link_id": "1cdrhdu", "Create Date": "2024-04-27 01:26:53+00:00"}, {"comment_ID": "l1fvedo", "comment_Body": "For us it's very cheap compared to other traditional databases as It increases productivity multiple times.", "comment_Score": 3, "comment_Author": "Middle-Salamander189", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 01:25:49+00:00"}, {"comment_ID": "l1fuarf", "comment_Body": "Nice try", "comment_Score": 3, "comment_Author": "cavoli31", "comment_Link_id": "1cdqz5r", "Create Date": "2024-04-27 01:18:23+00:00"}, {"comment_ID": "l1ftrfr", "comment_Body": "Big difference from 15 years ago though is you have scalable ephemeral compute in the cloud. Only pay for what you use.  \n\nWith on prem you have to purchase for peak capacity even though you may only hit that sparingly - or you underpurchase and have users waiting for queries to run. \n\nSlower? Not at TB/PB scale.  For GBs of data, yes.", "comment_Score": 0, "comment_Author": "the-ocean-", "comment_Link_id": "1ce0ohq", "Create Date": "2024-04-27 01:14:42+00:00"}]