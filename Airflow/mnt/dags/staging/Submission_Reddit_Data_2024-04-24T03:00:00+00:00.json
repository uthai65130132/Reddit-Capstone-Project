[{"submission_ID": "1cbkvjb", "submission_Title": "Databricks Asset Bundles now GA - thoughts? ", "submission_Score": 9, "submission_Author": "justanator101", "submission_selftext": "Databricks announced that assert bundles has become GA - https://www.databricks.com/blog/announcing-general-availability-databricks-asset-bundles. They also teased a future feature, ability to write DABS in Python. \n\nMy work is looking at switching to DAB from Terraform. Are you currently using it? Any gotchas or issues you\u2019ve had? ", "Create Date": "2024-04-24 00:37:18+00:00"}, {"submission_ID": "1cbkotm", "submission_Title": "Single vs man Data Warehouses", "submission_Score": 4, "submission_Author": "scan-horizon", "submission_selftext": "I work at a company with 200+ staff spread across 5 core departments. We have a presence in Azure and MS Entra.\n\nWe are currently in the midst of a debate around using a single DW for the entire company, or multiple DWs with each serving the specific needs of each department and teams within.\n\nWe currently have a complex Azure DW (Azure data factory + Azure Databricks) which processes and serves travel and transport data to internal users, external users from other businesses, and also the data is available to the general public.\n\nA different department wants a DW to store their data which is highly sensitive / personal. None of this data relates to the data in the transport DW. The solution would be a data factory moving data between SQL databases, and interfaces with Power Apps/BI apps. Only select users can ever see the data stored here.\n\nOther departments now want DWs to process and store their data (for example, a place to keep all our workstreams, projects, and client info across the company).\n\nSo lots of different use cases, but all requiring some kind of DW to take data from various sources, transform it, and load it into databases/sink ready for consumption. I\u2019m suggesting each business case should have their own DW solution whereas my bosses are suggesting a single solution so we don\u2019t have loads of different DWs all over the place. Any thoughts on the best approach to take from a DE stance? Thanks", "Create Date": "2024-04-24 00:28:27+00:00"}, {"submission_ID": "1cbhw1f", "submission_Title": "How is the performance of managers measured within Data Engineering", "submission_Score": 3, "submission_Author": "Mr-Bovine_Joni", "submission_selftext": "Hey all, current DE IC here - thinking vaguely about moving into manager roles\n\n**The questions is less \u201cwhat makes a good manager\u201d - but rather, how do directors gauge the performance of managers? **\n\nFrom the outside looking in it all seems much more subjective than being an individual contributor - just more meetings, people management, priority management \n\nIn general I think I could do it - I\u2019ve lead small teams in the past. But I worry about getting a manager role and not knowing what skills to grow\n\nI\u2019m sure all companies do it differently. Would love any thoughts! ", "Create Date": "2024-04-23 22:23:35+00:00"}, {"submission_ID": "1cbgwfr", "submission_Title": "Anyone ever import walmart ad data?", "submission_Score": 1, "submission_Author": "meyerovb", "submission_selftext": "I can get snapchat/facebook/google/tiktok ads, no problem (airbyte/hevo)... looks like walmart locks down their api to ad partners?", "Create Date": "2024-04-23 21:43:25+00:00"}, {"submission_ID": "1cbgswz", "submission_Title": "Design question about architecture for high availability datasets", "submission_Score": 2, "submission_Author": "nariver1", "submission_selftext": "Hey guys, hope everyone is doing fine! \n\nI have the following situation and I would like to read different opinions.\n\nSituation:\n\n* We have 3 data sources (all file based) with one table each source (for the sake of the context)\n* We read that data, we process it by joining data and doing some actions on it\n* We expose it on MySQL normalized enough to have various tables to join\n* This data is exposed through an API endpoint that basically runs the query joining the data\n\nClearly this is running some slowness and scalability issues so our next step is define how to improve performance without sacrificing too much update time of the data.\n\nWe have two approaches:\n\n1. Materialize results with certain cadence \n   * I'm thinking here an airflow dag running each 15'\n   * Data either pre processed or already modeled, will be moved to a different database\n2. Move towards event based actions\n   * Pretty sure standard here is to use kafka and ksql to join data or move data to a different place for processing\n\nMy question would be:\n\n* Question for point 1, would it be ok use airflow to run, let's say, dags each 5' to move data from MySQL 1 to MySQL 2? Already processed or not. Not sure if is an issue run DAGs with high frequence.\n* And more open question, is it out there any other technology that helps to process data fast and expose it for an endpoint that can be consumed with high workload? I think NoSQL database will be good for this scenario such as Mongo.\n\nThanks", "Create Date": "2024-04-23 21:39:39+00:00"}, {"submission_ID": "1cbeu50", "submission_Title": "A synchronous streaming model", "submission_Score": 6, "submission_Author": "woggle_bug", "submission_selftext": "", "Create Date": "2024-04-23 20:21:29+00:00"}, {"submission_ID": "1cbef8q", "submission_Title": "The Birth of SQL & the Relational Database - Asianometry", "submission_Score": 0, "submission_Author": "tomekanco", "submission_selftext": "", "Create Date": "2024-04-23 20:05:08+00:00"}, {"submission_ID": "1cbcfsv", "submission_Title": "CSV viewer/editor with formatting", "submission_Score": 2, "submission_Author": "sidewise8", "submission_selftext": "Does anyone know of a good CSV viewer that can save formatting? (potentially as a sidecar file to the CSV file) Like if I want to color the background of certain rows or adjust column widths. It would also be a plus if I could even hide or move around rows in the viewer without changing anything in the actual CSV file. I've used Excel and LibreOffice to edit the CSV files before, but I want to save my formatting.\n\nEDIT: I should add that the point of this is to keep the data in CSV format so it can be used with my data analysis pipeline (in Python), while being able to view it in a way I want also.", "Create Date": "2024-04-23 18:46:25+00:00"}, {"submission_ID": "1cbbj06", "submission_Title": "Which tools would you recommend for learning as \"industry standard\" to someone looking for a career change?", "submission_Score": 1, "submission_Author": "dataismysuperpower", "submission_selftext": "Hello. PM here with CS degree and 2 year DE experience before moving to PM work.\n\nI'm tired of my role and looking forward to come back to being a DE.\n\nI do know my way about Python and basic libraries.\nI also have experience with SQL and Power BI from my times as a PM.\n\nWhat other tools do you think are the \"industry standard\" at this point for DE roles?\n\nSo far I've seen in my last companies a little bit of people focusing on\n\nDatabricks\nPower BI\nTrino\nSQL Server\n\nWould you agree? Any other modern and widely accepted tools that would help me transit careers?\n\nMy main intention is to build some projects portfolio with real data (I've already scraped some real state sites, for example) to showcase interviewers since I don't think I'd be able to leverage my previous DE experience (it was seven years ago).\n\nThanks in advance!", "Create Date": "2024-04-23 18:10:18+00:00"}, {"submission_ID": "1cbbcdn", "submission_Title": "The Data Engineer's Guide to Building Data Products in Minutes, not Months", "submission_Score": 0, "submission_Author": "Dkreig", "submission_selftext": "In today's data-driven landscape, the demand for creating high-quality Data Products more quickly is greater than ever. In this hands-on webinar for data engineers and data product owners, you will learn how we use DataOps.live Create and DataOps.live Assist to build trusted Data Products literally in minutes.\n\nIn this webinar, you'll see a data owner and a data engineer:\n\n* Build and Develop a Data Product\n* Operationalize it as a data pipeline\n* Review and approve all changes before it goes live\n* And finally promote it to production\n* In less than 30 minutes!\n\nJoin us\u00a0**April 25th, 8am PDT | 11am EDT | 4pm GMT**.\n\n  \nRSVP here - [https://www.dataops.live/dataproductsinminutes](https://www.dataops.live/dataproductsinminutes)", "Create Date": "2024-04-23 18:03:07+00:00"}, {"submission_ID": "1cbamp4", "submission_Title": "Career Advice for Existing DE Moving Up to Senior DE and Beyond?", "submission_Score": 6, "submission_Author": "Oldwoodforest", "submission_selftext": "Tldr; as an existing data engineer, what should I be doing/learning to advance my skills (and career)?\n\nHave seen a lot of great advice and resources on here for people who are breaking into DE from a different career. Like many of those, years back I was working as a data analyst with a lot of overlapping skills and responsibilities to data engineering, and successfully made the jump a couple years ago. Since then, have had lots of exposure and projects using a lot of the usual DE suspects \u2013 ETL (s3->python/scala->redshift), job orchestration (via Airflow and more recently via Databricks), AWS, Databricks, etc. However, I have found myself in a rut where I am not really sure what I should be learning/doing to get to the next level.\n\nWhen I first started, I was by far the most junior DE on my team (all staff and principal engineers), but had a great DE manager who mentored me and provided assignments and projects to get me transitioned. Fast forward a year, and most of my team (and manager) had moved on to other companies and I lost that mentorship \u2013 but suddenly I was drinking from the firehose on a skeleton-crew team of just a few people, left largely to figure out things on my own. This did help me grow quite a bit as well, but it meant that all my learning from that point onwards was focused on whatever crisis or urgent project was on my plate. My company has other DE teams, but they are pretty silo\u2019d off to other branches of the business so that we only occasionally interact when there are company-wide infrastructure or tool changes. I am still on a skeleton-crew team where we large chunks of the year handling infrastructure or major tool changes as our primary projects keeping our supported parts of the business working, with roughly the other half of the year designing and creating new ETL\u2019s, pipelines, and other projects.\n\nI have tried to get some mentorship or direction within my company without much success. My management above me are not DE\u2019s, and the other few DE\u2019s I have contact with within my company/team have been pretty vague or uninterested. I have been working on some certifications (currently preparing to take the Databricks Certified Data Engineer Associate certification since we work a lot with Databricks), and have been perusing some of the DE resources linked to this reddit to shore up my foundational knowledge. Not specifically planning to stay or leave, but after being on such a lean team for so long, I feel like I would have better opportunities somewhere else, so I am happy to learn and develop new skills that would help my career growth, not just my current job (though that is a plus too).", "Create Date": "2024-04-23 17:34:23+00:00"}, {"submission_ID": "1cb95lt", "submission_Title": "Does transition from Devops(infra/platform) to data engineering make sense?", "submission_Score": 5, "submission_Author": "colderness", "submission_selftext": "Hi everyone,\n\nI got an offer but role is not specific. It's like a de job but they need a cloud engineer too. they use cloud, kubernetes, airflow etc. i told them i don't know anything about de. They said you can learn, i can learn of course but i'm not sure of this path, What do de guys think about someone from infra getting a both infra and de job? Any advice?\n\n&#x200B;", "Create Date": "2024-04-23 16:35:17+00:00"}, {"submission_ID": "1cb9365", "submission_Title": "Data Engineering Survey", "submission_Score": 1, "submission_Author": "dataengineeringdude", "submission_selftext": "", "Create Date": "2024-04-23 16:32:28+00:00"}, {"submission_ID": "1cb8yh8", "submission_Title": "What Signals Do you Look for to determine whether a Pipeline should be Streaming over Batch?", "submission_Score": 11, "submission_Author": "AMDataLake", "submission_selftext": "What signals to you that you should take a streaming approach over batch?", "Create Date": "2024-04-23 16:27:12+00:00"}, {"submission_ID": "1cb7mqe", "submission_Title": "Lakehouse doesn't seem to be advantageous for our Data Warehouse. Am I missing something(s)?", "submission_Score": 33, "submission_Author": "cdigioia", "submission_selftext": "We're a Microsoft shop that went from a SQL Server data warehouse (DW), to a Lakehouse DW in Synaspse Serverless, with our facts & dims in Delta tables.  It seems worse.  \n\nI'm thinking Azure SQL, *rather than Databricks*, would have been / would be better for our stiuation. \n\nThat said: \n\n  * I'm not a data engineer (Nor do we have one)\n  * I may be biased, given my only experience is with Synapse Serverless, and pretty sure Databricks is much better. \n  * I *am* sold on our datalake as an ingestion point, just not a *Lakehouse for our DW*\n\nSo here's a lakehouse.  Wow, all these advantages, except none seem to apply to us very much:\n\n  * Ability to fine tune and scale compute with variously sized Spark pools spinning up (then down) as needed.  That is legitimately *neat*. A static compute in SQL Server worked great for us though\n  * Opens up your Facts & Dims to easy acess with additional languages in Spark Notebooks (PySpark at least...perhaps someone uses Scala).  Nope, no-one has utilized that here. No-one is clamoring for it. \n  * Cheap storage.  True, but our data warehouse on SQL Server was like 70GB so...\n\nOn the flip side, read performance of Delta tables (in Synapse Serverless) is quite a bit worse than unoptimized Azure SQL / SQL Server, and development is a fair bit more onerous.  \n\nAm I missing something(s) on why a Lakehouse architecture would be better for our DW?", "Create Date": "2024-04-23 15:34:59+00:00"}, {"submission_ID": "1cb747m", "submission_Title": "How to make a checklist table with multiple sets of things?", "submission_Score": 1, "submission_Author": "PatatoPatatone", "submission_selftext": "Hello smart people, I'd like to make a checklist table with not just two sets of things, but with multiple. I'll explain better. Assume you have two sets of three ingredients each, one with fruits and one with vegetables, and you want to create different recipes using, every time, one fruit and one vegetable. When you complete a recipe, you just check it from your checklist. For example:\n\nWe have BANANA, APPLE and ORANGE in the fruits set and we have CARROT, SALAD and ZUCCHINI in the vegetable set (ok, carrots and zucchini aren't really vegetables, but let's just assume they are). Then, mixing every time one fruit and one vegetable, I can make up to nine recipes. One with BANANA and CARROT, one with BANANA and SALAD, one with BANANA and ZUCCHINI, one with APPLE and CARROT and so on. I can represent the ingredients used in the recipes with a checklist table where I put the fruits in the horizontal array and the vegetables in the vertical array, obtaining nine slots which are to be checked.\n\nSo far so good. \n\nBut let's now assume we add another set of ingredients, say sweets. This time we have two sweets, CHOCOLATE and CANDIES. We want to expand the previous table by obtaining a total of 3\u00d73\u00d72=18 different recipes (basically the same ones we made earlier, but this time adding one time CHOCOLATE and one time CANDIES). The question is, how can I visualize the new checklist table? One way is to use a third dimension and to make a sort of 3D tensor, but the problem is that the more sets of ingredients I have, the more dimensions I need to represent the table, and beyond 3 dimentions it becomes impossible. So, is there a way I can create a table that is graphically represented in 2 dimensions, but having many sets of things that mix together to create a checklist table? More generally, how to visualize such a table with N sets with n_i (with _i_ running from 1 to n) elements? Any ideas?", "Create Date": "2024-04-23 15:13:10+00:00"}, {"submission_ID": "1cb6yc1", "submission_Title": "Notebook transformations", "submission_Score": 2, "submission_Author": "Mathlete7", "submission_selftext": "Hi all, I am relatively new to using Synapse notebooks, and I am trying to use them to do a series of transformations such as derived columns etc, rename columns and so on.\n\nThe way that I am doing it at the moment is loading the data into a python dataframe, performing SQL to cleanse the data and then exporting it to the silver data file store again using python.\n\nI plan to include this in my Bronze to Silver pipeline to apply the transformations daily as part of the daily refresh for my data model. \n\nDoes this sound like an effective way of regularly applying transformations? Also this may be a dumb question,  but are dataframes expensive? the amount of data we are using is pretty small, but thought I may as well ask (Total of around 500mb of data).\n\nDoes not seem to be much in the way of guides/best practices for pyspark and sparkssql so any help greatly appreciated. ", "Create Date": "2024-04-23 15:06:21+00:00"}, {"submission_ID": "1cb6c9a", "submission_Title": "Nomenclature: dimensions vs dimension attributes", "submission_Score": 1, "submission_Author": "garbanzhell", "submission_selftext": "So... I'm a newbie, learning about SCDs, and one thing is bugging me. Why are SCDs calleed SCDs? From my understanding, what changes are the dimension attributes, no? Shouldn't they be called SCDAs? Heck, apparently even the Kimball Group recognizees this when they write:\n\n>Ralph introduced the concept of slowly changing dimension (SCD) attributes in 1996.  \n>  \n>source: [https://www.kimballgroup.com/2013/02/design-tip-152-slowly-changing-dimension-types-0-4-5-6-7/](https://www.kimballgroup.com/2013/02/design-tip-152-slowly-changing-dimension-types-0-4-5-6-7/)\n\n&#x200B;", "Create Date": "2024-04-23 14:40:20+00:00"}, {"submission_ID": "1cb5t3s", "submission_Title": "Which are the best Al, MLOps or Data Conferences in USA or Europe in 2024?", "submission_Score": 2, "submission_Author": "Far-Koala-519", "submission_selftext": "I do not have a budget in mind yet, but i prefer to go in person.", "Create Date": "2024-04-23 14:17:53+00:00"}, {"submission_ID": "1cb5h8y", "submission_Title": "Must Data Warehouse have dimension and fact tables?", "submission_Score": 5, "submission_Author": "quangbilly79", "submission_selftext": "I've just joined a company, and I'm so confused about what I'm working with. Is it a **Data Warehouse** or a **Data Lake** or a **Data Mart** or and **ODS**?\n\nHere is my case: \n\n\\- We're working with Telecommunication Data (phone numbers and their call/message/money recharge). We **integrate data from many sources** like MySQL, Kafka,.. do some **processing**, and then **load** them all together into a **database on HDFS**, **managed by Hive**. \n\n\\- Inside that **Hive \"database\"**, there will be many tables like **Call** (containing records about the phone number and calls from/to that number), **Message** (same as Call), **Charge** (phone number and record every time they recharged),... They aren't related to each other so there is **no \"Dimension\" or \"Fact\" table.**\n\n\\- Based on that data, we do a bit of processing and then calculate the credit score for each phone number (we calculate many model variables for each phone number from the \"database\" on Hive above). This is our **final purpose (Machine Learning - AI)**\n\nI mean according to the Data Warehouse definition on the internet, DWH is just a place that **contains data from many sources**, both **historical** and current data, and that data will be available for **analysis, report, data-mining, BI, DA, DS, AI-ML**,... purposes. So our Hive \"database\" satisfies all the said conditions. They're also \"**Subject-oriented**\", \"**Integrated**\", \"**Nonvolatile**\", and \"**Time-variant**\". The only thing that our Hive \"database\" **doesn't have are Dimension and Fact tables.**\n\nSo is our **Hive \"database\"** above a **Data Warehouse,** a **Data Lake,** a **Data Mart,** or an **ODS**?\n\n&#x200B;", "Create Date": "2024-04-23 14:04:04+00:00"}]