[{"submission_ID": "1cdha2w", "submission_Title": "ability to set breakpoints in sql to debug intermediate states of tables?", "submission_Score": 1, "submission_Author": "cyberjar09", "submission_selftext": "hi I was recently looking at some relatively large dbt models (\\~300 lines) and it had many CTEs which built up intermediate states. I had such a tough time refactoring the query and was wondering \"if there was only a way to set a breakpoint here and look at the state of the intermediate table/cte\". So im here to ask the community, is there such a tool? (using redshift and databricks for context)\n\nAre we not severely handicapped compared to more general purpose software engineering where this is taken for granted? ", "Create Date": "2024-04-26 09:46:25+00:00"}, {"submission_ID": "1cdgi6u", "submission_Title": "Any good resources to learn about networking/sec/infra as a data engineer painlessly?", "submission_Score": 2, "submission_Author": "user2401372", "submission_selftext": "Not sure that's the best group for the question, but here I go: I've been working in data, ML and AI for years. I know data science, how to build AI/ML-using apps, prepare data, etc. I specialize on Azure.\n\nWhat's the fastest way for me to learn about networking/ sec/ generally infra? Any good books? I feel that that's something that's a blocker for me when I design architectures.", "Create Date": "2024-04-26 08:51:57+00:00"}, {"submission_ID": "1cdgbjw", "submission_Title": "Peer Reviews - they actually add more value than you think", "submission_Score": 5, "submission_Author": "nydasco", "submission_selftext": "If you see peer reviews as a bit of a check box process that you just need to do, please take 5 minutes to read my article on their value. Hopefully I can change your mind.\n\nNote: I learned my lesson with my last link to Medium, so this is direct to the friends and family access. Should be no log in required.", "Create Date": "2024-04-26 08:38:39+00:00"}, {"submission_ID": "1cdg89d", "submission_Title": "is spark a \"boring \" technology?", "submission_Score": 5, "submission_Author": "Automatic-Law2404", "submission_selftext": "referring to the \"choose boring technology\" essay.  would you consider spark as such a technology.  im talking about  building on prem spark cluster and * not*  using a managed service.  \nfor a company that knows its way around high performance cpp and python. (no java).\n\nis getting into  the spark advanture \"boring\"?\nor maybe just dask jobs are good enough?\n\n ", "Create Date": "2024-04-26 08:32:09+00:00"}, {"submission_ID": "1cdfiow", "submission_Title": "Seeking Suggestions: Can I Secure a Job as a Big Data Engineer as a Fresher with No Experience?", "submission_Score": 1, "submission_Author": "ZenphyriX", "submission_selftext": " I'm currently a computer science and engineering major with approximately 1 year left until graduation. I'm contemplating applying for positions in data engineering, even though I lack professional experience in the field. I have a decent grasp of Python for coding, along with basic knowledge of Java. Additionally, I'm proficient in data structures and algorithms, and I have a good understanding of front-end development (HTML, CSS, JS, Angular). Furthermore, I possess basic SQL skills.\n\nConsidering the booming demand for data engineering roles, I believe it could be a promising career path for me. However, I'm seeking guidance on the necessary tools and skills to acquire to make myself marketable in this field. What specific technologies should I focus on learning? Are there any particular projects or certifications that would enhance my chances of landing a job as a data engineer?\n\nMoreover, I'm curious about the salary prospects for freshers in the data engineering domain. Can I expect a competitive salary despite my lack of professional experience?\n\nAny advice or suggestions would be greatly appreciated as I navigate this transition from academia to the professional world of data engineering.\n\nThank you all for your insights and support!", "Create Date": "2024-04-26 07:42:02+00:00"}, {"submission_ID": "1cdf4f4", "submission_Title": "Series UID for images extracted from a Multi-frame DICOM image(video)", "submission_Score": 2, "submission_Author": "Past_Key9901", "submission_selftext": "I wanted to know that if I'm extracting an image from the video can I save it under the same series as of the video or need to have a new Series UID for the images\u00a0and if I need to create a new series and extracting another image(with different measurements) from the same video, should that image be saved in the same Series UID as of the newly created UID for the first frame.\n\nAnd if there is 2 videos under the same series UID, do I need to keep the series of extracted images from the both the videos same or they need to differ, EX - I've got a OS and OD scans under the same Series, do extracted images from both the video be under the same series UID or different as for my guess it should be under same series UID and shouldn't create a new one. It would be of great help if someone can help me out on this one as I've been stuck on this for a while now and unable to find any leads on official NEMA documentation.", "Create Date": "2024-04-26 07:14:26+00:00"}, {"submission_ID": "1cdf21y", "submission_Title": "i am beginner to data engineering i am familiar with python and sql . i am currently under the training my trainer gave the project description . but i don't know what to do next please help me what do i need to do this project.", "submission_Score": 0, "submission_Author": "Worried-Towel-5886", "submission_selftext": "Context\n\nOptimizing Operations for Lenskart\n\nBackground\n\nLenskart, a leading eyewear retailer, aims to streamline its operations by centralizing product information,\n\npricing details, and customer reviews, and enhancing customer and employee databases.\n\nProcedure Implementation\n\n1.1 Gather Product Data\n\n\uf0b7 The team accesses the Lenskart website to extract detailed information on eyeglasses, sunglasses, and\n\ncontact lenses.\n\n\uf0b7 Product pricing details, including discounts and promotions, are compiled for a comprehensive\n\noverview.\n\n\uf0b7 Customer, order-related information with employee details needs to be obtained from the client side.\n\n\n\n1.2 Platform build\n\n\uf0b7 Build an architecture for this data platform.\n\n\uf0b7 Apply data transformation techniques to cleanse and structure the extracted data.\n\n\uf0b7 Need to ingest them all into the platform and ensure data integrity and consistency during the loading\n\nprocess for accurate analytics and reporting.\n\n\n\n1.3 Document and Metadata Creation\n\n\uf0b7 Create documentation outlining the structure, relationships, and metadata of the loaded data within the\n\ndata warehouse.\n\n\uf0b7 This documentation serves as a reference guide for team members interacting with the data.\n\n\n\nDataset information\n\nScraped Dataset: (Need to scrape from the site)\n\n\uf0b7 Product\n\n\uf0b7 Store\n\nClient Dataset: (Will be shared through the Azure Blob storage)\n\n\uf0b7 Customer\n\n\uf0b7 Transaction\n\nExpecting KPI\n\n\uf0b7 LTM bridge\n\n\uf0b7 Top store, customer, and product sliced and diced in different dimensions.\n\n\uf0b7 Heat maps (e.g., sales distribution with geo-spatial data)\n\n\uf0b7 Customer LTV (Optional if we have time towards end of project)\n\nOutcome\n\n\n\n3\n\nThe streamlined procedure facilitates better coordination between different teams, leading to optimized\n\nwarehousing operations. The shared document serves as a central reference point, enabling Lenskart to\n\nrespond proactively to market trends and customer preferences. Sales and customer service teams use the\n\ncustomer details to tailor their interactions, enhancing the overall customer experience. Also, this platform needs\n\nto provide a solution for operational-related areas. Overall, Lenskart achieves operational efficiency and\n\nimproved customer satisfaction through the implementation of this comprehensive procedure.", "Create Date": "2024-04-26 07:09:55+00:00"}, {"submission_ID": "1cdc878", "submission_Title": "Using WSL2 full time", "submission_Score": 11, "submission_Author": "nydasco", "submission_selftext": "I may need to switch back to using Windows. I\u2019ve been on a Mac and/or Linux at work and home for the last decade or so. I *think* the last version of Windows I used was Vista. I\u2019m far more comfortable in the CLI than the GUI.\n\nI understand Windows has WSL now. Where is that at from a capability perspective? Can I simply use WSL and Firefox and ignore all the other \u2018Windows stuff\u2019? As in have a couple of full screen spaces, and just flick between the two? For the odd GUI app I use, does WSL support a GUI?\n\nDon\u2019t really want to need to learn a new OS if I don\u2019t need to.", "Create Date": "2024-04-26 04:19:20+00:00"}, {"submission_ID": "1cda190", "submission_Title": "Sanity Check: Mage", "submission_Score": 9, "submission_Author": "Lord_Lloydd", "submission_selftext": "I feel like I\u2019m overthinking this, but what\u2019s the general opinion regarding Mage? In a recent post, someone asked about the current state of orchestration and got several good replies. I lead a small data team who just went through our evaluation process so I replied with my experience running Mage in production. However, my reply received far more downvotes than I was expecting. We\u2019re 6 months into our deployment and it\u2019s gone well so far, but the swift negative reaction has me second-guessing myself. We\u2019re not so far along that we can\u2019t pivot but it would be inconvenient. Did I miss something?", "Create Date": "2024-04-26 02:25:31+00:00"}, {"submission_ID": "1cd8xbl", "submission_Title": "Data Catalogue Buy In", "submission_Score": 4, "submission_Author": "InsightByte", "submission_selftext": " Just delivered a new Data Platform with massive success.\n 36 months of hard work, within a global team spreading across multiple timezone.\n \n After delivery, we got huge positive feedback but we also got overwhelmed with question around provenance, glossary info, metadata desc, process flow, freshess, etc \n  Were data consumers want to plug in there process into our data streams and consume them, all in all fair questions.\n\n So that is the problem.\n\n Solution Data Catalogue solution.\n\nMe as a definitive geek, went and implement one as POC.  In this case  I used datahub. \n\nIssue i have. \nGetting pushback from leadership on this, and i am super confused about it.\nThey prefer doing lineage in drawio in confluence.\nTransformation mapping in excell in some obscure sharepoint folder.\nAnd the governance manager does not even know tools like datahub, open metadata, Amundsen even exists... wtf.\n\nI love where i work but this kinda raised an alarm. \n\nAnyway,  i still wanna add Datahub to our stack and prove is worth it. \n\nHow woukd you tackle this?", "Create Date": "2024-04-26 01:33:07+00:00"}, {"submission_ID": "1cd8sem", "submission_Title": "Query engine to combine Graph and NoSQL Data", "submission_Score": 3, "submission_Author": "Hephaestite", "submission_selftext": "I'm essentially looking for a way to run SQL like queries across our data, that consists of graph data in neo4j (a representation of users, groups they belong to, etc) and MongoDB data that contains richer data about those graph nodes.\n\nWe want to be able to design queries that for example build a list of Users based on conditions that might exist in either the graph or the mongo data store. As a software engineer I'm a bit out of my depth when it comes to data engineering.\n\nOpen question, is there something out there that will let us achieve this? or do I need to start considering building a relational database representation of our data? I'm a bit reluctant to go down the traditional relational SQL avenue because the data structures and types of data we are storing are likely to change quite a bit over time and possibly from customer to customer.", "Create Date": "2024-04-26 01:26:30+00:00"}, {"submission_ID": "1cd8bs7", "submission_Title": "Noob learning resources ?", "submission_Score": 0, "submission_Author": "rantedranter", "submission_selftext": "Hi everyone, complete Newbie to data Engineering working in a junior role. What book recs do you have to master Microsoft SQL specifically. \n\nI\u2019m currently practicing with sqlbolt, watched a bunch of YouTube videos but I think reading, might really help too. \n\nAny recs?\n\nThanks in advance ", "Create Date": "2024-04-26 01:04:33+00:00"}, {"submission_ID": "1cd89wr", "submission_Title": "Data pipeline for a new blogs/ articles notification of Tech blogs", "submission_Score": 5, "submission_Author": "FriendshipEastern291", "submission_selftext": "Hi everyone, I currently want to do a data pipeline for an app that users can insert any news url(tech blog site, news site,...) that they want to get notification if that site upload a new blog. What Im going to do is:\n\n\\- use scrapy to scrape all url that appear on the input url (my scale is going to be 1000s of url a day). Output of 1 scraped url is all the urls that appear on that site(example. Link to home, link to blogs, links to pages,...)   \n\\- after scrape through 1000s of url I will store the output in S3 bucket (output of scraped url), I will process that to get what is the blogs url in the output and I will save it into database Ex. Reddit. 'com has 20 blog links  \n \\- i will run that loop each day to refresh if there are any site that post new blogs and update my database\n\n\\- in addition, i have to serve data for my DS to build a model to recommend users another blogs base on there urls subscription they want to get notified  \nWhat will you do if you approach this problem, is my plan for this problem optimized ?  \nWhat should I do to serve the data for the DS smoothly?  \nAppreciate every advice !", "Create Date": "2024-04-26 01:02:08+00:00"}, {"submission_ID": "1cd6v3c", "submission_Title": "Data Engineering Requirements vs. Data Science - Career Decision", "submission_Score": 14, "submission_Author": "motiontrading", "submission_selftext": "Hi Data Engineers,\n\nI hope to get some insight here as I am making a career transition decision. I have formerly been a systems administrator but I quit because of the on-call requirements. I am currently enrolled in University to get my Data Science degree and I just recently got interested about a bootcamp to become a Data Engineer. My question is do you have to be on-call as a Data Engineer? Asking the bootcamp they told me that most Data Engineer jobs do not require you to be on-call and that you work only during normal business hours.\n\nThanks in advance for your answers!", "Create Date": "2024-04-25 23:58:18+00:00"}, {"submission_ID": "1cd4p69", "submission_Title": "How do you handle changes upstream to field names that affect your scripts downstream?", "submission_Score": 2, "submission_Author": "miserablywinning", "submission_selftext": "Hi everyone,\n\nA system migration project I\u2019m working on right now has an issue where changes are being made to field names and shifting fields from one table to another as they continue to design the system while we are building curated datasets/implementing at the same time. \n\nThe problem is, they\u2019ll change a field name from say, field_name_10 to field_name_20, or shift a field from table_4 to table_8, without letting our team or other downstream teams know. As a result, our scripts will fail and we have to find where the change was made to. There are multiple changes being made during the week/month, and we have over 50 tables in our data warehouse so we cannot check every single table for changes. \n\nMy question is, how have some of you dealt with this? How are changes communicated to you? Any particular tools or apps that we can use to help with this? I\u2019m looking for something that can maybe notify us of changes made if it directly affect us/our tables in the DW. \n\nI thought of using the github versioning stuff, but it doesn\u2019t seem like the best option since we aren\u2019t all using the same schemas. \n\n\n", "Create Date": "2024-04-25 22:28:10+00:00"}, {"submission_ID": "1cd4ouc", "submission_Title": "Cloud Function to trigger Cloud composer DAG", "submission_Score": 2, "submission_Author": "brittlet", "submission_selftext": "I need some help in coding/building the following scenario since I'm new to this profession,\n\nI have a DAG which should be triggered when a file is dropped in a GCS bucket folder. This can be achieved by an event driven cloud Function. Is it possible to suppress the firing of the cloud function until the DAG run is complete and make it active once the DAG run is complete. A step by step explanation of the solution will be greatly appreciated.", "Create Date": "2024-04-25 22:27:48+00:00"}, {"submission_ID": "1cd3xyl", "submission_Title": "Sync tables from Mysql to any OLAP", "submission_Score": 5, "submission_Author": "diceHots", "submission_selftext": "Hi everyone, that's say we have some operational database (mysql) and to do some ELT to directly dump the data into a OLAP.  I am not sure what's the proper tool to use here. I have played around with some potential tools here,\n\n* Airbyte (no code solution, it works for how's the scalability?)\n* Flink and flink cdc\n* Meltano (yaml-based but not a lot of people use it)\n* any other recommendation?\n\nJust trying to see the pro and cons here. Thanks", "Create Date": "2024-04-25 21:52:28+00:00"}, {"submission_ID": "1cd3saf", "submission_Title": "Database Access Solution", "submission_Score": 2, "submission_Author": "sdsmith0610", "submission_selftext": "Not sure I'm in the right place, I checked out the database sub but it doesn't seem to be as active.  I started a job a few months (accounting role), in previous places of employment had few problems accessing data via power query for use in excel and for use in power bi.  As background, I know enough basic coding and sql to be dangerous and google is my friend.  Current company uses Sage  with ODBC.  They have the US Sage application and the UK Sage application stored on an Azure cloud server.  Access is via dns router (i think).  To query data and refresh reports I have to use a terminal server and the dns connection is set up as a \"silent\" (??) Connection to the ODBC database, anyone that wants to refresh reports also has to have this access point.  They don't want more than one person doing this as it puts excessive load on the server.  Thus, we are meeting next week to discuss transition to Sage with SQL database, but from what I have heard this will not solve the problems due to the way it is set up.  My question is what other alternatives are there to easily access data without use of terminal server and having the ability for access by multiple people?  Is there a solution that could potentially employ a gateway connection, or a repository that can be auto updated?  I have never had so many problems with accessing data for use in reports.  FYI, our IT group seems to have very little experience in setting this up, so far their solution has been there isnt one and we just can't do it unless we get Sage with SQL.   Any help or solutions I can research would be a lifesaver!!!!", "Create Date": "2024-04-25 21:46:01+00:00"}, {"submission_ID": "1cd2hq4", "submission_Title": "Question regarding relation denormalization ", "submission_Score": 3, "submission_Author": "xyzb206", "submission_selftext": "I was toying around with the stack overflow data dumps ( in my case Law Exchange since it was smaller ) mainly trying to implement TF-IDF in plpgsql. The data came in these big denormalized XML files, and I while its clear why they would decide to denormalize the relations, I also kinda was wondering if its posible to have the cake and both eat it with a implementation like this: \n\n \u2022 Data is stored in a normalized form\n\n \u2022 The database has a materialized view with a query that denormalizes this data into itself.\n\n \u2022 Any read query is sent to the materialized form, where it will be a lot faster to query all the relevant data without extra joins.  \n\n \u2022 Any writes are sent to the Normalized relationships who will perform a easy write that can actually be given efficient Foreign Key constraints, from there a trigger will be fired up after the write and this trigger will modify the materialized view apropriatly ( not rerun the base query but rather insert/update a row )\n\n \u2022 Database backups will truncate the materialized view and due to the normalized data structure backups will become smaller, in the case of a need to restore from backup, the materialized view query can be rerun.\n\nFor me this seems like a sitatuion without a downside,\n\n \u2022 You get smaller file sizes (since for every prod database Im assuming there are at least a few in backup). \n\n \u2022 Intuitive consistency checks that dont need to be implimented in the application layer. \n\n \u2022 A much more logical and easy to work with schema.\n\n \u2022 Slight performance improvments on writes ( if I'm not wrong, since the extra write to the view could eat that, either way for such a read intensive app the benifit is pretty meaningless )\n\n \u2022 Cott is happy\n\nOf course this dosnt change the fact that those guys know what they are doing and probably have a good reason to do it this way, My question is, is such a implementatiom valid? Am I missing something here?\n", "Create Date": "2024-04-25 20:54:59+00:00"}, {"submission_ID": "1cd1ce6", "submission_Title": "Neo4j as a NoSQL database for large data and location data", "submission_Score": 4, "submission_Author": "__bdude", "submission_selftext": "Hi all,\n\nI've been exploring various NoSQL databases that effectively handle large datasets (1 entry per second) and provide robust support for location data and big data sets. Recently, I've been delving into Neo4j, and I got inspired by its graph-based model, which seems to offer significant advantages for complex queries that involve relationships and spatial data.\n\nMy thought is in between neo4j and elastic search. Is the approach of Neo4J a logical one? What are your experiences?\n\nKind regards,\n\n\\_\\_bdude", "Create Date": "2024-04-25 20:11:39+00:00"}]